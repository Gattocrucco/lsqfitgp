La formula con le gvar per il processo gaussiano è

    y* = y*_p + Kxsx (Kxx + Cy)^-1 (y - y_p)

Avevo già controllato che questa formula mi dà la matrice di covarianza
corretta per y*. Però vengono fuori anche le correlazioni tra y* e y, y_p,
y*_p. Che cosa rappresentano statisticamente?

Assumiamo che y e y(*)_p siano indipendenti. Allora

    Cov[y*, y*_p] = Cov[y*_p] = Kxsxs

    Cov[y*, y_p] = Cov[-Kxsx (Kxx + Cy)^-1 y_p, y_p] =
                 = -Kxsx (Kxx + Cy)^-1 Kxx
    
    Cov[y*, y] = Kxsx (Kxx + Cy)^-1 Cy

2022-04-17
==========

Cose da fare per le PDF:

1)  Mi invento io delle funzioni e dei dati dal culo e faccio un fit,
    solo roba lineare (FATTO)
2)  Aggiungo roba quadratica (sempre dal culo) (FATTO)
3)  Aggiungo l'errore sulle M (FATTO)
4)  Uso delle PDF vere anziché culiche
5)  Aggiungo i dati veri

2022-04-18
==========

Come implemento addproc etc.?

1)  Chiamo _KernelDeriv -> _CrossKernel (FATTO)
1a) Sposto la logica di diff fuori da fun e giro i test (FATTO)
1b) Modifico diff in modo da tener conto che le x a sinistra possono essere di
    tipo diverso dalle x a destra (SALTARE)
2)  Sposto _binary in _KernelBase e tolgo il controllo che gli scalari siano
    positivi, come classe uso type(self) anziché Kernel (FATTO)
3)  Sovrascrivo _binary in Kernel controllando che l'eventuale scalare sia
    positivo e poi chiamo super (FATTO)
4)  Aggiungo un metodo rescale a _KernelBase, funziona come diff cioè fa
    trasf. diverse per x e y, se sono diverse (definito con is not) cambia la
    classe dell'output a _CrossKernel (FATTO)
5)  Definisco nuove classi _Proc, _ProcKernel, _ProcTransf e _ProcDeriv
    analoghe di _Element, _Points e _Transf (NON FARE _ProcDeriv) (FATTO)
5a) Aggiungo un attributo proc di tipo _Proc a _Points (FATTO)
6)  Definisco _GP._crosskernel che calcola il kernel per due _Points
    ricorsivamente, con metodi analoghi a quelli che uso per implementare
    _makecovblock: _crosskernel_kernel_kernel, _crosskernel_transf_any,
    _crosskernel_deriv_any (NON FARE _deriv) (FATTO)
7)  _crosskernel_kernel_kernel semplicemente restituisce il kernel se i due
    _ProcKernel coincidono, altrimenti un sigleton _ZeroCov (FATTO)
8)  _crosskernel_deriv_any calcola il kernel e poi chiama diff, _ZeroCov viene
    passato (SALTARE)
9)  _crosskernel_transf_any cicla sugli elementi calcolando i kernel e
    combinandoli con _binary, gli _ZeroCov vengono ignorati, gli scalari nulli
    non producono _ZeroCov altrimenti si incasinano le derivate, alla fine se è
    uno _ZeroCov viene passato (FATTO)
10) Aggiungo GP.addproc, addproctransf e addprocderiv, i _Proc vengono infilati
    in un dizionario apposito, possono avere gli stessi nomi degli x (NON FARE
    addprocderiv) (FATTO)
11) Aggiungo proc= ad addx, deriv= continua ad avere la sua implementazione a
    parte in _Points e _makecovblock_points (FATTO)
12) Controllo che le x abbiano lo stesso tipo, per adesso (FATTO)
13) Riimplemento la somma di componenti con questa roba (FATTO)

Problema serio: come faccio a specificare cosa derivare se ogni processo ha la
sua x? Cioè: le x sono le stesse, oppure ogni processo ha la sua x?

=> Soluzione rapida e parziale: posso derivare solo singoli processi e non le
trasformazioni. In questo modo continuo ad assumere che le x siano le stesse.

=> Soluzione più completa: quando dico la derivata, devo poter specificare
rispetto a quale processo, o se derivare rispetto a tutte le x, in generale
devo poter specificare vari criteri. Forse è troppo complicato e l'utente non
avrebbe chiaro cosa sta facendo.

L'interfaccia elegante sarebbe questa: quando definisco una trasformazione dei
processi, devo dire se le x sono le stesse o sono diverse. In formule, devo
distinguere

    h(x) = f(x) + g(x)

da

    h(x, y) = f(x) + g(y)

Un modo sensato sarebbe proprio leggersi una stringa del genere. Una versione
più accroccata intermedia sarebbe un'opzione di addproctransf che mi
permette di dire se le x sono tutte le stesse o sono diverse, per fare
combinazioni più strane chiamare più volte addproctransf. Se sono le stesse poi
addx controlla che metti dentro x con lo stesso dtype. Poi però devo inventarmi
un modo di propagare queste informazioni fino a _KernelBase.diff.

--------

What calculation should I do to bypass the limitation that new gvars can not be
correlated with old gvars?

Old = x, new = y

Desired covariance matrix:

    V = [ V_xx   V_xy]
        [ V_yx   V_yy]

z = auxiliary variables, as many as y, independent of x

I have to write y as a combination of z and x:

    y = Az + Bx

Cov[x, y] = Cov[x, Bx] = V_xx B^T = V_xy   ==>  B^T = V_xx^-1 V_xy
Cov[y, y] = Cov[Az, Az] + Cov[Bx, Bx] = A V_zz A^T + B V_xx B_T = V_yy  ==>
    ==>  A V_zz A^T = V_yy - V_yx V_xx^-1 V_xy = V/V_xx

I can choose A = I and then V_zz = V/V_xx. I have to decompose V_xx.

gac, I rediscovered conditioning

2022-04-22
==========

The nonlinear fit isn't working. With enough quadratic data the fitted data
(even the linear one) is far from the actual data, but even with one single
nonlinear point the fit starts to fail in some places.

Is this a bug or an excess of nonlinearity?

In the latter case, I expect that reducing the errors on the quadratic data
should effectively linearize it. Then, provided I start the fit already in the
correct neighborhood, it should work.

=> tried: it's even worse!

I'm combining 9 latent points into one single quadratic datapoint. Maybe even
if the error on the datapoint is small, the freedom left to the latent points
makes their nonlinear bias large.

=> Using only one point as quadratic input, still does not work at all. This
suggests it is a trivial bug of some sort.

Bug found: I was passing the prior through the nonlinear function and then
sampling, instead of sampling and then passing the sample through the nonlinear
function. Since the prior is wide, it was being affected a lot by the
nonlinearity.

--------

Next thing to do: pdf6.py, fit hyperparameters and quadratic data together. I
foresee that it will be slow because I can't take derivatives through lsqfit
with autograd. I should read how lsqfit.empbayes_fit is implemented.

Actually, I could first add errors to M and M2 instead of fitting kernel
hyperparameters.

2022-04-23
==========

I must also devise a convenient extension of predfromfit for the case where I
condition on some stuff with predfromdata and on some other with lsqfit. For
the time being, since the sum rule constraints are exact, I think that
conditioning twice is valid (but I should check to be sure, maybe add a test
case).

--------

The fit with the errors on M is not working. Two alternative explanations:

  * It can't work because there is too much uncertainty. But I would expect the
    fit to return an appropriately large sdev in this case, which is not
    happening.

  * The sheer amount of the M parameters (2700) makes the minimization "happy"
    too early because M is easy to fit.

To check this, I can hold M fixed to its mean (different from the true value
used to generate the data) without fitting it. If the result I get is still
nonsense, then probably it's a statistical and not a numerical problem.

=> The fit makes sense. This indicates that it's a numerical problem. Possible
quick hack: very low termination tolerance.

=> The quick fix didn't work, but starting from the true values gives a good
result, which confirms it is a numerical problem.

Anyway now I found out that in the actual problem M is actually parametrized by
just a few numbers despite being a large matrix, so whatever.

To do next time: pdf7.py, i.e., fit hyperparameters on top of nonlinear fit
(will be damn slow!)

Following steps:
  * pdf8.py scale the number of datapoints/parameters to a realistic one
  * pdf9.py change the fake data, the grids and the kernel to realistic ones

--------

I did a quick benchmark of pdf6. Most of the time is spent in evaluating the
jacobian of the model function with gvar. Time per call:

    fcn(floats): 200 us
    fcn(gvars): 230 ms
    forward jacobian with jax: 30 ms
    backward jacobian with jax: 3 ms
    jacobian with autograd: seconds

I don't expect to find a way to easily optimize the gvar calculation. 7 times
slower than the compiled jax version seems already quite good considering it's
naive sparse operations.

The other half of the time spent by the fit is computing as svd in trf, I
assume it's the svd of the jacobian. So even optimizing the derivative
calculation would give at most a factor of two. Maybe there are other faster
methods in least_squares for a large dense jacobian? => maybe lm? look also
into GSL

If I did things manually, maybe I could take into account that the jacobian is
split in two blocks, one the identity and one dense, and then multiplied by
something. If this something is a diagonalization instead of cholesky then
maybe I can obtain the svd of the jacobian.

2022-04-25
==========

If I set p0 in fitargs of lsqfit.empbayes_fit, will it still adapt p0
automatically? => Yes, it works.

Idea: since I know the components of M and M2, could I first transform the grid
of points for each component? => Maybe it is not doable for the quadratic data,
and moreover in the real problem there are more datapoints than grid points.

--------

I talked to Alessandro about the details of the complete PDF fit. Summary:

The grid of x points is
[
    1.9999999999999954e-07, # start logspace
    3.034304765867952e-07,
    4.6035014748963906e-07,
    6.984208530700364e-07,
    1.0596094959101024e-06,
    1.607585498470808e-06,
    2.438943292891682e-06,
    3.7002272069854957e-06,
    5.613757716930151e-06,
    8.516806677573355e-06,
    1.292101569074731e-05,
    1.9602505002391748e-05,
    2.97384953722449e-05,
    4.511438394964044e-05,
    6.843744918967896e-05,
    0.00010381172986576898,
    0.00015745605600841445,
    0.00023878782918561914,
    0.00036205449638139736,
    0.0005487795323670796,
    0.0008314068836488144,
    0.0012586797144272762,
    0.0019034634022867384,
    0.0028738675812817515,
    0.004328500638820811,
    0.006496206194633799,
    0.009699159574043398,
    0.014375068581090129,
    0.02108918668378717,
    0.030521584007828916,
    0.04341491741702269,
    0.060480028754447364,
    0.08228122126204893,
    0.10914375746330703, # end logspace, start linspace
    0.14112080644440345,
    0.17802566042569432,
    0.2195041265003886,
    0.2651137041582823,
    0.31438740076927585,
    0.3668753186482242,
    0.4221667753589648,
    0.4798989029610255,
    0.5397572337880445,
    0.601472197967335,
    0.6648139482473823,
    0.7295868442414312,
    0.7956242522922756,
    0.8627839323906108,
    0.9309440808717544,
    1, # end linspace
]

Number of datapoints: 4535 total, of which 3089 linear. The relative errors are
tipically 2-10 %, with some exceptions down to 0.1 % and up to 50 %.

It is convenient to describe the PDFs with a change of basis, see:

https://eko.readthedocs.io/en/latest/theory/FlavorSpace.html#qcd-evolution-basis

First, for each quark, define
    
    q+ = q + qbar,
    q- = q - qbar.

The gluon g is left alone. Then

    Sigma = sum_q q+
    V     = sum_q q-
    
    V3  = u- - d-
    V8  = u- + d- - 2s-
    V15 = u- + d- + s- - 3c-
    
    T3  = u+ - d+
    T8  = u+ + d+ - 2s+
    T15 = u+ + d+ + s+ - 3c+

So the Ts are like the Vs but with q+ instead of q-. The constraints are:

    For all f: f(1) = 0

    Total momentum: int dx x (Sigma(x) + g(x)) = 1

    The integrals of the Vs are
        V   3
        V3  1
        V8  3
        V15 3
    
    Sigma and g diverge power-like for x -> 0, the Ts and Vs don't
    
    For g and Sigma: x^2 f(x) -> 0 for x -> 0
    
    For Vs and Ts: x f(x) -> 0 for x -> 0

2022-04-28
==========

How should I parameterize the PDFs in the fit? I should pick the basis which
makes the kernel easier to write. The divergent behaviour is specified in the
Sigma-T-V basis, so I should use it. I could also do like they are doing with
neural networks and multiply a base process by x^-a (1-x)^b, but only Sigma
and g, while the others would get (1-x)^b. Should I let each process have its
own b? Dunno.

--------

I'm having a problem with pdf8, the fit fails due to zeros in the covariance
matrix. The zeros must be numerical. I think it is due to the absurdly high
priors and data I'm putting in due to the divergence of Sigma(x) and g(x).
I should define the transformation w.r.t. x*Sigma and x*g probably, and only
use x from 1e-5 or 1e-4 onward for data.

2022-04-29
==========

In the end the main problem with pdf8.py was the zero error on f(1) due to the
constraint.

To do next time: implement Kernel.xtransf and use a linear interpolation of the
grid with ExpQuad instead of Gibbs(scalefun=x). But before that check the order
thing of the Gibbs kernel.

Implement a decomp parameter to lgp.raniter to see if eigcut- solves the
rough samples problem of gvar.sample (the corresponding functionality in
gvar.sample is broken).

I want to let xSigma and xg go power-like for x->0 with an exponent between
-1 and 1. The most convenient way is defining them as some process rescaled by
x^a, but this messes up the integrals. How do I fix this? What I need in
general is a kernel for a process whose derivative goes like x^a.

--------

Tentative:

    f ~ GP

And the correlation length of f(x) is x (uniform in log(x)), so

    f'(x) ~ 1/x

Where now with ~ we mean "goes like" instead of "is distributed as". So
defining the transformed process

    g(x) = x^(a+1)/(a+2) f(x)

We have
    
    g'(x) = x^a (a+1)/(a+2) f(x) + x^(a+1)/(a+2) f'(x) ~
          ~ (a+1 + 1)/(a+2) (x^a 1 + x^(a+1)/x) =
          = x^a

Then we take g'(x) to be x Sigma(x) or x g(x).

--------

More generally, let w(x) be the change of variable that makes the correlation
length constant. Then the scale function w.r.t. x is

    s(x) = 1/|w'(x)|.

So if f(x) ~ 1, we have f'(x) ~ 1/s(x). Let g(x) be the primitive of x Sigma(x)
or x g(x), which we define as

    g(x) = u(x) f(x)

for a generic factor u(x) which we want to determine such that

    g'(x) ~ x^a.

We have

    g'(x) = u'(x) f(x) + u(x) f'(x) ~
          ~ u'(x) + u(x)/s(x) =             (assuming w'(x) > 0)
          = u'(x) + w'(x) u(x) = x^a
    
This differential equation has the solution

    u(x) = exp(-w(x)) (int dx x^a exp(w(x)) + constant)

To check that we can reobtain the previous result, let
    
    s(x) = x  =>  w(x) = log(x),

then
    
    u(x) = 1/x (int dx x^a x + c) =
         = 1/x (x^(a+2) / (a+2) + c) =
         = x^(a+1)/(a+2) + c/x

Requiring a finite u(0) to have an integrable g'(x), we have

    g(x) = x^(a+1)/(a+2) f(x)   q.e.d.

2022-05-01
==========

pdf8.py works fitting exponents for xSigma and xg. To do next time: increase
the allowed range of exponents (use a uniform prior), use a realistic number
of datapoints (see above), save the history of hyperparameters and marginal
likelihood in the minimization to estimate the hessian with a fit in the end.
Maybe first implement the hessian thing with a simpler fit.

Once I have this I'd say I can stop for a while on the PDFs and maybe port
lsqfitgp to JAX and do other improvements to be able to optimize the fit.

2022-05-02
==========

New piece of information: there aren't only linear and quadratic data, there's
also ratios and other stuff. How many of them?

I won't do the hessian because lsqfit.empbayes_fit does not let me pass
parameters to scipy.minimize so I can't collect the target outputs with
options=dict(return_all=True).

Fit with many datapoints started at 11:15. => First iteration ended at 11:45.
Thus the complete fit would require about 60 hours. The bottleneck appears to
be the jacobian calculation with gvar, in particular the quadratic data tensor
contraction which sums along a 50x50x9 = 22500 axis.

How do I solve this problem?
    1) Verify that this is the bottleneck.
    2) Wrap fcn using gvar_function and compute the jacobian with jax.

2022-05-05
==========

Commands to get the version number:

grep __version__ lsqfitgp/__init__.py | sed -e "s/__version__ = //" -e "s/'//" -e "s/'//"
python -c 'import lsqfitgp;print(lsqfitgp.__version__)'

2022-05-31
==========

Next thing to do: test jax jit.

2022-06-01
==========

About Fisher scoring: the expected value of the log marginal likelihood is:

    E[-1/2 log(det(V)) -nlog(2π) -1/2 (x-mu)^TV^-1(x-mu)] =
    -1/2 log(det(V)) -nlog(2π) -1/2 V^-1_ijE[(x-mu)_i(x-mu)_j] =
    -1/2 log(det(V)) -nlog(2π) -1/2 V^-1_ij V_ij =
    -1/2 log(det(V)) -nlog(2π) -1/2 tr(V^-1 V) =
    -1/2 log(det(V)) -nlog(2π) -1/2 tr(I) =
    -1/2 log(det(V)) -nlog(2π) -1/2 n

So in practice the thing amounts to computing the hessian of -1/2 log(det(V)),
but the gradient of the full likelihood, and put these into a second order
minimizer. Algorithms that may be appropriate in optimize.minimize: dogleg,
trust-exact.

**** WRONG, SEE 2022-06-06 ****

Other hack-hessian option: external product of the residuals.

    V = LL^T
    V^-1 = L^T^-1 L^-1
    r = L^-1 (x - mu)

    D (-1/2 r^T r) = -r^T Dr
    D^2 (-1/2 r^T r) = -Dr^T Dr -r^T D^2r
    
So I would just keep -J^T J, which is positive definite, instead of the full
hessian. To do this with autodiff I would need to let jax go through the matrix
decomposition. Tentative interface: add an option direct_autodiff to the
__init__ wrapper in DecompAutoDiff.

2022-06-02
==========

The second derivative of logdet does not work. Possibilities:

1) I'm doing the test wrong => unlikely because it fails both with analytical
   solution and finite differences
2) There is a sneaky error in the jvp of solve
3) I have not properly understood how the tracing-stopping works and it doesn't
   with multiple layers to unpack

First things to do:

1) Check finite differences against the partially handwritten solution
2) Implement direct_autodiff and see if it works.

=> with direct_autodiff, the derivatives work, both compared to finite
differences and to the handwritten solution. This leaves (2) and (3), and I
guess (3) is the case since the jvp of solve is tested on its own.

I have no idea on what's going wrong, and I don't feel like diving into jax
tracing, so the next thing to do is implementing the second derivative test
for solve and quad and see if they fail too. Since I expect debugging this
will require some time, after checking that direct_autodiff works in those
cases I should switch to improving pdf*.py and using hessians in empbayes_fit.

2022-06-04
==========

Some benchmarks of toeplitz chol matvec:

In [344]: def trychol(n, func):
     ...:     t = jnp.exp(-1/2 * jnp.linspace(0, 5, n) ** 2)
     ...:     t = t.at[0].add(lgp._toeplitz.eigv_bound(t) * n * finfo(float).eps)
     ...:     b = np.random.randn(n)
     ...:     getattr(lgp._toeplitz, func)(t, b)
     ...: 

In [345]: %timeit trychol(10000, 'chol_solve')
440 ms ± 7.56 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

In [346]: %timeit trychol(10000, 'cholesky')
227 ms ± 1.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

In [347]: %timeit trychol(10000, 'cholesky_jit')
159 ms ± 164 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

In [348]: %timeit trychol(10, 'cholesky_jit')
692 µs ± 214 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)

In [349]: %timeit trychol(10, 'cholesky')
68 ms ± 146 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

In [350]: %timeit trychol(10, 'chol_solve')
910 µs ± 4.26 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)

2022-06-05
==========

The forward jacobian of quad and solve seems more accurate with custom
derivatives than with direct ones, but maybe only for some matrices.

--------

The forward second derivatives work. I can move on to implementing
hyperparameters algorithms.

2022-06-06
==========

Interesting piece of code:

File ~/Documents/Scuola/lsqfitgp/Repository/lsqfitgp/pythonvenv/lib/python3.10/site-packages/scipy/optimize/_trustregion_exact.py:245, in IterativeSubproblem.__init__(self, x, fun, jac, hess, hessp, k_easy, k_hard)
    242 self.dimension = len(self.hess)
    243 self.hess_gershgorin_lb,\
    244     self.hess_gershgorin_ub = gershgorin_bounds(self.hess)
--> 245 self.hess_inf = norm(self.hess, np.Inf)
    246 self.hess_fro = norm(self.hess, 'fro')
    248 # A constant such that for vectors smaler than that
    249 # backward substituition is not reliable. It was stabilished
    250 # based on Golub, G. H., Van Loan, C. F. (2013).
    251 # "Matrix computations". Forth Edition. JHU press., p.165.

--------

I noticed that I made a mistake in computing the Fisher matrix, I took the
expected value *before* differentiating. The correct expression is

    E[∂∂l] = 1/2 tr(K^-1 ∂K K^-1 ∂K)

How do I implement it in jax? => Use lax.stop_gradient(K_dot) in solve_vjp

2022-06-07
==========

Jax stuff
---------

From jax #9973:
jax.config.update('jax_default_matmul_precision', jax.lax.Precision.HIGHEST)
The default is not highest.

jax.lax.cond is the jittable equivalent of if
see https://github.com/google/jax/discussions/10306 about evaluating or not both
branches of a conditional.

https://github.com/google/jax/issues/5507#issuecomment-767931338
"JAX handles NumPy constants with zero strides in a smart way."

To pass a marked-as-zero tangent to jax.jvp, use the special dtype
jax.dtypes.float0. May work also for integer inputs not to be differentiated.

jax.tree_util.tree_map allows mapping a function through a set of trees. I
could use it with StructuredArray in place of my current custom traverser.
To handle array fields, transpose everything before and after the reduction,
to have automatic broadcasting (does tree_map support broadcasting?)

see https://github.com/josipd/jax/blob/master/jax/experimental/jambax.py to
convert numba functions to jax primitives

https://github.com/google/jax/discussions/10004 about using Fisher matrices

2022-06-08
==========

Next things to do:

1) Write pdf9.py stripping pdf8.py of all non-linear things and using
empbayes_fit for the hyperparameters (DONE)

2) Scale pdf9.py to a large dataset (SOMEWHAT DONE)

3) Write pdf10.py like pdf9.py but using real PDF data

2022-06-10
==========

I did the calculation of the Fisher information when the residuals (i.e., the
mean vector) depend on the hyperparameters. The result is that I have to add a
term ∂r^T K^-1 ∂r, i.e., quad(rjac).

2022-06-13
==========

pdf9.py with 500 datapoints runs smoothly, but does not give something
compatible with the truth. Even judging by eye, the fitted correlation length
is too short, the valence shell looks shrinked. The minimum seems robust to
change of fitting method. Ideas about this:

1) Instrument properly empbayes_fit
2) Maybe the link matrix with iid normal entries is particularly pathological
   for some reason. Like, the effect tend to cancel out statistically as I
   increase the size of the matrix, so it is numerically inaccurate doing
   the inference.

Another fit that does not work is the periodic one in test_fit. Maybe exploring
that will clear my understanding of using Laplace with a Gaussian process.

2022-06-17
==========

Thing that I could cite: Snelson et al., "Warped Gaussian Processes". It's a
bit trivial but people expect you to cite everything. Everything was redone
for Gaussian processes.

2022-06-20
==========

sinc(x) = sin(x)/x

sinc'(x) = cos(x)/x - sin(x)/x^2
         = (cos(x)x - sin(x))/x^2 =
         = cos(x) (x - tan(x)) / x^2

I have to compute a - b with a - b ~ 0:

a - b = b (a/b - 1)
a - b = log exp(a - b) =
      = log(exp(a/b - 1)^b) =
      = b log exp(a/b - 1)

Does not solve the problem, a/b is already imprecise.

2022-06-29
==========

I learned that the FK tables (the matrices PDF -> linear data) are 90 %
diagonal and the entries are mostly positive.

2022-07-04
==========

AR kernel
---------

- generate the companion matrix of the characteristic polynomial
- diagonalize it to get the roots
- evaluate the polynomial but for each root in turn to get the coefficients
  (not really sure about this part)
- evaluate the combination of powers

Alternative: parameterize with roots and amplitudes, then add class method
to convert parameters to coefficients

Alternative: rotate the yule-walker equations

Question: is the exponential mixture parametrization valid with roots with
multiplicity? For high enough distance I'm sure it is, there may be problems
near zero => Guy on wikipedia
https://en.wikipedia.org/wiki/Talk:Autoregressive_model#Multiplicities_in_characteristic_polynomial
says the general form is a combination of t^r y^-t, with y a root and r
an integer going up to y's multiplicity - 1. (ask Luca for a book about this)

Tentative interface: four parametrizations:
1) w : (p,)
      autoregressive weights
2) y : (p,)
      roots > 1
   a : (p,)
      amplitudes >= 0 for the terms y^-t
3) y : (n,)
      roots
   a : (n, m)
      amplitudes for the terms t^0 y^-t, ..., t^m-1 y^-t (p = n * m)
4) c : (p + 1,)
      first terms of the autocovariance function

=> Problem: there are nontrivial positivity constraints on the coefficients
in the case with multiplicity

=> Actually, even though (2) is always pos def, the amplitudes are uniquely
determined by the roots through the char poly. So I should take the roots
and amplitudes, generate the polynomial, compute the acf, and solve a linear
system for the amplitudes. I can't do this right away from the coefficients
because I need the multiplicity of the roots; my intuition suggests that
it is numerically more stable to evolve the acf instead of using the
analytical expression. The speed should be analogous in practical cases.

Problem with evolving the acf: the maximum lag, and thus the length of the
array, is determined by the data. This is not permitted under the jit =>
add a parameter maxlag, fill with nan for lags not computed.

New tentative parametrizations:
1) w : (p,)
      autoregressive weights
   maxlag : int
      maximum precomputed lag, outside will be filled with nan (must be
      specified even without jit---otherwise the eventual failure under jit is
      confusing)
2) c : (p + 1,)
      first terms of the autocovariance function
   maxlag : int
      maximum precomputed lag, outside will be filled with nan
3) y : (p,) complex
      roots > 1, multiplicity inferred with strict comparison (need a bit
      of work to group roots under the jit due to fixed size, but can be
      done)
4) f : (p,) complex
      y = exp(f)
5) y : (n,) complex
      Roots
   m : (n,) int
      Multiplicities
6) f, m like (4)-(5)

Problem: the complex roots must be conjugate-paired. If I take the real part,
implicitly it's like I'm duplicating any complex root which isn't paired.
However this also means that the actual AR order is higher than the number of
roots provided.

Actually, must the roots necessarily come in pairs? => Yes, and this holds
also with any multiplicity
https://en.wikipedia.org/wiki/Complex_conjugate_root_theorem

This poses continuity problems in the parametrization. I guess that as the
imaginary part of a complex root goes to zero, there will be numerical accuracy
issues since the solution must tend to a multi-pole filter. What should happen
is that the imaginary part of the amplitude diverges to compensate the period
of the sine going to infinity, this gives a linear slope in the limit.

2022-07-05
==========

New AR parametrization:

 1) w : (p,)
        autoregressive weights
    maxlag : int
        maximum precomputed lag, outside will be filled with nan (must be
        specified even without jit---otherwise the eventual failure under jit is
        confusing)
 2) c : (p + 1,)
        first terms of the autocovariance function
    maxlag
 3) lnr : (n,) real
        logarithm of real roots (multiplicity 1), > 0
    lnc : (m,) complex
        logarithm of complex roots (multiplicity 2), real part > 0
    mr : (n,) int
    mc : (m,) int
        multiplicities of roots, if None assumed 1, no check for equality

Problem: how do I implement the multiplicity-dependent terms in a
jit-compatible way? The size of the system changes based on the data. => Put
the roots in an array with multiplicity indicated by repetition. After sorting,
scan keeps track of repetition (a fixed size state is sufficient for this) and
generates the appropriate powers of t as needed along the way.

Problem 2: zero imaginary part in complex roots. I think this can be solved
with a pseudoinverse without further problems.

Problem 3: sign of real roots. I can take the sign of `lnr` as the sign and
its absolute value as logarithm. (The reason for using logarithms is that
roots near 1 would be numerically inaccurate. Also, the logarithm is
interpretable as the inverse of the correlation length.)

New AR parametrization:

 1) w : (p,)
        autoregressive weights
    maxlag : int
        maximum precomputed lag, outside will be filled with nan (must be
        specified even without jit---otherwise the eventual failure under jit
        is confusing)

    Implementation: compute the acf with inverse YW and evolve it up to
    maxlag. Do not check that the w are valid.

 2) c : (p + 1,)
        first terms of the autocovariance function
    maxlag

    Implementation: compute the coefficients with YW and evolve the acf like
    above. Do not check c is valid.

 3) slnr : (n,) real
        logarithm of real roots (multiplicity = 1 x exact repetitions), the
        sign is the sign of the root and the absolute value is the logarithm
    lnc : (m,) complex
        logarithm of complex roots (multiplicity = 2 x exact repetitions), real
        part > 0
    
    The multiplicity is assessed with strict equality. Very close yet distinct
    roots are treated as separate and lead to numerical instability. The order
    of the AR is n + 2 * m. Complex roots which are actually real or almost
    real lower the actual order but are not numerically problematic.
    
    Implementation: sort, then loop keeping track of repetitions generating the
    coefficients of the linear system for the amplitudes of the acf. Do a
    pseudoinverse to accomodate real or almost real complex roots. Generate the
    polynomial from the roots and compute the acf (the RHS) with inverse YW.
    Finally evaluate the analytical expression of the acf.

Problem: it would be very useful for the user to have a check on the validity
of w and c.

For w I can generate the polynomial, compute the roots and check they are above
1. In practice I expect the roots to be quite close to 1 because otherwise the
correlation length is less than 1 time step. I estimate this check to be
numerically accurate only up to ~tens of coefficients. Since I obtain the roots
as eigenvalues, I could use as tolerance the same I use for positivity checks.
I fear additional numerical instability I'm unaware of.

For c I can generate w from YW and check w. Since YW is invertible, obtaining a
valid w means that c was valid.

The most annoying task in all this is porting polyfromroots to jax. I bet that
a naive implementation would be too inefficient and inaccurate.

Also add the parameter conversion functions as static methods.

--------

Problem: even though the roots as logarithms preserve the numerical accuracy
near 1, I have to determine phi and then gamma from the characteristic
polynomial. Phi can not come out accurate in general because I need to pass
from the actual roots, also think about AR(1) where phi = 1/root. However this
affects only the amplitudes, not the decay. Moreover, with slow decay, if I 
use as RHS the first p terms, they will all be almost equal, so the system for
the amplitudes is very degenerate. I should pick the lags based on the
correlation lengths. => Or is there some way to directly obtain the phi for
the decimated process? That way I can move the "effective" roots away from 1.
=> I think that multiplying all the logarithms by an integer constant would
be sufficient.

2022-07-06
==========

Complete reasoning: using the analytical expression of the autocovariance,
I can obtain the one of the decimated process simply by raising all roots to
the decimation. The amplitudes do not change.

Question: is this an autoregressive process? Because the amplitudes are
determined by the roots. => Yes because I can expand recursively to obtain the
lagged y. However, the order will in general be infinite.

Thinking in Fourier space, the relationship between the amplitudes w/ and w/o
dilated roots is not trivial. Maybe in the end the first idea, determining
analytically the amplitudes, was the right one. The denominator is a product of
differences of the roots, which can be computed accurately with complex
sum(logaddexp).

Actually, logaddexp won't be numerically accurate on a difference. In general
it is impossible to recover accuracy because the roots can be arbitrarily close
in relative error (even in logarithm parametrization) and so their difference
is inherently imprecise. This means that log(roots) is not the best
parametrization for accuracy, I need to use diff(sort(log(roots)), prepend=0).

Or is it more convenient log (r_i+1 - r_i)/r_i, with r_0 = 1? This would be in
(-oo, oo) and be about the same value for evenly spaced lengthscales (need to
check this). Then if I compute the logarithm of the amplitudes with proper
combinations of logsumexp, I can write the autocovariance as a logsumexp itself,
which maybe allows to have arbitrarily close roots.

This parametrization implies that I have to treat all roots as complex. Might
as well take the real part only in the end. Wait, do I sort by real part or by
modulus? Maybe the parametrization should handle separately real parts and
phases => The phase is bad as parameter, must use numbers in (-oo, oo).

2022-07-07
==========

A random walk on the phase as prior does not make much sense, so maybe the
phases should be kept separate and not parametrized incrementally?

Tentative: use

    z_i = log (|r_i+1| - |r_i|)/|r_i|,  r_0 = 1,
    
and x_i, y_i such that arg(r_i) = atan2(y_i, x_i).

The quantities I need to obtain from xyz are log(r_i) and r_i - r_j for any
pair i, j. => I can already see it does not work: the differences in the
phases are not numerically accurate.

If I use z_i = log(r_i+1/r_i), a uniform i.i.d. prior on arg(z_i) implies a
uniform i.i.d. prior on arg(r_i).

    r_i = r_i/r_i-1 r_i-1/r_i-2 ... r_1/r_0
    
    log(r_i) = z_i-1 + z_i-2 + ... + z_0 = sum_k=0^i-1 z_k
    log(r_0) = 0
    
    d_i = r_i+1 - r_i = exp(sum_k=0^i z_k) - exp(sum_k=0^i-1 z_k) =
        = exp(sum_k=0^i-1 z_k) (exp(z_i) - 1) =
        = r_i expm1(z_i)

Can I break the accuracy of this? Take

    r_1 ~ 1, r_2 ~ -1, r_3 ~ 1.
    
So we have that r_1 is very close to r_3, but r2 is distant from both.

    r_2/r_1 ~ -1  -->  z_1 ~ 0 + i pi
    r_3/r_2 ~ -1  -->  z_2 ~ 0 + i pi
    
    d_1 = r_1 (e^z_1 - 1) ~ 1 (-1 -1) = -2
    d_2 = r_2 (e^z_2 - 1) ~ -1 (-1 -1) = 2
    
    r_3 - r_1 = d_1 + d_2 ~ 0  -->  cancellation!

If the parametrization is ordered w.r.t. the modulus, then it statisfies r_i >
1 but distant roots can be closer than neighbor roots because the phases
rotates. With lexicographical order there is the same problem. Indeed, I can
have paths on the complex plane such that roots get arbitrarily close again
after an arbitrarily long time (as a spiral). Indeed the fundamental problem
is that it is not possible to sort complex numbers topologically.

What is the least evil then? Depending on how the user wants to put the roots
around, different parametrizations would be appropriate. The base
parametrization should be convenient for generic use, and not impede any
sensible strategy from being adopted.

First I can send the donut into the open circle with the transformation 1/r.
Then maybe I can send the border of the circle to infinity somehow, to have
a parametrization in full R^2. What is the appropriate way here to send a
radius in [0, 1) to one in [0, infty)? It should probably be something that
plays nicely with complex numbers.

However I think it can't be done with an olomorphic function because the
radial stretching doesn't preserve angles.

Tentative:

    |z| = -log(1 - 1/|r|)
    arg z = -arg r

For |r| -> oo, |z| ~ 1/|r| -> 0

For |r| -> 1+, |z| = -log (|r| - 1)/|r| -> -log(|r| - 1) -> inf

If r = 1 + e, e << 1, the lengthscale is 1/log r = 1/log(1 + e) ~ 1/e. The
logarithm of the lengthscale is ~ log(1/e) = -log e = -log(r - 1).

Thus |z|, if somewhat large, is interpretable as the logarithm of the
correlation length.

For r -> oo, scale is 1/log r while z is 1/r. So scale is 1/log(1/z), z is

    s = 1/log r
    log r = 1/s
    r = exp(1/s)
    z = 1/r = exp(-1/s)

Thus for small z, infinitesimal changes bring the correlation length away from
zero, which makes sense since below 1 at some point it's all white noise the
same.

For |r| -> oo, z behaves like (complex) 1/r, since

    |z| ~ 1/|r|, and
    arg z = -arg r.

This means that locally in 0 the circle is sent to itself, this means that the
derivatives can be defined. The stretch becomes "less olomorphic" moving
towards the border.

Now about the differences. Since there is no general compact strategy to
represent accurate differences, I need the user to pass (optionally) the full
matrix of pairwise differences. They have to be in z space to have domain R^2.

    1 - 1/|r| = exp(-|z|)
    1/|r| = 1 - exp(-|z|)
    |r| = 1/(1 - exp(-|z|)) = -1/expm1(-|z|)
    arg r = -arg z
    r = |r|exp(i arg r)
    
Ho d = z - z' (oppure su modulo e fase) e voglio calcolare r - r' usando d.
    
(Usare |z|^2 = -log(1 - 1/|r|^2) mi aiuterebbe? Boh)

    r - r' = |r|exp(i arg r) - |r'|exp(i arg r') =
           = |r|exp(i arg r)(1 - |r'/r|exp(i(arg r' - arg r)))
    
    |r'/r| = (1 - exp(-|z|)) / (1 - exp(-|z'|)) =
           = (exp(|z|') - exp(-|z| + |z'|)) / (exp(|z'|) - 1) = ?
    
    1/|r'| - 1/|r| = (1 - exp(-|z|')) - (1 - exp(-|z|)) =
                   = exp(-|z|) - exp(-|z'|) =
                   = exp(-|z|) (1 - exp(|z| - |z'|)) =
                   = -exp(-|z|) expm1(|z| - |z'|)
    
    |r'/r| = |r'|(1/|r| - 1/|r'|) - 1 --> ?

Ok I have it:

    |r| - |r'| = 1/(1 - exp(-|z|)) - 1/(1 - exp(-|z'|)) =
               = (1 - exp(-|z'|) - 1 + exp(-|z|)) / ((1 - exp(-|z|))(1 - exp(-|z'|))) =
               = |r||r'|exp(-|z'|)(exp(|z'| - |z|) - 1) =
               = |r||r'|exp(-|z'|)expm1(|z'| - |z|)
    
    r - r' = |r|exp(i arg r) - |r'|exp(i arg r') =
           = |r|exp(i arg r) - |r|exp(i arg r') + |r|exp(i arg r') - |r'|exp(i arg r') =
           = |r|exp(i arg r) (1 - exp(i (arg r' - arg r))) + (|r| - |r'|)exp(i arg r') =
           = -r expm1(i (arg r' - arg r)) + (|r| - |r'|)exp(i arg r')

Mmm but I'm actually interested in 1/(r - r'). So
    
    x = 1/r
    |x| = 1 - exp(-|z|)
    arg x = arg z
    
    r - r' = 1/x - 1/x' =
           = (x' - x)/(xx')
    
    1/(r - r') = xx'/(x' - x)
    
    |x'| - |x| = exp(-|z|) - exp(-|z'|) =
               = exp(-|z'|) (exp(|z'| - |z|) - 1)
    
    x' - x = |x'|exp(i arg x') - |x|exp(i arg x) =
           = (|x'| - |x|)exp(i arg x) + |x'| (1 - exp(i (arg x - arg x'))) =
           = exp(-|z'|) (exp(|z'| - |z|) - 1) exp(i arg x) +
             + (1 - exp(-|z|')) (1 - exp(i (arg x - arg x')))
        
    xx' = |x||x'|exp(i (arg x + arg x'))

About the same, can't really write it in a better way. Now it remains to get
the differences in |z| and arg z from the one in z = a + ib.

    |z| - |z'| = √(a^2 + b^2) - √(a'^2 + b'^2) =
               = |z'|(|z/z'| - 1) =
               = |z'|(√((a^2 + b^2)/(a'^2 + b'^2)) - 1)

I have sqrtm1, so let's go inside

    A = a - a'
    B = b - b'

    |z|^2 = a^2 + b^2 =
          = (a' + A)^2 + (b' + B)^2 =
          = a'^2 + b'^2 + 2(a'A + b'B) + A^2 + B^2 =
          = |z'|^2  +  2(a'A + b'B) + A^2 + B^2

Then the phase

    tan arg z' = b'/a'
               = (b - B)/(a - A) =
               = b/(a - A) - B/(a - A) =
               = b/a 1/(1 - A/a) - B(a - A)
    
    (f = arg z - arg z')
    
               = tan (arg z - f) =
               = (tan arg z + tan f) / (1 - tan arg z tan f)

Etc., I guess the calculation can be expressed in terms of cosm1. => No, wait:
I can do it inverting the expression for z - z' in terms of |z| - |z'|, it
contains 1 - exp(i (arg z - arg z')), which is directly what I need.

    z' - z = (|z'| - |z|)exp(i arg z) + |z'| (1 - exp(i (arg z - arg z'))) -->
    
    --> 1 - exp(i (arg z - arg z')) =
        = (z' - z)/|z'| + (|z| - |z'|)/|z'| z/|z|
        
Maybe I could have a parametrization in terms of |z|, a, b where

    z = |z|(a + ib)/hypot(a, b)
    
to have uniform phase distribution but |z| away from zero. => But then |z|
would have to be parametrized with log|z|, log(log(scale))... not convenient.

2022-07-08
==========

The roots at infinity mean that the effective order of the AR is lower, so
shrinkage to z = 0 selects for short AR.

2022-07-14
==========

To parametrize arbitrary roots effectively, I have to find a way to map an
arbitrary invalid difference matrix to a valid one, or in general a
latent parametrization of a difference matrix.

2022-07-16
==========

release notes for 0.12:

--removed maternX2
--removed ratquad
--added cauchy
--added maxdim check
--support numpy functions on StructuredArray
--conversion StructuredArray -> array
--conversion StructuredArray -> unstructured
--Constant and White support arbitrary input (not just numerical)
--Matern now works with any derivative for any nu, extended to nu=0
--fix derivatives for GammaExp
--Gibbs works in nd
--fractional -> bifractional
--extend fractional to negative axis
--rename ppkernel -> wendland, accept real parameter
--extend fourier to arbitrarily large n
--hole effect kernel
--bessel kernel
--pink kernel
--color kernel
--sinc kernel
--stationary frac brownian
--causalexpquad
--decaying
--log
--circular
--MA
--AR
--BART

2022-07-17
==========

Are all the amplitudes of the AR correlation always positive? (With an
appropriate convention for phases)

The characteristic polynomial is P(x) = 1 - phi @ x^n, and all the roots have
|x| > 1. P(0) = 1 + 0j, but I don't know how to extend the positivity on the
complex plane. Maybe there is something about radii like with power series.

I dunno. I should expose a function to compute the amplitudes and try to get
them negative to see if it's a good idea.

=> It's not a good idea, obvious in hindsight.

2022-07-19
==========

Above I wrote that I can compute the AR amplitudes accurately if I start from
the differences between the roots. However, as the roots get close together,
the amplitudes diverge with opposite phases with a precise cancellation to make
the exponentials into a power law. This means that probably I also need the
next order differences, and so on.

But when the roots are close I expect the shape to be more or less fixed. It's
the damn power law I know exactly. So maybe I can change formula when
the roots are closer than something. The only thing that I still can't compute
is the final variance. I have the same problem in computing gamma, where with
very small roots I know it's practically constant, however the normalization
heavily depends on how close they are.

Since huge normalizations are not really interesting, I can solve this kind of
problems by always normalizing the variance to 1.

2022-07-20
==========

Can I write exp(x) - 1 - x as a hypergeometric function?

    exp(x) - 1 - x = 
    = sum_k=2^oo x^k / k! =
    = x^2 sum_k=0^oo x^k / (k + 2)! =
    = x^2 sum_k=0^oo k!/(k+2)! x^k / k! =
    
        k!/(k+2)! = 1·2···k / 1·2···k+2 = 1/2 (1)_k/(3)_k
    
    = x^2/2 sum_k=0^oo (1)_k/(3)_k x^k / k! =
    = x^2/2 1F1(1, 3, x)

However in scipy 1F1 is implented by summing the taylor series, and 1F1 is not
implemented in jax, so whatever.

--------

I think that the space of allowed phi is a simplex whose vertices are the
polynomials with roots either -1 or 1. To prove it, I need to prove that
    1) any convex cobination of valid phi is valid
    2) any valid phi can be expressed as a convex combination of the vertices
Regarding 1), see
https://en.wikipedia.org/wiki/Geometrical_properties_of_polynomial_roots
https://en.wikipedia.org/wiki/Rouché%27s_theorem

--------

Method to allow arbitrarily many large roots in np.poly:
    1) find the smallest in modulus non-zero root x_0
    2) rescale all roots such that |x_0| = 1
    3) invert roots, let zero roots be inf (check complex does not produce nan)
    4) use poly
    5) reverse
    6) rescale with cumprod(|x_0|)

--------

What is the meaning of the phi in the center of the simplex? It is zero at odd
lags. How does a truncated phi appear in baricentric?

2022-07-21
==========

Method to obtain the amplitudes of the AR covariance:

    1) The z transform (with sign +) of gamma is 1/P(z)
    2) Thus gamma_m = 1/m! d^m/dz^m 1/P(z)|_z=0
    3) Decompose 1/P(z) as sum_k c_k / (1 - lambda_k z)
    4) The c_k are given by Hamilton (1994, p. 34-35), or in general see
       https://en.wikipedia.org/wiki/Partial_fraction_decomposition
    5) Use that d^m/dz^m 1/(1 - az)^l = (m+l-1)!/(l-1)! a^m / (1 - az)^m+l =
                                      = (l)_m a^m / (1 - az)^m+l

When two roots are close, assuming no other roots are close, I just need the
difference to write a stable formula. When (inverse) roots are close to zero,
even if they are close together, I think the correct limit is zero.

--------

scipy.special.binom does not obey https://dlmf.nist.gov/1.2.E6. maybe open an
issue.

--------

I think that https://dlmf.nist.gov/25.11.E7 is wrong or incoherent with
https://dlmf.nist.gov/1.2.E6, according to which (n k) = 0 if n is an integer <
k. => Nope, it just means the last term and some of the higher terms of the
summation vanish, the first terms of the summation are nonzero because in (n k)
n < 0.

2022-07-23
==========

Even after computing in a numerically stable way cos(π/2(1-s)), the periodic
zeta becomes inaccurate close enough to even s. The problems just begin at
smaller scale (surely < 1e-4 compared to 1e-2 before). Since the inaccuracy
increases smoothly as x comes closer to half integer, which is the condition
under which the series for the hurwitz zeta has larger terms, it must be some
problem there. Since I obtain the pochhammer symbol and the factorial with
direct multiplication, I guess the most likely culprit is scipy's zeta. Maybe
it's inaccurate near negative odd integers but not exactly at negative odd
integers, possibly because it uses the reflection formula.

=> Nope, scipy's zeta seems accurate enough for small odd integer s (< 10 ULP).
On the other hand, it goes to >100 ULP for large negative s in general,
possibly because of the cosine in the reflection formula. Maybe open an issue.

So the culprit could be scipy's zeta pole? => nope, it's perfect:

    In [685]: special.zeta(1+1e-15)*(1+1e-15-1)
    Out[685]: 1.0000000000000004
    
    In [686]: special.zeta(1+2e-16)*(1+2e-16-1)
    Out[686]: 0.9999999999999998

Other problem: the inaccuracy appears even for s that should be dealt with by
the polylog series. What??

    In [675]: lgp._patch_jax.periodic_zeta_real(.5,12+1e-15).item()
    Out[675]: -0.9997576851438581

    In [676]: periodic_zeta_real(.5,12+1e-15).item()
    Out[676]: -0.9997577024369573

Went through it with pdb, the function correctly returns large s, and large s
and small s agree to 6 ulp:

    ipdb> p z_smalls.item()
    -0.9997576851438587
    ipdb> p z_larges.item()
    -0.9997576851438581

So maybe mpmath is wrong? I already knew it was probably using a different
formula for integer s in the polylog, so it could be possible.

My implementation:

    In [688]: for x in lgp._patch_jax.periodic_zeta_real(.5,12+10.**-arange(1,20)):
         ...:     print(x)
         ...: 
    -0.9997738459102947
    -0.9997593517493558
    -0.9997578523212023
    -0.999757701866772
    -0.9997576868162014
    -0.9997576853110928
    -0.9997576851605816
    -0.9997576851455304
    -0.9997576851440254
    -0.9997576851438748
    -0.9997576851438598
    -0.9997576851438583
    -0.9997576851438581
    -0.9997576851438581
    -0.9997576851438581
    -0.9997576851438581
    -0.9997576851438581
    -0.9997576851438581
    -0.9997576851438581

Mpmath:

    In [689]: for x in periodic_zeta_real(.5,12+10.**-arange(1,20)):
         ...:     print(x)
         ...: 
    -0.9997738459102948
    -0.9997593517493558
    -0.9997578523212024
    -0.9997577018667723
    -0.9997576868162014
    -0.9997576853110931
    -0.9997576851605819
    -0.9997576851455319
    -0.9997576851440595
    -0.999757685144186
    -0.9997576851451117
    -0.9997576851619757
    -0.999757685340211
    -0.9997576864396291
    -0.9997577024369573
    -0.9997576851438582
    -0.9997576851438582
    -0.9997576851438582
    -0.9997576851438582

Ok, seems mpmath sure enough. Can I fix it brutally by increasing the precision?

    In [692]: @np.vectorize
         ...: def periodic_zeta_real(x, s):
         ...:     with mpmath.workdps(32):
         ...:         arg = mpmath.exp(2j * mpmath.pi * x)
         ...:         return float(mpmath.polylog(s, arg).real)
         ...: 

    In [693]: for x in periodic_zeta_real(.5,12+10.**-arange(1,20)):
         ...:     print(x)
         ...: 
    -0.9997738459102948
    -0.9997593517493558
    -0.9997578523212024
    -0.9997577018667723
    -0.9997576868162014
    -0.999757685311093
    -0.9997576851605817
    -0.9997576851455305
    -0.9997576851440254
    -0.9997576851438749
    -0.9997576851438599
    -0.9997576851438583
    -0.9997576851438582
    -0.9997576851438582
    -0.9997576851438582
    -0.9997576851438582
    -0.9997576851438582
    -0.9997576851438582
    -0.9997576851438582

--------

The closest thing to a gamma formula for integer + epsilon is
https://dlmf.nist.gov/5.9.E8.

2022-07-24
==========

jnp.polyval does not respect integer types, while numpy.polyval does. Maybe
open an issue.

--------

_gamma_incr is in general accurate enough (< 10 ULP) apart from x = 1, e = -0.4:

ipdb> p np.stack([np.ceil(np.abs((g2-g1)/g1)/np.finfo(float).eps).astype(int).squeeze(),(100*e).astype(int)],1)
array([[  2, -50],
       [  2, -49],
       [  3, -48],
       [  3, -47],
       [  4, -46],
       [  2, -45],
       [  4, -44],
       [  4, -43],
       [  3, -42],
       [145, -41],
       [ 78, -40],
       [ 41, -39],
       [ 21, -38],
       [ 12, -37],
       [  7, -36],
       [  5, -35],
       [  5, -34],
       [  3, -32],
       [  2, -32],
       [  2, -31],
       [  2, -30],
       [  2, -29],
       [  2, -28],
       [  3, -27],
       [  3, -26],
       [  3, -25],
       [  3, -24],
       [  2, -23],
       [  2, -21],
       [  3, -21],
       [  2, -20],
       [  3, -19],
       [  2, -18],
       [  2, -17],
       [  4, -15],
       [  3, -14],
       [  2, -14],
       [  2, -13],
       [  3, -12],
       [  1, -10],
       [  1,  -9],
       [  4,  -8],
       [  3,  -8],
       [  3,  -7],
       [  4,  -6],
       [  2,  -4],
       [  3,  -3],
       [  4,  -2],
       [  3,  -2],
       [  2,  -1],
       [  0,   0],
       [  3,   1],
       [  3,   2],
       [  3,   3],
       [  3,   4],
       [  3,   5],
       [  2,   6],
       [  4,   7],
       [  4,   7],
       [  4,   8],
       [  4,   9],
       [  3,  10],
       [  3,  12],
       [  3,  13],
       [  3,  14],
       [  4,  15],
       [  4,  16],
       [  5,  17],
       [  3,  18],
       [  4,  19],
       [  4,  20],
       [  3,  20],
       [  3,  21],
       [  4,  23],
       [  5,  24],
       [  3,  25],
       [  4,  26],
       [  5,  27],
       [  4,  28],
       [  5,  29],
       [  5,  30],
       [  5,  31],
       [  4,  32],
       [  4,  33],
       [  5,  34],
       [  6,  35],
       [  5,  36],
       [  4,  37],
       [  4,  38],
       [  5,  39],
       [  5,  40],
       [  7,  41],
       [  7,  42],
       [  2,  43],
       [  6,  44],
       [  2,  45],
       [  6,  46],
       [  7,  47],
       [  7,  48],
       [  7,  49],
       [  5,  50]])

Who's the culprit? Trying values of x in 1...2 it appears that:
  - the problematic values of e more or less translate according to x
  - the problem starts to disappear at x=1.8
  - the inaccuracy is in general much worse (2000 ULP)

Whatever, I don't need it for the periodic zeta.

--------

Failing qdiff tests:

qzero-xother        600-2000 ULP    
qeven-xnearzero     10^14-18 ULP    (not real problem, was ~underflow)

qzero-xother-medrange: the only really bad guy is x=0.16, a=-0.02
qzero-xother-shortrange: 25 bad guys, all have x=0.16, various a
qzero-xother-tinyrange: 25 bad guys, all have x=0.16, various a

Since there are 25 a x 1 q, 25 is all the times x was 0.16. So 0.16 is a
particularly bad value that breaks the series. Must be some sort of error
resonance in the multiplications. Maybe I should compute all terms individually
instead of using cumprod?

=> Wait it's q=0, there's no series involved! (And there would not be in any
case a series in x so whatever I was confused.) The only operations involving
x are x ** q and expm1(-a log(x)).

Since q=0, x**q = 1. And it does not work for any value of a, so it probably
isn't expm1, it should be log(x).

Checking expm1(-a log(x)), jax against mpmath, with a=-0.02 and x=0.16, gives 0
ULP of error.

So, it's actually x=0.15999999999999998, but the single evaluation does not
change.

Nor if I use exactly the same input arrays of the test.

Look at the worst precision for qzero-xother-medrange:

    In [5]: x.squeeze()[i[-100:,0]]
    Out[5]: 
    array([0.1 , 0.14, 0.12, 0.24, 0.12, 0.12, 0.1 , 0.14, 0.14, 0.26, 0.24,
           0.26, 0.16, 0.2 , 0.1 , 0.14, 0.14, 0.1 , 0.14, 0.12, 0.24, 0.1 ,
           0.16, 0.2 , 0.2 , 0.12, 0.2 , 0.24, 0.14, 0.14, 0.2 , 0.2 , 0.18,
           0.22, 0.2 , 0.14, 0.18, 0.1 , 0.24, 0.2 , 0.12, 0.12, 0.12, 0.12,
           0.2 , 0.24, 0.24, 0.12, 0.2 , 0.18, 0.2 , 0.22, 0.12, 0.14, 0.18,
           0.18, 0.2 , 0.14, 0.14, 0.2 , 0.18, 0.16, 0.16, 0.16, 0.18, 0.16,
           0.14, 0.2 , 0.18, 0.16, 0.16, 0.18, 0.16, 0.2 , 0.18, 0.14, 0.18,
           0.18, 0.18, 0.16, 0.18, 0.16, 0.18, 0.18, 0.16, 0.16, 0.16, 0.16,
           0.16, 0.18, 0.16, 0.16, 0.16, 0.18, 0.16, 0.16, 0.18, 0.16, 0.18,
           0.16])

Problem identified: cancellation in pdif + qdif. Worst for x=0.1591562038550412:

    In [64]: f = lambda x, func=func: _patch_jax._power_diff(np.array(x), np.array(0), np.arra
        ...: y(-1e-15)).item() / func(x, 0, -1e-15) - 1

    In [66]: optimize.minimize_scalar(f, bounds=(0.01, 0.5), method='bounded')
    Out[66]: 
         fun: -3.287381478145335e-11
     message: 'Solution found.'
        nfev: 24
         nit: 24
      status: 0
     success: True
           x: 0.1591562038550412

    In [67]: np.expm1(1e-15*np.log(0.1591562038550412)).item()
    Out[67]: -1.8378691448322458e-15

    In [68]: 2*_patch_jax._zeta_zero(-1e-15).item()
    Out[68]: 1.8378770664093433e-15

Possible solution: I can't just set to 0 things below tol, I have to ignore
digits below tol.

In the meanwhile, I may have identified the source of problems for the whole
periodic zeta: inaccurate scipy's zeta zero near -2:

    In [913]: f(2,1e-10)
    Out[913]: -0.030448487956593377

    In [914]: mpmath.diff(mpmath.zeta,-2)
    Out[914]: mpf('-0.030448457058393271')

So the error is in hze. Maybe open an issue on scipy.

--------

scipy's zeta does x % 4 instead of x % 2, and indeed the zeros are less
accurate. However for x multiple of 4 they are still not good. Maybe it depends
on how they compute the Gamma (Lanczos approximation).

My own implementation of zeta works almost perfectly, if not that at z = -16
the zero breaks. The reason is the 1 - s taken for the reflection. I need to
keep s1 split into -q + a, pass q and a to _hurwitz_zeta_series, and also to
zeta, such that a never changes as it is added to larger integers.

2022-07-25
==========

Some residual inaccuracy in my zeta zeros for somewhat large negative s
(1000 ULP) is totally due to jnp.exp(jspecial.gammaln(s)). I guess it's the
gammaln and not the exp. Open an issue on jax.

2022-07-26
==========

jax.random.poisson seems very slow, open an issue

does scipy's zeta just return 1 above a certain s? It should

Is jax's zeta more accurate than scipy's zeta? Or scipy's hurwitz zeta than
scipy's zeta? Check

2022-07-27
==========

In scipy there ought to be a test that checks that zeta(53) == 1 + eps and not
1 (currently ok).

mpmath.polylog(s, 1) != zeta(s), open an issue.

--------

List of possible issues to open:

- mpmath.polylog(s, 1) != zeta(s)
    => turns out the problem is the exponentiation of 2j * pi, if I pass 1
    directly it works fine, I think it is mathematically correct
- jax.random.poisson seems very slow
- jnp.polyval does not respect integer types, while numpy.polyval does
    => OPEN
- jax.scipy.special.gammaln seems too inaccurate for large negative argument
- scipy.special.binom does not obey https://dlmf.nist.gov/1.2.E6
    => there's an open PR #15216

scipy.zeta:
- does scipy's zeta just return 1 above a certain s?
- Is jax's zeta more accurate than scipy's zeta?
- Is scipy's hurwitz zeta more accurate than scipy's zeta?
- test that checks that scipy.zeta(53) == 1 + eps and not 1
- scipy.zeta zeros are inaccurate
- scipy.zeta inaccurate for large negative s
- hurwitz zeta for s < 1, 0 <= a <= 1

open zeta issues on scipy:
#15036
#14073

2022-08-08
==========

Observations on pdf9:

- one source of problems can be the noisy fake data with grid points near 1,
  since there the fit is forced to return a very small number with small
  uncertainty
- using chol instead of the default eigcut+ removes most of the noise from the
  fake data (make chol default? => need an autorange for the eps in this case)
- increasing the number of datapoints fixes everything and the true
  hyperparameters are recovered correctly.

2022-08-09
==========

To scale the linear PDF fit I need to use Woodsbury's formula because the data
has errors. The true data has a nondiagonal error covariance matrix, so to be
efficient I have to invert the matrix once for the whole optimization. The most
general way to add this feature is passing a decomposition to GP.addcov. To
expose the decompositions, I can add a class method to GP.

2022-08-10
==========

How do I add a user-provided decomposition in GP? Should it be generic, or
only for addcov?

Only in addcov would make the code cleaner probably. There's a single point in
which it is visible that the decomposition is passed along with the original
matrix.

Interface: another single-key dictionary argument with the decompositions.

--------

Instead of saving the decomposition in the _Cov object, save it directly to the
solver cache.

2022-08-11
==========

Write a decomposition class that starts from a whole decomposition and provides
the one for a block. => This is wasteful because the calculations involve the
full matrix. I think that all the decompositions would let me chop off a
submatrix easily. => A method that produces a new decomposition for a submatrix.

--------

gvar bug (ISSUE OPENED):

    In [11]: gvar.var(np.ones(1000))
    ---------------------------------------------------------------------------
    ZeroDivisionError                         Traceback (most recent call last)
    Input In [11], in <cell line: 1>()
    ----> 1 gvar.var(np.ones(1000))

    File _utilities.pyx:900, in gvar._utilities.var()

    ZeroDivisionError: float division

2022-08-14
==========

Notes on implementing automatic solving strategy:

a method _recursive_virtual_solver

has an argument virtual_cache, if it is None (def value for root call) it calls
itself with an empty cache

tries various strategies in hardcoded order, first is naive. Accumulates
computational cost top->down. The non-naive cases stop as soon as the minimum
so far is exceeded. So the first thing that happens is that the completely
naive case is traversed to the end and its cost is determined and works as
first minimum.

no wait, the very first is cache lookup. The cache uses keys in order,
frozenset and reordering to be added later. If successful, cache lookup has
cost zero, else +inf. whenever something is virtually decomposed, it's added to
a copy of the virtual cache for children calls.

the strategy must be traced in a tree that can be executed with actual
operations. the method that does this is _recursive_solver.

2022-08-15
==========

Leave out for now the multiple keys case, since it's combinatorial and thus
more difficult to do efficiently. Just cache it.

Instead of having a _recursive_solve method, I can use a class hierarchy.
Each class has methods to apply virtually or concretely the transformation.

Maybe to start I should just hack woodbury in to make pdf9 work.

2022-08-22
==========

M = B A B^T
B = Q R
M = Q R A R^T Q^T

If B is short, then R is not square, and thus can not be inverted with
solve_triangular.

B = R Q
M = R Q A Q^T R^T

In this case R is square and Q rectangular. Q^T is a pseudoinverse of Q.

M^-1 = R^-T Q A^-1 Q^T R^-1

To obtain the RQ decomposition, use

B^T = Q R
B = (Q R)^T = R^T Q^T

2022-08-23
==========

M^-1 is not a valid pseudoinverse! MM^-1 b != b. M is short so I want MM^-1 = I
even though it's a pseudoinverse.

The culprit is Q. QQ^T = I, but Q^TQ not.

--------

I'm having problems implementing even just woodbury into the automatic
decomposition optimizer.

  - I need to notice, in a generic linear transformation, that one of the
    operands is just summed without changes. If I don't do it, the explicit
    square matrix coefficient, which is the identity, must be fully decomposed
    with QR, making the optimization moot.

  - In general I need to decompose things which are not indexed as covariance
    matrices in the GP. I need something more generic, like an optimizer over
    expressions on p.d. matrices.

I can't afford this much time now. Reasonably quick alternative: allow ycov to
be a decomposition, in which case woodbury is used.

2022-08-24
==========

For pdf9, I can't do woodbury only for single-key data because there are also
the constraints.

2022-08-25
==========

pdf9 with data covariance matrix:

    hyperparameters (true, fitted, prior):
    log(scale)           0.94       0.96(16)      -0.7(1.0)
    scale                 2.6       2.62(43)       0.50(50)
    U(alpha_Sigma)       -1.8       0.09(96)       0.0(1.0)
    alpha_Sigma         -0.46       0.04(38)       0.00(40)
    U(alpha_g)          -0.47      -0.91(25)       0.0(1.0)
    alpha_g             -0.18     -0.319(65)       0.00(40)
    
... and with decomposition of the matrix + woodbury:

    hyperparameters (true, fitted, prior):
    log(scale)           0.94       0.94(16)      -0.7(1.0)
    scale                 2.6       2.55(40)       0.50(50)
    U(alpha_Sigma)       -1.8      -0.16(90)       0.0(1.0)
    alpha_Sigma         -0.46      -0.06(35)       0.00(40)
    U(alpha_g)          -0.47      -0.93(24)       0.0(1.0)
    alpha_g             -0.18     -0.324(63)       0.00(40)

It works, but they are too different. In the woodbury case, minimize complained
that "minimization failed: A bad approximation caused failure to predict
improvement."

Also, it worked only with method='hessian'; 'fisher' fails completely, while
'gradient' botches the covariance (while tipically the bfgs inverse hessian
is decent).

What could be the culprit(s)? Maybe the sandwiched covariance being not
exactly but quite degenerate is a problem for Woodbury? Or derivatives
through Woodbury are broken? (If so, why would the linalg tests not notice?)

w/o woodbury and all decomps are eigcut-:

    hyperparameters (true, fitted, prior):
    log(scale)           0.94       0.95(13)      -0.7(1.0)
    scale                 2.6       2.57(33)       0.50(50)
    U(alpha_Sigma)       -1.8      -0.15(81)       0.0(1.0)
    alpha_Sigma         -0.46      -0.06(32)       0.00(40)
    U(alpha_g)          -0.47      -0.74(17)       0.0(1.0)
    alpha_g             -0.18     -0.271(51)       0.00(40)

w/ woodbury and all decomps are eigcut-:

    hyperparameters (true, fitted, prior):
    log(scale)           0.94       0.97(13)      -0.7(1.0)
    scale                 2.6       2.63(35)       0.50(50)
    U(alpha_Sigma)       -1.8      -0.17(99)       0.0(1.0)
    alpha_Sigma         -0.46      -0.07(39)       0.00(40)
    U(alpha_g)          -0.47      -0.95(22)       0.0(1.0)
    alpha_g             -0.18     -0.330(54)       0.00(40)

and the minimizer complains as above.

Conclusion: it was not stable even without woodbury, the solver changes the
result. Maybe I should increase the eps?

w/o woodbury and all decomps are eigcut- and eps=1e-10:

    hyperparameters (true, fitted, prior):
    log(scale)           0.94       0.95(13)      -0.7(1.0)
    scale                 2.6       2.57(33)       0.50(50)
    U(alpha_Sigma)       -1.8      -0.15(81)       0.0(1.0)
    alpha_Sigma         -0.46      -0.06(32)       0.00(40)
    U(alpha_g)          -0.47      -0.74(17)       0.0(1.0)
    alpha_g             -0.18     -0.271(51)       0.00(40)

w/ woodbury and all decomps are eigcut- and eps=1e-10:

    hyperparameters (true, fitted, prior):
    log(scale)           0.94   1 +- 8.1e+04      -0.7(1.0)
    scale                 2.6   3 +- 2.1e+05       0.50(50)
    U(alpha_Sigma)       -1.8  -0.2 +- 3e+03       0.0(1.0)
    alpha_Sigma         -0.46-0.07 +- 1.2e+03       0.00(40)
    U(alpha_g)          -0.47  -2 +- 5.2e+06       0.0(1.0)
    alpha_g             -0.18-0.5 +- 3.4e+05       0.00(40)

Broken!

Next step: test woodbury with unit tests to check at least it works without
degeneracies. Test all possible relevant structures of transformations.

2022-08-30
==========

Next things that I could do:
 - understand the inaccuracy of woodbury
 - add variance hp to pdf9

--------

Summary of observations on woodbury's inaccuracy from the unit test:

  - happens with either degenerate A or C (in A + B C B^T)
  - tall B (i.e., degerate B C B^T) is not a problem
  - gets worse with chol and even more with eigcut-

Here I'm measuring "inaccuracy" with the matrix relative 2-norm between the
inverse computed on the dense matrix and with woodbury, where the reference
matrix for the norm is the first. Is this a relevant measure of inaccuracy? Is
the problem that woodbury does not constitute a moore-penrose pseudo-inverse,
while the dense decompositions do?

Can't be exactly this, because with nonsingular A and degenerate C, the whole
matrix is nonsingular, so I'm computing a true inverse.

Woodbury identity:

    (A + BCB^T)^-1 = A^-1 - A^-1 B (C^-1 + B^T A^-1 B)^-1 B^T A^-1

Wikipedia gives a version for singular C:

    = A^-1 - A^-1 B (I + C B^T A^-1 B)^-1 C B^T A^-1

I can write a new woodbury and test if this solves at least the problems with
singular C. A will be more difficult I fear.

Also, this formula does not require to decompose C. It seems strictly better
than normal woodbury even if C was invertible. What's the catch? Why isn't it
the standard one then? => See 2022-09-06.

--------

The formula for the derivative of A^-1 does not generalize to A^+
https://en.wikipedia.org/wiki/Moore–Penrose_inverse#Derivative

2022-08-31
==========

For nonsingular C and singular A, see Bernstein 2018, Fact 8.4.13 p. 635. But
it works only if the range of B is contained in the range of A. Does it work
for singular C by moving it like in the formula above?

--------

List of useful things from Bernstein 2018:
    
    Fact 8.4.13 p. 635: (A + BCB^t)^+ with singular A, nonsingular C, rB in rA
    Fact 8.4.33 p. 638: (AA^t + BB^t)^+
        Consider Israel and Greville 2003, eq. 1.17 p. 262, for (CC^t)^+
    Fact 8.9.30-31 p. 667: [A B; B^t 0]^+
    Fact 8.12.4 p. 678: [A B; C D]^D
    Proposition 10.2.1: LDLT of pos sdef [A B; B^t C] with singular A and/or C
    Fact 10.9.4 p. 725: det and quad of Wiener covariance matrix
    Fact 10.9.5 p. 725: list of 10 covariance functions
    Fact 10.9.6 p. 726: det and inverse of Wiener on regular grid
    Fact 10.9.7 p. 726: list of 18 complex covariance functions
    Fact 10.9.8 p. 727: list of 6 covariance functions on integers
    Fact 10.9.10-10.9.15 p. 727-728: other covariance functions
    Fact 10.9.18 p. 729: a pd-preserving matrix transformation
    Fact 10.24.2 p. 815: A^+ = A^# = A^D for psd A

Question: for pos sdef A, is it true that A^+ = lim x->0 (A + Ix)^-1?

=> Nope, it's false. The correct limit (for sym A) is

    A^+ = lim x->0 A (A^2 + xI)^-1

So maybe when I do cholesky I should compute A^2, regularize, decompose.
Problem: this formula is not symmetric (=> no (de)correlate), and A^2 is worse
conditioned than A. Can I write the limit as L^T (A^2 + xI)^-1 L with A = LL^T?
=> still bad because I need to decompose A before reg, so not useful. Maybe A
(A^3 + xI)^-1 A works? But no decorrelate still.

And the logdet?

See https://en.wikipedia.org/wiki/Pseudo-determinant

Probably I should not sum all eigenvalues in eigcut+ and svdcut+, but just
the unregulated ones.

2022-09-04
==========

jax 0.3.16 introduces pure_callback, to use python functions within
jit-compiled ones. Coupled with custom_jvp, I think I can use it to quickly add
jit support for some kernels until I implement them in JAX, Matérn in
particular

2022-09-06
==========

The formula

    (A + BCB^T)^-1 = A^-1 - A^-1 B (I + C B^T A^-1 B)^-1 C B^T A^-1

is not directly usable because C B^T A^-1 B is not symmetric in general.
Perhaps I can do something with the outer product decomposition of C:

    C = LL^T (not necessarily cholesky)
    
    (A + BCB^T)^-1
    = A^-1 - A^-1 B (C^-1 + B^T A^-1 B)^-1 B^T A^-1
    = A^-1 - A^-1 B I (C^-1 + B^T A^-1 B)^-1 I B^T A^-1
    = A^-1 - A^-1 B L L^-1 (C^-1 + B^T A^-1 B)^-1 L^-T L^T B^T A^-1
    
        X^-1 Y^-1 Z^-1 = (ZYX)^-1
        
    = A^-1 - A^-1 B L (L^T C^-1 L + L^T B^T A^-1 B L)^-1 L^T B^T A^-1
    
        C^-1 = L^-T L^-1

    = A^-1 - A^-1 B L (L^T L^-T L^-1 L + L^T B^T A^-1 B L)^-1 L^T B^T A^-1
    = A^-1 - A^-1 B L (I + L^T B^T A^-1 B L)^-1 L^T B^T A^-1

Problem: if C is not full-rank, then L is tall, L L^+ != I, and the above proof
does not work replacing L^-1 with L^+. However, since the final expression
does not involve L^+, it could be that it still works. Let's check:

    (A + BCB^T) (A^-1 - A^-1 B L (I + L^T B^T A^-1 B L)^-1 L^T B^T A^-1) =
    
    = A A^-1 +
    - A A^-1 B L (I + L^T B^T A^-1 B L)^-1 L^T B^T A^-1 +
    + B C B^T A^-1 +
    - B C B^T A^-1 B L (I + L^T B^T A^-1 B L)^-1 L^T B^T A^-1
    
    = I +
    - B L (I + L^T B^T A^-1 B L)^-1 L^T B^T A^-1 +
    + B C B^T A^-1 +
    - B C B^T A^-1 B L (I + L^T B^T A^-1 B L)^-1 L^T B^T A^-1
    
    = I +
    - B L I (I + L^T B^T A^-1 B L)^-1 L^T B^T A^-1 +
    + B C B^T A^-1 +
    - B L L^T B^T A^-1 B L (I + L^T B^T A^-1 B L)^-1 L^T B^T A^-1
    
    = I +
    - B L (I + L^T B^T A^-1 B L) (I + L^T B^T A^-1 B L)^-1 L^T B^T A^-1 +
    + B C B^T A^-1

    = I +
    - B L L^T B^T A^-1 +
    + B C B^T A^-1

    = I +
    - B C B^T A^-1 +
    + B C B^T A^-1

    = I
    
It works!

=> 2022-11-19 I'm dumb: this is just a particular case of normal Woodbury.

2022-09-07
==========

However to use it I need the operation X -> X L or X -> L^T X. Currently I only
have X -> L X and X -> L^-1 X.

2022-09-10
==========

Other problem: I can't have custom derivatives for X -> L^T X, and thus
derivatives for the whole decomposition.

Idea: in decompautodiff, instead of assuming that K is the first positional
argument, call a method that takes all the arguments and produces K. The
default implementation returns the first argument, however subclasses can
customize it. Woodbury would return A ± BCB^T.

This means I need a method `matrix()` that returns the decomposed matrix.
decompautodiff provides its implementation, like it does for n().

--------

For Woodbury with singular A, nonsingular C, singular M: see Riedel 1992:

    M = A + BCB^T, with C smaller than A
    
    B = X + Y, where
        im X is in im A
        im Y is orthogonal to im A
        Y is full rank
    
    Z = Y^+ = (Y^T Y)^-1 Y^T
    
    M^+ = A^+ - Z^T X^T A^+ - A^+ X Z + Z^T (C^-1 + X^T A^+ X) Z

How do I split B into X + Y? QR of B?

    B = QR

(B is tall, so Q is tall and R is square)

No wait I need the space of A, not B. Let P_A be the orthogonal projector into
im A, then

    X = P_A B
    Y = (I - P_A) B
    
    X + Y = P_A B + (I - P_A) B = B => ok
    P_A B in im A => ok
    P_A (I - P_A) B = P_A B - P_A B = 0 => ok
    
To compute P_A, I need to diagonalize A.

Add methods improj and nullproj to Decomposition. The cholesky decompositions
work by assuming pos def matrix so they will implement identity and zero, while
diagonalizations work. There shouldn't be undefined cases. Composite
decompositions may have problems as usual.

Note: since Y must be full rank and be orthogonal to im A, it must be
rank A <= size A - size C. This is very limiting. <======== *******

--------

things to do:

    - add composite decomposition support to decompautodiff (DONE)
    - apply it to all existing CDs and rerun tests (DONE)
    - add option "transpose" to correlate, test it (DONE)
    - write Woodbury2 for singular C and test it (DONE)
    - write Woodbury3 using Bernstein 8.4.33 for singular A and C

--------

Bernstein 8.4.33:

    (AAt + BBt)+ = [A+ (I - BC+)]t E A+ (I - BC+) + (CCt)+
    
    C = (I - AA+) B
    
    E = I - A+B (I - C+C) [I + (I - C+C) Bt (AAt)+ B (I - C+C)]-1 (A+B)t

The BBt term corresponds to the first term in woodbury, since
    
    CCt = (I - AA+) BBt (I - AA+)

Many terms are projectors. I suspect BC+ is a projector too. I need to
check if projectors are respected by the pseudoinverse.

Hypothesis: A = PX, A+ = X+P

Properties to check:

    1   AA+A = A
    2   A+AA+ = A+
    3   (AA+)t = AA+
    4   (A+A)t = A+A

 1) PX X+P PX = PXX+PX = ?

Nope I can't get PX out of it, I would need to get XX+X without the P in
between.

What I can say with projectors is that (PX)+P = (PX)+.

What about PXP? Is it (PXP)+ = PX+P?

 1) PXP PX+P PXP = PXPX+PXP
 
=> nope

Problem: E does not seem to be hermitian. What's up? => It's an error,
backfixed now. Correct reference: Schott 2017, th. 5.15.

Other problem: even though I think BBt aesthetically corresponds to the first
term in woodbury, actually in practice it's AAt, because it's the one whose
inverse I can precompute. Check of feasibility: assume that B is tall. C has
the same shape as B, so CCt is large. The term [I + ...] is small because
the sandwich contains C+C. I have to invert CC+ every time and it's large.
However, since C is tall, maybe it's efficient.

I try with the SVD of C:

    C = UDVt
    Ct = VDUt
    CCt = UDVt VDUt = U D^2 Ut
    (CCt)+ = U D^2+ Ut

It works.

Question: is (AAt)+ = A+tA+?

 1) AAt A+t A+ AAt =
    A (A+A)t A+A At =   (A+A is hermitian)
    A (A+A)^2 At =      (A+A is idempotent)
    A A+A At =          (property 1 of A+)
    A At

Ok it's on wikipedia, I skip the whole check.

Question: can I replace all separate occurrences of A with AAt and (AAt)+,
such that I can use the decomposition of AAt without correlate/decorrelate?

    (AAt + BBt)+ = [A+ (I - BC+)]t E A+ (I - BC+) + (CCt)+
    C = (I - AA+) B
    E = I - A+B (I - C+C) [I + (I - C+C) Bt (AAt)+ B (I - C+C)]-1 (A+B)t

First I address the projector AA+. Does this project on the image of
X = AAt?

    AA+ X = AA+A At = AAt = X

This means that AA+ projects on something containing the image of X. I need to
check that the intersection with the kernel is 0.

    X AA+ = AAt AA+

No wait not the kernel of X, the orthogonal complement of the image of X:

    X+ = (AAt)+ = A+t A+
    X+ AA+ = A+t A+AA+ = A+t A+ = X+
    XX+ = AAt A+t A+ = A (A+A)t A+ = A A+A A+ = AA+

    (I - XX+) AA+ = AA+ - X X+AA+ = AA+ - XX+ = 0  => ok

So I can use the projector of X instead of the one of A. Wait it was way
easier: XX+ = AA+. Whatever.

    C = (I - XX+) B
    
    (X + BBt)+ = (CCt)+ + (I - BC+)t A+t E A+ (I - BC+) =
        = (CCt)+ + (I - BC+)t A+t [I - A+B (I - C+C) [I + (I - C+C) Bt (AAt)+ B (I - C+C)]-1 (A+B)t] A+ (I - BC+) =
    
            M := (I - C+C) [I + (I - C+C) Bt (AAt)+ B (I - C+C)]-1
               = (I - C+C) [I + (I - C+C) Bt X+ B (I - C+C)]-1
               = (I - C+C) [I + (I - C+C) Bt X+ B (I - C+C)]-1 (I - C+C)

        = (CCt)+ + (I - BC+)t A+t [I - A+B M (A+B)t] A+ (I - BC+) =
        = (CCt)+ + (I - BC+)t A+t A+ (I - BC+) - (I - BC+)t A+t A+B M (A+B)t A+ (I - BC+) =
        = (CCt)+ + (I - BC+)t (AAt)+ (I - BC+) - (I - BC+)t A+t A+ B M Bt A+t A+ (I - BC+) =
        = (CCt)+ + (I - BC+)t (AAt)+ (I - BC+) - (I - BC+)t (AAt)+ B M Bt (AAt)+ (I - BC+) =
        = (CCt)+ + (I - BC+)t X+ (I - BC+) - (I - BC+)t X+ B M Bt X+ (I - BC+)

Now I have to replace BBt with VYVt.
    
    BBt = VYVt
    Y = LLt (not necessarily cholesky)
    B = VL
    CCt = (I - XX+) BBt (I - XX+) = (I - XX+)V Y Vt(I - XX+)

2022-09-17
==========

The projectors of the kind I - UU+ are tipically very low-rank. For
decompositions which assume the matrix is pd (like cholesky), the projector
is zero i.e. a product of empty vectors.

How should I implement them? Alternatives:

  - Produce them manually
  - Introduce new methods in Decomposition

In the second case:

  - Return a decomposition of the projector
  - Provide a quad/solve operation

The projectors appearing in the formula are:

  * I - XX+
  * I - C+C
  * I - BC+

Since C is rectangular, only the first can be obtained from a Decomposition.
For the other two I have to use the svd manually.

    (CCt)+(CCt) = Ct+ C+ C Ct =
                = Ct+ (C+C) Ct =
                = Ct+ (C+C)t Ct =
                = Ct+ Ct Ct+ Ct =
                = (CC+CC+)t =
                = (CC+)t =
                = CC+

I can't obtain C+C from CCt. It's a smaller matrix.

    C = (I - XX+) B =
      = B - XX+B ->
      
      -> B = C + XX+B
    
    I - BC+ = I - (C + XX+B)C+ =
            = I - CC+ - XX+BC+

2022-09-18
==========

Other thoughts on pinv with cholesky: The limit form of the pseudoinverse is

    A+ = lim x->0 A (A^2 + xI)^-1

(for symmetric A). Maybe this works in practice because: A^2 is more
ill-conditioned, so the low eigenvalues are more "buried" beneath x. But I
still multiply by A so the approximate kernel is respected, without being
counterbalanced with a multiplication by a large number.

Defect: I want a symmetric formula for correlate & decorrelate.

    A = LLt
    
    A2 = LLt LLt
    
    A+ =? lim x->0 Lt (A2 + xI)^-1 L

    A+(x) A A+(x) = Lt (A2 + xI)-1 L LLt Lt (A2 + xI)-1 L

Dunno, let's first check with the inverse:

    A-1 = Lt-1 L-1
    
    L-1 A-1 L = L-1 Lt-1 L-1 L = L-1 Lt-1 => nope
    
    X A-1 Y = A-1
    (X Lt-1) (L-1 Y) = Lt-1 L-1

It's impossible. The other idea was

    A+(x) = A (A3 + xI)-1 A
    
    A+(x) A A+(x) = A (A3 + xI)-1 A3 (A3 + xI)-1 A
    
    A A+(x) A = A2 (A3 + xI)-1 A2 = A3 (A3 + xI)-1 A

In the limit xI (A3 + xI)-1 produces something living in the kernel/cokernel
of A so it's zero as long as it's multiplied by A. A commutes with A3 + xI and
so also with (A3 + xI)-1.

    (AA+(x))t = (A2 (A3 + xI)-1 A)t =
              = (A3 (A3 + xI)-1)t =
              = (A3 + xI)-1 A3 =
              = A2 (A3 + xI)-1 A =
              = AA+(x)

    Operation       Cost [matmul]
    A3              3
    chol(A3 + xI)   1
    L-1 A           1

Usage:

    A3 + xI = LLt
    
    A+ = A (A3 + xI)-1 A =
       = A Lt-1 L-1 A =         (solve)
       = (L-1 A)t (L-1 A)       (quad, decorrelate)
      
    A = A A+ A =
      = A (L-1 A)t (L-1 A) A =
      = (L-1 A2)t (L-1 A2)      (correlate)
    
    det(A3) = det(A)^3
    logdet(A) = 6 logdet L      (logdet)

Question: should I premultiply L-1 by A? Or do it after applying A? If the
operand is a vector/tall matrix, applying L-1 only at the end is more
convenient.

On the other hand I should also care about numerical accuracy. Maybe
keeping L-1 last means that the irregularity prevails because A cannot enforce
its kernel beyond floating point accuracy.

--------

On the logdet of eigcut+/-: maybe I should compute the logdet of eigcut-
by considering all the missing dimensions at the threshold instead of removing
them, for continuity.

Choices:

    - pdet: the marginal likelihood is in some sense more correct
    - det: the marginal likelihood is continuous w.r.t. hyperparameters

Probably the second is more important. => If needed I can make a separate
method logpdet.

--------

How do I compute the logdet with the pinv version of woodbury?

2022-10-31
==========

New things since v0.12:

- Fourier -> Zeta
- GP.decompose
- GP.addcov(decomps=...)
- GP.pred and GP.marginal_likelihood accept a decomposition as y error
  covariance, and use it with woodbury
- lsqfitgp.sample
- empbayes_fit(initial=...)
- empbayes_fit(verbosity=...)
- empbayes_fit.pmean, .pcov, .minargs
- MA(norm=...)
- BART.correlation
- BART optimization for last two levels
- fixed bugs with new jax versions

2022-11-01
==========

There are many serious bugs due to jax updates.

GP prediction with gvars does not work anymore. The cause is that custom_jvp
functions now can not return object dtypes. I think the only case where this is
a problem is in quad_autodiff, so I can just call oldquad if c's dtype is
object. => seems solved.

GammaExp(gamma=2) is not differentiable at distance 0. I guess that now
differentiating x ** 1 at x = 0 gives nan. => the second derivative, not the
first.

--------

Github actions is still emitting warnings. They are due to
JamesIves/github-pages-deploy-action@v4.3.3 and actions/setup-python@v3, I have
to try changing the version.

2022-11-05
==========

After the last update to jax (0.3.24), I've noticed the following xpasses in
linalg tests:

tests/test_linalg.py::TestBlockDiag::test_solve_vec_jac_rev PASSED                                                                 [ 57%]
tests/test_linalg.py::TestBlockDiag::test_solve_matrix_jac_rev PASSED                                                              [ 59%]
tests/test_linalg.py::TestBlockDiag::test_solve_vec_jac_rev_jit PASSED                                                             [ 60%]
tests/test_linalg.py::TestBlockDiag::test_solve_matrix_jac_rev_jit PASSED                                                          [ 61%]
tests/test_linalg.py::TestBlockDiag::test_solve_matrix_jac_rev_matrix PASSED                                                       [ 62%]
tests/test_linalg.py::TestBlockDiag::test_quad_vec_jac_rev PASSED                                                                  [ 63%]
tests/test_linalg.py::TestBlockDiag::test_quad_matrix_jac_rev PASSED                                                               [ 65%]
tests/test_linalg.py::TestBlockDiag::test_quad_vec_jac_rev_jit PASSED                                                              [ 66%]
tests/test_linalg.py::TestBlockDiag::test_quad_matrix_jac_rev_jit PASSED                                                           [ 67%]
tests/test_linalg.py::TestBlockDiag::test_logdet_jac_rev PASSED                                                                    [ 68%]
tests/test_linalg.py::TestBlockDiag::test_logdet_jac_rev_jit PASSED                                                                [ 69%]

tests/test_linalg.py::TestSandwichSVDDiag::test_solve_matrix_hess_fwd_rev PASSED                                                   [ 77%]

tests/test_linalg.py::TestWoodburyDiag::test_solve_vec_jac_rev PASSED                                                              [ 84%]
tests/test_linalg.py::TestWoodburyDiag::test_solve_matrix_jac_rev PASSED                                                           [ 85%]
tests/test_linalg.py::TestWoodburyDiag::test_solve_vec_jac_rev_jit PASSED                                                          [ 86%]
tests/test_linalg.py::TestWoodburyDiag::test_solve_matrix_jac_rev_jit PASSED                                                       [ 87%]
tests/test_linalg.py::TestWoodburyDiag::test_solve_matrix_jac_rev_matrix PASSED                                                    [ 89%]
tests/test_linalg.py::TestWoodburyDiag::test_quad_vec_jac_rev PASSED                                                               [ 90%]
tests/test_linalg.py::TestWoodburyDiag::test_quad_matrix_jac_rev PASSED                                                            [ 91%]
tests/test_linalg.py::TestWoodburyDiag::test_quad_vec_jac_rev_jit PASSED                                                           [ 92%]
tests/test_linalg.py::TestWoodburyDiag::test_quad_matrix_jac_rev_jit PASSED                                                        [ 93%]
tests/test_linalg.py::TestWoodburyDiag::test_logdet_jac_rev PASSED                                                                 [ 95%]
tests/test_linalg.py::TestWoodburyDiag::test_logdet_jac_rev_jit PASSED                                                             [ 96%]

Are they new? Do they still fail in old jax versions? In which version did they
stop failing? Should I unmark them and require jax >= 0.3.24? Or wait to see
if they remain xpasses with further updates or regress back? => They are still
ok in jax 0.3.25, but let's wait until 0.4 which should be coming soon.
=> turns out they work already in 0.3.17, I removed the marks.

2022-11-06
==========

I think I have found a jax bug: jnp.mean(..., where=...) return wrong result
under jit, should be reproducible with 1-length arrays and all go mask.

2022-11-20
==========

In the eigenvalue truncating decompositions, add the missing epsilons to the
determinant to make it smooth.

2023-01-28
==========

Idea: to make pdf9 work, instead of implementing Woodbury3, I can condition on
the constraints, obtain all the covariance matrices of data + plot, put back
these matrices into the GP with addcov, then condition these on the data. The
first step is doable because I condition on a small number of values; the
second is numerically ok because the error matrix is not degenerate, and even
if the constrained prior is very ill-conditioned, that can be handled by
Woodbury2.

This should be equivalent to using Block. Think it through. I could make it
automatic by grouping the given keys with zero error.

2023-01-29
==========

Degenerate Woodbury (copied from 2022-09-10) is

    C = (I - XX+) B
    
    M = (I - C+C) [I + (I - C+C) Bt X+ B (I - C+C)]-1 (I - C+C)

    (X + BBt)+ = (CCt)+ + (I - BC+)t X+ (I - BC+) - (I - BC+)t X+ B M Bt X+ (I - BC+)

............

If X = 0, then X+ = 0, so it becomes

    C = B
    
    M = (I - B+B) [I + (I - B+B) Bt X+ B (I - B+B)]-1 (I - B+B) =
      = (I - B+B) [I + Bt X+ B]-1 (I - B+B) =
      = (I - B+B) I (I - B+B) =
      = I - B+B

    (X + BBt)+ = (BBt)+ + (I - BB+)t X+ (I - BB+) - (I - BB+)t X+ B M Bt X+ (I - BB+) =
               = (BBt)+ - (I - BB+) X+ B (I - B+B) Bt X+ (I - BB+) =
               = (BBt)+ - (I - BB+) X+ BBt X+ (I - BB+) =
               = (BBt)+

                                                                        g.a.c.

............

I try a simplification

    (I - C+C) Bt = (I - C+(I - XX+) B) Bt =
                 = Bt - C+ (I - XX+) BBt

nope, does not help

............

    C = (I - XX+) B
    
    M = (I - C+C) [I + (I - C+C) Bt X+ B (I - C+C)]-1 (I - C+C)

    (X + BBt)+ = (CCt)+ + (I - BC+)t X+ (I - BC+) - (I - BC+)t X+ B M Bt X+ (I - BC+)
    
I rewrite it in a slightly different way:

    C = (I - XX+) B
    Z = B (I - C+C)
    W = [I + Zt X+ Z]-1
    (X + BBt)+ = (CCt)+ + (I - BC+)t X+ (I - BC+) - (I - BC+)t X+ Z W Zt X+ (I - BC+)
    (CCt)+ = C+t C+
    
How do I evaluate it in practice?

  - the matrix to be decomposed is X + VYVt
  - inputs: decomposition of X, decomposition of Y, matrix V
  - the decomposition of X must support ill-conditioning, the one of Y not
  - B = Y.correlate(V.T, transpose=True).T
  - C = X.kerproj(B) (XX+ = X+X because X is hermitian, so no ambiguity)
  - compute the thin svd of C           <----- expensive
  - compute C+ (it's tall so it's a "fast" matmul)
  - Z = B (I - C+C)
  - W = I + X.quad(Z)
  - decompose W                         <----- expensive (much less than C svd because I can use Cholesky)

These are the initialization computations. To compute a quadratic form bt A c:

  - compute C+b and C+c
  - compute b' = (I - BC+)b if b is short, b' = b - B(C+b) if b is tall
  - analogously for c'
  - X.quad(b', c')
  - W.quad(X.quad(Z, b'), X.quad(Z, c'))

I have to implement kerproj in Decomposition.

Can I accelerate the computation of C+? If C if full column rank, I can use
Cholesky on (CtC)^-1:

    C+ = (CtC)^-1 Ct

This won't work if C is not maximum rank, but maybe it does if I replace the
inverse with a pseudoinverse?

    C' = (CtC)+ Ct
    
    CC'C = C (CtC)+ CtC =
         = C C+ C+t Ct C =
         = CC+ (CC+)t C =
         = CC+ CC+ C =
         = CC+C
         = C
    
    etc.

I also found it on Wikipedia. This means I can use my cholesky-pseudoinverse
trick to approximate (CtC)+.

    C+C = (CtC)+ CtC = image projector of CtC
    I - C+C = kernel projector of CtC

Other alternative to SVD which should be equivalent though: diagonalize
CtC to compute (CtC)+. Diagonalization is twice as fast as SVD.

Paper on Cholesky pinv: jhurani2012

............

Xpasses on CI #208:

XPASS tests/test_fit.py::test_period
XPASS tests/test_kernels.py::TestWendland::test_positive_deriv2
XPASS tests/test_linalg.py::TestWoodbury2_EigCutFullRank::test_solve_matrix_jac_fwd_da
XPASS tests/test_linalg.py::TestWoodbury2_EigCutFullRank::test_quad_matrix_matrix_jac_fwd_da
XPASS tests/test_linalg.py::TestWoodbury2_EigCutFullRank::test_quad_matrix_matrix_hess_da
XPASS tests/test_linalg.py::TestWoodbury2_EigCutFullRank::test_logdet_hess_da

XPASS tests/test_kernels.py::TestWendland::test_jit_deriv2_nd
XPASS tests/test_kernels.py::TestWendland::test_positive_deriv2
XPASS tests/test_kernels.py::TestWendland::test_jit_deriv_nd
XPASS tests/test_linalg.py::TestWoodbury2_EigCutFullRank::test_solve_matrix_jac_fwd_da
XPASS tests/test_linalg.py::TestWoodbury2_EigCutFullRank::test_quad_matrix_matrix_jac_fwd_da
XPASS tests/test_linalg.py::TestWoodbury2_EigCutFullRank::test_quad_matrix_matrix_hess_da

XPASS tests/test_fit.py::test_method
XPASS tests/test_kernels.py::TestWendland::test_positive_deriv2
XPASS tests/test_kernels.py::TestWendland::test_positive_deriv_nd
XPASS tests/test_linalg.py::TestWoodbury2_EigCutFullRank::test_solve_matrix_jac_fwd_da
XPASS tests/test_linalg.py::TestWoodbury2_EigCutFullRank::test_solve_matrix_hess_da
XPASS tests/test_linalg.py::TestWoodbury2_EigCutFullRank::test_quad_matrix_matrix_jac_fwd_da
XPASS tests/test_linalg.py::TestWoodbury2_EigCutFullRank::test_logdet_hess_da

XPASS tests/test_fit.py::test_method
XPASS tests/test_kernels.py::TestWendland::test_jit_deriv2_nd
XPASS tests/test_kernels.py::TestWendland::test_positive_deriv2
XPASS tests/test_kernels.py::TestWendland::test_jit_deriv_nd
XPASS tests/test_linalg.py::TestWoodbury2_EigCutFullRank::test_solve_matrix_jac_fwd_da
XPASS tests/test_linalg.py::TestWoodbury2_EigCutFullRank::test_quad_matrix_matrix_jac_fwd_da
XPASS tests/test_linalg.py::TestWoodbury2_EigCutFullRank::test_quad_matrix_matrix_hess_da

Union:

XPASS tests/test_fit.py::test_method  ==>  old xfail, no longer applies
XPASS tests/test_fit.py::test_period  ==>  could reasonably happen by chance
XPASS tests/test_kernels.py::TestWendland::test_jit_deriv2_nd  ==>  new xpass
XPASS tests/test_kernels.py::TestWendland::test_jit_deriv_nd  ==>  new xpass
XPASS tests/test_kernels.py::TestWendland::test_positive_deriv2  ==> from here on, known xpasses
XPASS tests/test_kernels.py::TestWendland::test_positive_deriv_nd
XPASS tests/test_linalg.py::TestWoodbury2_EigCutFullRank::test_logdet_hess_da
XPASS tests/test_linalg.py::TestWoodbury2_EigCutFullRank::test_quad_matrix_matrix_hess_da
XPASS tests/test_linalg.py::TestWoodbury2_EigCutFullRank::test_quad_matrix_matrix_jac_fwd_da
XPASS tests/test_linalg.py::TestWoodbury2_EigCutFullRank::test_solve_matrix_hess_da
XPASS tests/test_linalg.py::TestWoodbury2_EigCutFullRank::test_solve_matrix_jac_fwd_da

............

How do I generate a degenerate random Toeplitz matrix? Is the product
circulant x toeplitz still toeplitz?

    C_ij = C_i+k mod n,j+k mod n
    T_ij = T_i+k,j+k
    
    (CT)_i+k,j+k =
        = ∑ l=1^n  C_i+k,l T_l,j+k =
        = ∑ l=1^n  C_i+k,l T_l,j+k

Problem: I can not shift l in T because it runs from 1 to n.

Found on Wikipedia: a p.s.d. Toeplitz matrix diagonalizes with a Vandermonde
matrix:

    V_ij = e^(i2π (i-1) f_j)
    T = VDV*
    T_ij = V_i+k,l D_l V_j+k,l =
         = e^(i2π (i+k-1 - (j+k-1)) f_l) D_l =
         = e^(i2π (i-j) f_l) D_l

Now I want T real. I can't just take the real part because the rank changes
(a cosine can not be split as a product like the exponential). I have to
choose f and D to get a real matrix. For each f ≠ 0, -f must appear. Use f=0
to pad if odd.

    T_ij = D_0 + (e^(i2π (i - j) f_l) + e^(i2π (i - j) -f_l)) D_l/2 =
         = D_0 + cos(2π (i - j) f_l)

The rank is #D/2.

............

To use my decompositions with rectangular matrices, I can define a class
that decomposes AtA or AAt with a symmetric decomp.

2023-01-30
==========

In particular, to decompose a non p.s.d. symmetric matrix A with cholesky, do

    A^-1 = A(A^2)^-1.

2023-02-07
==========

jhurani2012 gives

    K+ = ∑ n=1^∞ Kt εⁿ⁻¹ (KKt + εI)⁻ⁿ,

And I tested it in playground/pinv/diagpinv.py. Is it also, for symmetric K,

    K+ = ∑ n=0^∞ εⁿ K (K³ + εI)⁻ⁿ⁻¹ K ?

Let's try with the MP identities:

    f(K) := ∑ n=1^∞ K (I + K^3)^-n K
    
    K f(K) K = ∑ n=1^∞ K^2 (I + K^3)^-n K^2 =
    
K commutes with I and K^3, so with I + K^3, so with (I + K^3)^-n, so
    
             = ∑ n=1^∞ K^3 (I + K^3)^-n K =
             =: g(K) K
    
    g(K) = K^3 (I + K^3)^-1 + g(K) (I + K^3)^-1 -->
    --> g(K) (I + K^3) = K^3 + g(K) -->
    --> g(K) K^3 = K^3 -->
    --> g(K) K = K  ==>  ok

Second identity: K commutes with f(K), so f(K) K = K f(K), so

    f(K) K f(K) = K f(K) f(K) =
                = ∑ n=1^∞ K f(K) K (I + K^3)^-n K =
                = ∑ n=1^∞ K (I + K^3)^-n K =
                = f(K)  ==>  ok

Third and fourth identity: they hold because K commutes with f(K).

    f(K/ε^1/3) = ∑ n=1^∞ K/ε^1/3 (I + K^3/ε)^-n K/ε^1/3 =
               = ∑ n=1^∞ ε^-2/3 K ε^n (εI + K^3)^-n K =
               = ε^1/3 K+   -->
    
    -->  K+ = ∑ n=1^∞ K ε^(n-1) (εI + K^3)^-n K                         q.e.d.

To do next: implement this as _decomp.Pinv2.

2023-02-10
==========

For Woodbury3, to compute the pseudodeterminant, I can use pdet(AA') =
pdet(A'A) = det(A'A), if A'A is invertible. A is the matrix that I use for
correlate etc. But how do I compute det(A'A)? => Not applicable: A'A is larger
than AA' for Woodbury.

2023-02-12
==========

Idea: maybe the usual formula det(A + BB') = det(A)det(I - B'A⁻¹B) works for
pdet in the special case of ker A in ker B?

............

I'm dumb. I'm losing all this time with Woodbury, I still don't know how to
compute the pseudodeterminant, and I didn't notice that my new computer churns
through 3000 datapoints in a few seconds. All my design was based on my old
low-end 2011 laptop. Crap. At least I've learned a lot about pseudoinverses.

............

How can I compute tr(A⁺∂A) in O(n^2)? If I have A⁺ as a matrix, it's

    ∑_ij A⁺_ij ∂A_ji = sum(A⁺ * ∂A)

But if I have A = LL', and I don't want to form the inverse explicitly for
numerical accuracy reasons, then what?

    K = AA'
    K⁺ = A⁺'A⁺
    tr(K⁺∂K) =
        = tr(A⁺'A⁺∂K) =
        = tr(A⁺∂KA⁺')

    tr(ABA') =
        = A_ij B_jk A_ik
        = A_ij A_ik B_jk

Forming the inverse would be O(n^3), anyway.

In a triangular solve, can I speed up getting only the diagonal? It seems not.

2023-02-13
==========

pdf10.py with epsabs=1e-10, epsrel=0:

    hyperparameters (true, fitted, prior):
    log(scale)          -0.49      -0.09(10)      -0.7(1.0)
    scale                0.61      0.911(93)       0.50(50)
    U(alpha_Sigma)      0.041       0.14(20)       0.0(1.0)
    alpha_Sigma         0.016      0.055(80)       0.00(40)
    U(alpha_g)           -2.2      -2.22(40)       0.0(1.0)
    alpha_g             -0.49     -0.487(13)       0.00(40)

with epsabs=0, epsrel='auto':

    hyperparameters (true, fitted, prior):
    log(scale)          -0.49      -0.09(10)      -0.7(1.0)
    scale                0.61      0.911(93)       0.50(50)
    U(alpha_Sigma)      0.041       0.14(20)       0.0(1.0)
    alpha_Sigma         0.016      0.055(80)       0.00(40)
    U(alpha_g)           -2.2      -2.22(40)       0.0(1.0)
    alpha_g             -0.49     -0.487(13)       0.00(40)

It does not change anything. The scale is still off.

It takes 45 s.

To do next: use the fisher matrix for the covariance, but the gradient for
minimization.

With Fisher, back to default eps:

    hyperparameters (true, fitted, prior):
    log(scale)          -0.49      -0.09(13)      -0.7(1.0)
    scale                0.61       0.91(12)       0.50(50)
    U(alpha_Sigma)      0.041       0.14(24)       0.0(1.0)
    alpha_Sigma         0.016      0.055(96)       0.00(40)
    U(alpha_g)           -2.2      -2.22(88)       0.0(1.0)
    alpha_g             -0.49     -0.487(30)       0.00(40)

As expected, same minimum but larger errors. Much slower, 190 s, with many more
iterations (why?). Add separate covariance determination to empbayes_fit.

2023-02-14
==========

My current decomposition system is a mess. I can't take reverse gradients.
I can't straightforwardly implement optimized algorithms that compute together
likelihood, gradient, and fisher. Jax patterns break down unpredictably. I
have to redesign it from scratch.

Guidelines and requirements:

  - Sufficient modularity to implement composite decompositions (Woodbury,
    Block)

  - Does not mess up jax in any way
  
  - Caches decompositions

  - Favors optimizing together the likelihood and its derivatives

Operations (in the following I indicate with lowercase inputs which are
typically vectors or tall matrices, and uppercase inputs which are typically
large matrices, since optimization requires taking it into account):

    pinv_bilinear(A, r) => A'K⁺r (for the posterior mean)
    pinv_bilinear_robj(A, r) same but r can be gvars
    ginv_quad(A) => A'K⁻A (for the posterior covariance)
        I want the pseudoinverse for the mean because the data may not be
        in the span and I want to project it orthogonally, while for the
        covariance I expect A and K to come from a pos def matrix so they are
        coherent
    ginv_diagquad(A) => diag(A'K⁻A) (for the posterior variance)
    minus_log_normal_density(
        r: 1d array,      # the residuals (data - prior mean)
        dr_vjp: callable, # x -> x_i ∂r_i/∂p_j,   gradrev and fishvec
        dK_vjp: callable, # x -> x_ij ∂K_ij/∂p_k, gradrev and fishvec
        dr_jvp: callable, # x -> ∂r_i/∂r_j x_j,  fishvec
        dr_jvp: callable, # x -> ∂K_ij/∂p_k x_k, fishvec
        dr: 2d array,     # ∂r_i/∂p_j,  gradfwd and fisher
        dK: 3d array,     # ∂K_ij/∂p_k, gradfwd and fisher
        vec: 1d array,    # input vector of fishvec, same size as params
        value: bool,
        gradrev: bool,
        gradfwd: bool,
        fisher: bool,
        fishvec: bool,
    )
        This computes on request
            value: 1/2 tr(KK⁺) log 2π
                 + 1/2 tr(I-KK⁺) log 2π
                 + 1/2 log pdet K
                 + 1/2 tr(I-KK⁺) log ε
                 + 1/2 r'(K⁺+(I-KK⁺)/ε)r
            gradrev,
            gradfwd: 1/2 tr(K⁺dK)
                    + r'(K⁺+(I-KK⁺)/ε) dr
                    - 1/2 r'(K⁺+2(I-KK⁺)/ε)dKK⁺r
            fisher: 1/2 tr(K⁺dK(K⁺+2(I-KK⁺)/ε)d'K)
                  - 2 tr(K⁺dK(I-KK⁺)d'KK⁺)
                  + dr'(K⁺+(I-KK⁺)/ε)d'r
            fishvec: fisher matrix times vec
        There should be options for omitting the pieces with ε. I also need a
        way to make densities with different values of ε comparable with each
        other (may not be possible, if it is, it probably requires a history of
        ranks and ε). gradfwd forms K⁺ explicitly to compute tr(K⁺dK) for
        efficiency.
    correlate(x)
        Zx where K = ZZ'.
    back_correlate(X):
        Z'X, this is used by Sandwich and Woodbury.

Since I also want to compute the Student density, I could split
minus_log_normal_density's return value into logdet and quad. The gradient
splits nicely between the two terms, but I have to redo the calculation of the
Fisher matrix for the Student distribution. Alternatively, I could use the the
Normal Fisher. => See Lange et al. (1989, app. B). => I think I can split the
gradient and Fisher matrix too.
    
............

Proof of the ∂ log pdet:

    log det (A + ε(I - AA⁺)) =
        = log pdet A + tr(I - AA⁺) log ε  -->
    
    -->  log pdet A = log det (A + ε(I - AA⁺)) - tr(I - AA⁺) log ε
    
    ∂ log pdet A = tr((A + ε(I - AA⁺))⁻¹ ∂(A + ε(I - AA⁺))) +
                   - tr(∂(I - AA⁺)) log ε =
                  
                 = tr((A⁺ + (I - AA⁺)/ε) (∂A + ε ∂(I - AA⁺))) +
                   - 2 tr((I - AA⁺) ∂A A⁺) log ε =
                   
                 = tr((A⁺ + (I - AA⁺)/ε) (∂A - ε(I - AA⁺)∂AA⁺ - εA⁺∂A(I - AA⁺))) =

There are 6 terms. The only nonzero terms are those with ∂A alone.

                 = tr(A⁺∂A + (I - AA⁺)∂A/ε)

And now if ∂A intersects the kernel of A I'm fucked, like Holbrook (2018) says.
But the derivative of ∂ log pdet A(x)/∂x should be defined even if A changes
kernel.

Example: let's say A is 2x2, and R(x) = rotation by x, and

    A(x) = R(x) diag(x, 0) R'(x).

Then

    log pdet A = log x
    ∂ log pdet A/∂x = 1/x

even though the range of A is the first column of R, (cos(x), -sin(x)), and
changes with x:

    ∂R(x)/∂x = ∂/∂x [cos(x) sin(x); -sin(x) cos(x)] =
             = [-sin(x) cos(x); -cos(x) -sin(x)]
    
    ∂A/∂x = ∂R(x)/∂x diag(x, 0) R'(x) +
            + R(x) diag(1, 0) R'(x) +
            + R(x) diag(x, 0) ∂R'(x)/∂x =
        
          = [-sin(x) -cos(x)]' x [cos(x) -sin(x)] +
            + [cos(x) -sin(x)]' [cos(x) -sin(x)] +
            + [cos(x) -sin(x)]' x [-sin(x) -cos(x)] =
        
          = x [-CS SS; -CC CS] +
            + [CC -CS; -SC SS] +
            + x [-CS -CC; SS CS] =
        
          = x [-2CS, SS - CC; SS - CC, 2CS] +
            + 

Whatevs I can skip the explicit calculation:

    A⁺ = R(x) diag(1/x, 0) R'(x)
    tr(A⁺∂A/∂x) = tr(R(x) diag(1/x, 0) R'(x) [
        ∂R(x)/∂x diag(x, 0) R'(x) +
        + R(x) diag(1, 0) R'(x) +
        + R(x) diag(x, 0) ∂R'(x)/∂x
    ])

Quicker: is ∂A in the range of A for hermitian A?

    A = QDQ'
    QQ' = I  -->  ∂QQ' + Q∂Q' = 0  -->  ∂Q = -Q∂Q'Q
    ∂A = ∂Q DQ' + Q ∂D Q' + QD ∂Q' =
       = -Q∂Q'QDQ' + Q∂DQ' - QDQ'∂QQ' =
       = -Q∂Q'A + Q∂DQ' -A∂QQ'
    I - AA⁺ = QD̅Q', where D̅D = 0, D̅∂D = 0
    (I - AA⁺)∂A = -QD̅∂Q'A = -QD̅∂Q'QDQ = QD̅Q'∂QDQ' = (I - AA⁺)∂QDQ'

But:

    tr((I - AA⁺)∂A) = tr((I - AA⁺)∂A(I - AA⁺))
    (I - AA⁺)∂A(I - AA⁺) = 0

Easier proof of (I - AA⁺)∂A(I - A⁺A) = 0: differentiate (I - AA⁺)A(I - A⁺A) = 0.

............

    det(A + LBL') = det A det B det(B⁻¹ + L'A⁻¹L)

    det(A + εI) = det A det(I + εA⁻¹)
    
    det(εI + A) = 
        = det(εI + LL') =
        = εⁿ det(I + L'L/ε)

Bah, nothing useful. Indeed it must not be possible to recover the eigenvalues
below machine precision.

2023-02-16
==========

Changes v0.13 --> v0.14:

- scipy 1.5, jax 0.4.1, gvar 11.10.1
- StructuredArray.from_dataframe
- Bessel, Matern, Taylor, lanczos work with jax jit
- renamed lowrank --> lanczos
- GP checkpos wth lobpcg, and coherent
- addproc(lin)transf by default assings to the DefaultProcess
- fixed bugs
- woodbury -> woodbury2 (supports singular inner matrix)
- new decomp lobpcg
- eps --> epsrel, epsabs
- correlate and decorrelate transposed
- n, m, eps, matrix()
- BART: weights, interpolation, reset depth

2023-02-20
==========

With 12 hyperparameters and 3000 datapoints, the fisher matrix is already
totally undoable with my current implementation! I really have to rewrite
the linalg system to compute it directly.

2023-02-22
==========

Changed forward to reverse jac in empbayes_fit after whitening the parameters.
Results:

totals: time: 0:00:35.640701, calls: fun 0, funjac 32, fisher 0, hess 0
             param         prior           initial     posterior    post-prior      post-ini
--------------------------------------------------------------------------------------------
        log(scale)    -0.7 (1.0)     -0.491478 (0)    -0.18 (13)     0.5 (1.0)     0.31 (13)
     log(sigma_T3)    -0.7 (1.0)     -0.652642 (0)    -0.94 (42)    -0.2 (1.1)    -0.29 (42)
     log(sigma_T8)    -0.7 (1.0)      -2.93305 (0)    -1.99 (78)    -1.3 (1.3)     0.95 (78)
    log(sigma_T15)    -0.7 (1.0)     -0.245323 (0)    -0.17 (29)     0.5 (1.0)     0.08 (29)
      log(sigma_V)    -0.7 (1.0)      -1.59091 (0)     0.20 (16)     0.9 (1.0)     1.79 (16)
     log(sigma_V3)    -0.7 (1.0)     -0.209121 (0)    -0.27 (22)     0.4 (1.0)    -0.06 (22)
     log(sigma_V8)    -0.7 (1.0)     -0.344949 (0)     0.32 (16)     1.0 (1.0)     0.67 (16)
    log(sigma_V15)    -0.7 (1.0)       1.20511 (0)     0.99 (15)     1.7 (1.0)    -0.22 (15)
  log(sigma_Sigma)    -0.7 (1.0)     -0.192868 (0)     0.44 (40)     1.1 (1.1)     0.64 (40)
      log(sigma_g)    -0.7 (1.0)      -1.45668 (0)    -1.98 (90)    -1.3 (1.3)    -0.53 (90)
    U(alpha_Sigma)     0.0 (1.0)    -0.0685453 (0)     0.37 (54)     0.4 (1.1)     0.44 (54)
        U(alpha_g)     0.0 (1.0)       1.16963 (0)     0.76 (50)     0.8 (1.1)    -0.41 (50)

Previous results:

totals: time: 0:02:12.439041, calls: fun 0, funjac 32, fisher 0, hess 0
             param         prior           initial     posterior    post-prior      post-ini
--------------------------------------------------------------------------------------------
        log(scale)    -0.7 (1.0)     -0.491478 (0)    -0.18 (13)     0.5 (1.0)     0.31 (13)
     log(sigma_T3)    -0.7 (1.0)     -0.652642 (0)    -0.94 (42)    -0.2 (1.1)    -0.29 (42)
     log(sigma_T8)    -0.7 (1.0)      -2.93305 (0)    -1.99 (78)    -1.3 (1.3)     0.95 (78)
    log(sigma_T15)    -0.7 (1.0)     -0.245323 (0)    -0.17 (29)     0.5 (1.0)     0.08 (29)
      log(sigma_V)    -0.7 (1.0)      -1.59091 (0)     0.20 (16)     0.9 (1.0)     1.79 (16)
     log(sigma_V3)    -0.7 (1.0)     -0.209121 (0)    -0.27 (22)     0.4 (1.0)    -0.06 (22)
     log(sigma_V8)    -0.7 (1.0)     -0.344949 (0)     0.32 (16)     1.0 (1.0)     0.67 (16)
    log(sigma_V15)    -0.7 (1.0)       1.20511 (0)     0.99 (15)     1.7 (1.0)    -0.22 (15)
  log(sigma_Sigma)    -0.7 (1.0)     -0.192868 (0)     0.44 (40)     1.1 (1.1)     0.64 (40)
      log(sigma_g)    -0.7 (1.0)      -1.45668 (0)    -1.98 (90)    -1.3 (1.3)    -0.53 (90)
    U(alpha_Sigma)     0.0 (1.0)    -0.0685453 (0)     0.37 (54)     0.4 (1.1)     0.44 (54)
        U(alpha_g)     0.0 (1.0)       1.16963 (0)     0.76 (50)     0.8 (1.1)    -0.41 (50)

It works and its 4x faster.

2023-02-23
==========

pdf4.py, t.py and u.py, if use empbayes_fit(..., jit=True), fail with leaking
tracer errors in logdet. Running with jax.checking_leaks stops on a previous
leak occurring in elementwise_grad instead. For the time being I run them
without the jit. => disabling the new jit implementation of jax 0.4.4 solves
the problem. I wonder if it is a jax bug. I'll wait for the next jax release.

2023-02-28
==========

Continuing with the new linalg. Is it possible to compute a fisher-vector
product more efficiently than by evaluating the fisher matrix explicitly?

    tr(K⁺dKK⁺dK)_ij v_j =
        = K⁺_kl dK_lmi K⁺_mn dK_nkj v_j

Also, how do I implement Fisher with a vector-jacobian product for dK?

Since K is larger than the number of parameters, maybe I should use the forward
gradient up to K? But the operation I need for the gradient is exactly the vjp.
=> the fact that the optimal way of computing the hessian is fwd(rev(·)) means
that one of the dK should be a vjp and the other a jvp. So I compute K⁺dKK⁺ with
the explicit forward jacobian, and then tr((K⁺dKK⁺)dK) with the vjp. So for
Fisher I need value_and_op(K, jacfwd, jacrev). A similar thing is valid for dr.

=> Wait. If I have to do O(n^3d) K⁺dKK⁺ anyways, then doing O(n^2d) vjp(K⁺dKK⁺)
does not really solve anything. I can do

    K = ZZ'
    K⁺ = Z⁺'Z⁺
    tr(K⁺dKK⁺dK) = tr(Z⁺'Z⁺dKZ⁺'Z⁺dK) = tr(Z⁺dKZ⁺'Z⁺dKZ⁺') =
        Y = Z⁺dKZ⁺'
    = tr(YY) = Y_ij Y_ji = Y.flat @ Y.T.flat (what would einsum do?)

In [12]: %timeit jnp.einsum('ij,ji', m, m)
1.68 ms ± 5.85 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)

In [13]: %timeit jnp.sum(m * m.T)
1.48 ms ± 11.4 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)

In [17]: %timeit m.flatten() @ m.T.flatten()
2.36 ms ± 38 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

2023-03-02
==========

The BART kernel is very inefficient with a high number of covariates because it
builds explicitly the broadcasted input arrays. The broadcast occurs when
computing n-, n0, n+ as

--> 177     l = jnp.minimum(ix, iy)
    178     r = jnp.maximum(ix, iy)
    179     before = l
    180     between = r - l
    181     after = splits[0] - r

Is there a way to avoid this broadcast?

Other question: even if I fixed this, would no other broadcast occur?

To do:

  - try to call directly BART.correlation with large arrays (n=3000, p=50) and
    see what happens. => still bad, fills up memory

  - if it works fine, try to remove the broadcast on n*. => not possible because
    n0 is the difference

  - if it is not possible, rewrite correlation to take as input the indices. =>
    does not apply

Questions:
  - would BART.correlation under jit do the same? => better but yep, it fills 14
    GB with n=3000, p=100, and 3000*3000*100*4 = 3.6 GB. The only improvement is
    not wasting additional memory.
  - is it a general characteristic of jnp.vectorize? => looking at the code of
    jnp.vectorize, it appears that explicit broadcasting is done there to
    simplify the implementation.
  - is it a general characteristic of jax.vmap? => no, vmap seems to work well.

How do I reimplement vectorize?
  - copy the jax implementation in a new file (DONE)
  - remove broadcasting of the arguments (DONE)
  - produce "broadcasting shapes" with ones filled in (DONE)
  - apply vmap for each axis, the outer vmap corresponds to the first axis 
    (DONE)
  - each vmap has in_axes=0 or None depending on the axis of the broadcasting
    shape being > 1 or 1 (DONE)
  - call the vmapped function with arguments squeezed on non-core axes (DONE)

The new implementation works. Now:
  - make _patch_jax into a module (DONE)
  - move the new vectorize implementation there in its own file (DONE)
  - import it in _patch_jax (DONE)
  - check if the problem is solved in the test script => nope
  - rewrite BART.correlation in terms of pairs of indices and totals (make a
    wrapper to keep the old interface around for compatibility)
  - if it works, reinstall lsqfitgp in bart/code/pyenv and try again there

The new vectorize works well with a simple test function, but not with the BART
kernel. Maybe there is some operation that forces a full broadcast when vmapped?
Investigate this:
  - redo the test with maxd=0,1,2 and check if 0 and 1 work
  - read carefully the code searching for such an operation, and try it
    separately in the test script

Quick test: even replacing x @ y by sum(x * y) is sufficient to break it,
although it is still true that the new vectorize works much better than the old.
Maybe any operation that would broadcast the arrays if applied in isolation will
be bad. If so, I would need to somehow end up doing the broadcast only after
reducing the covariates.

Problem: anything that depends on n0 = ix - iy is already a nightmare. If there
was a sum I could maybe write some operations as dot products and split them
because of positivity, but with a difference I can't. anyn0 is already
off-limits. Looking around I see many operations that I could write as dot
products, but also many I can't, like jspecial.digamma(1 + nminus0).

General solution: implement an ufunc batcher and use it in CrossKernel.__call__.
It triggers by passing batchsize: int to __init__. The size refers to the number
of bytes of the broadcasted input arrays above which the arrays are split.

How do I implement an ufunc batcher?

I have to use jax.lax loops to avoid tracing out all the batches. This means
that it will work only with jax-traceable code. I could write later the non-jax
version if I need it.

Determine the split size per axis. Start from the outer axis. If a single "row"
of the first axis is smaller than the batch size, batch only over that axis.
Otherwise split the first axis over all indices and then move on to splitting
along the second axis, and so on, until either the axes are exhausted or the
batch size is small enough. (Implement nbytes in StructuredArray.)

This produces a list of split sizes for a certain axis, with the precedent axes
split over all items.

Or maybe the whole implementation can be recursive. Check if the leading axis is
sufficient to split (with the broadcasted size). If it is, lax.fori_loop over
the even splits, and then do apart the remainder. If it isn't, lax.fori_loop
over all indices calling the batcher on each slice of the array. The recursion
path is static because it depends only on the shapes, so abstract tracing is
fine with it.

TODO tomorrow: 
  - write the batcher in its own file in _patch_jax (DONE)
  - test it in test_jax.py (DONE)
  - use it to implement batchsize in CrossKernel (DONE)
  - apply batchsize to _BARTBase in the decorator, with some reasonable size
    (100 MB?) (DONE)

2023-03-03
==========

Next todo: test batcher on ordinary arrays (DONE), then test in on
StructuredArray (DONE).

2023-03-05
==========

Looks like scipy.optimize.LbfgsInvHessProduct.todense

    https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.LbfgsInvHessProduct.todense.html#scipy.optimize.LbfgsInvHessProduct.todense

    https://github.com/scipy/scipy/blob/c1ed5ece8ffbf05356a22a8106affcd11bd3aee0/scipy/optimize/_lbfgsb_py.py#L474-L494

is O(n^3) when it could be O(n^2). The update step can be written as:

    H - rho ((Hy)s') - rho ((Hy)s')' + rho^2 |y|^2 ss'

Maybe do a issue+pr to scipy.

2023-03-07
==========

To compute a Fisher-vector product when there are many parameters, do

    tr(K+ dK K+ dK) v =
    = K_vjp(K+ K_jvp(v) K+)

2023-03-10
==========

pdf4 fails with jax 0.4.6, I think it's skipifabstract again.

..........

CI fails, only on ubuntu old:

FAILED tests/test_linalg.py::TestBlock_EigCutFullRank::test_quad_vec_jac_fwd_jit - ValueError: array must not contain infs or NaNs

FAILED tests/test_linalg.py::TestChol::test_solve_matrix_jac_fwd_da - AssertionError: matrices actual and desired are not close in 2-norm
norm(desired) = 1.3
norm(actual - desired) = 0.061  (atol = 0)
ratio = 0.045  (rtol = 1e-06)

FAILED tests/test_linalg.py::TestChol::test_solve_matrix_hess_da - AssertionError: matrices actual and desired are not close in 2-norm
norm(desired) = 2.5
norm(actual - desired) = 2.5  (atol = 0)
ratio = 1  (rtol = 1e-06)

FAILED tests/test_linalg.py::TestChol::test_quad_matrix_matrix_jac_fwd_da - AssertionError: matrices actual and desired are not close in 2-norm
norm(desired) = 11
norm(actual - desired) = 10  (atol = 0)
ratio = 0.91  (rtol = 1e-06)

FAILED tests/test_linalg.py::TestChol::test_quad_matrix_matrix_hess_da - AssertionError: matrices actual and desired are not close in 2-norm
norm(desired) = 2.2
norm(actual - desired) = 2.2  (atol = 0)
ratio = 1  (rtol = 1e-06)

FAILED tests/test_linalg.py::TestChol::test_logdet_hess_da - AssertionError: matrices actual and desired are not close in 2-norm
norm(desired) = 0.36
norm(actual - desired) = 0.36  (atol = 1e-60)
ratio = 1  (rtol = 1e-05)

So it's all of the 5 _da tests failing for Chol. What??

I really don't know what's going on. I'll put my head under the sand and hope
for this to go away. (I really need to write the new linalg...)

........

Idea on pseudoinversion with Cholesky:

    K = LL'
    L⁺ ≈ L'(LL' + Iε)⁻¹

Nope not useful.

    (L + Iε)(L + Iε)'
        = LL' + Lε + L'ε + Iε²
        ≈ LL' + ε(L + L')

Noope.

Let L = f(K) such that K = LL'. I am forced to compute f(K + Iε), and I want to
obtain f(K) from it afterwards.

    f(K + Iε) ≈ f(K) + ε ∂f/∂K I

Could I do this with the formula for the cholesky jvp/vjp?

    f(K) ≈ f(K + Iε) - f_jvp(I)

From Murray (2016, eq. 6-8):

    ∂L = LΦ(L⁻¹∂KL'⁻¹)
    Φ(A) := lower triangular part and halve the diagonal

    f_jvp(I) = LΦ(L⁻¹L'⁻¹)

Better to write as a difference:

    K = K̃ + Iε = LL'
    f(K̃) = f(K - Iε) = f(K) - ε f_jvp(I)
    L̃ = L - ε LΦ(L⁻¹L'⁻¹) =: L - εLM (with M and LM lower triangular)
    K̃⁺ = L̃⁺'L̃⁺
    L̃⁺ = L̃'((L̃L̃' + Iε)⁻¹ + ε(L̃L̃' + Iε)⁻² + O(ε²)) =
       = (L - εLM)' K⁻¹ (I + εK⁻¹ + O(ε²)) =
       = (L' - εM'L') L'⁻¹L⁻¹ (I + εK⁻¹ + O(ε²)) =
       = (I - εM') L⁻¹ (I + εL'⁻¹L⁻¹ + O(ε²)) =
       = L⁻¹ - εM'L⁻¹ + εL⁻¹L'⁻¹L⁻¹ + O(ε²) =
       = L⁻¹ + ε(L⁻¹L'⁻¹ - M')L⁻¹ + O(ε²) =
       = L⁻¹ + εML⁻¹ + O(ε²)

Decomposition steps:        (times for size 10000)
    0   chol    K = LL'     6s
    1   trimat  L⁻¹ I       5s
    2   trimat  [1] L'⁻¹    5s

Note: matmul takes 10s.

Quad steps:
    0   trivec  L⁻¹ v
    1   matvec  M' [0]

So it's 3x cholesky. Accuracy? Since I decompose directly the matrix, it should
be accurate I guess? It depends if the εLM correction is accurate. M gets 1/ε
terms from the forward substitutions, that then get reduced to O(1) multiplying
by ε. That does not smell accurate. I'll see.

Is there a way to do compute Φ(L⁻¹L'⁻¹)v without evaluating Φ(L⁻¹L'⁻¹)?

    Φ(L⁻¹L'⁻¹)_ij v_j
        = L⁻¹_ik L⁻¹_jk v_j δ_j≤i 2^-δ_ij

No, I can't see it.

(X - Φ(X))_ij = X_ij - [X_ij if i > j, 1/2 X_ii if i = j, 0 if i < j] =
              = [0 if i > j, 1/2 X_ii if i = j, X_ij if i < j] =
              = Φ(X')'

(L⁻¹L'⁻¹)' = L⁻¹L'⁻¹

2023-03-14
==========

=> tested in playground/pinv/pchol, does not seem to work well. Anyway, PSTRF
does pivoted cholesky in 2x time and is rank revealing. => I think there's a
simple pinv formula for a block-triangular singular matrix like the pivoted
cholesky factor, I could do clean pinv with 2x cholesky time.

2023-03-15
==========

I added the conversion hess_inv lbfgs -> bfgs, it works fine. Then I tried
setting l-bfgs-b as default, but it's still not fine when the linear search
fails, while bfgs from the start gives a sensible inverse hessian. Maybe lfbgs
does not keep the updates in that case? I have to find a way to fix that. If
it's missing updates, I could keep my own buffer of gradients and values.

=> Logging bart/comp42/comp42-test.py confirms it's dropping updates. Ack.
With 15 evals in the failed linsearch, n_corrs=0. Maybe all failed searches are
dropped, without restoring previous values?

..........

I implemented a staged timer in empbayes_fit. I made it to work under jit by
wrapping with pure_callback and passing through it a token value which is added
to the result but has no effect. However this still does not work; I guess jit
is reordering the computations. => fixed by passing intermediate results through
the callback.

..........

CI failures:

ubuntu-latest:

    FAILED tests/test_linalg.py::TestBlock_EigCutFullRank::test_quad_vec_jac_fwd_jit - ValueError: array must not contain infs or NaNs
    FAILED tests/test_linalg.py::TestChol::test_solve_matrix_jac_fwd_da - AssertionError: matrices actual and desired are not close in 2-norm
    norm(desired) = 5.5
    norm(actual - desired) = 5.2  (atol = 0)
    ratio = 0.95  (rtol = 1e-06)
    FAILED tests/test_linalg.py::TestChol::test_solve_matrix_hess_da - AssertionError: matrices actual and desired are not close in 2-norm
    norm(desired) = 5.4
    norm(actual - desired) = 5.4  (atol = 0)
    ratio = 1  (rtol = 1e-06)
    FAILED tests/test_linalg.py::TestChol::test_quad_matrix_matrix_jac_fwd_da - AssertionError: matrices actual and desired are not close in 2-norm
    norm(desired) = 14
    norm(actual - desired) = 6.1  (atol = 0)
    ratio = 0.44  (rtol = 1e-06)
    FAILED tests/test_linalg.py::TestChol::test_quad_matrix_matrix_hess_da - AssertionError: matrices actual and desired are not close in 2-norm
    norm(desired) = 0.28
    norm(actual - desired) = 0.28  (atol = 0)
    ratio = 1  (rtol = 1e-06)
    FAILED tests/test_linalg.py::TestChol::test_logdet_hess_da - AssertionError: matrices actual and desired are not close in 2-norm
    norm(desired) = 0.24
    norm(actual - desired) = 0.24  (atol = 1e-60)
    ratio = 1  (rtol = 1e-05)

ubuntu-20.04:

    FAILED tests/test_fit.py::test_method - ValueError: failed to initialize intent(inout) array -- input not fortran contiguous

        @util.tryagain
        def test_method():
        
            hp = gvar.BufferDict({
                'log(sdev)': gvar.log(gvar.gvar(1, 1))
            })
            x = np.linspace(0, 5, 10)
            def gpfactory(hp):
                gp = lgp.GP(lgp.ExpQuad() * hp['sdev'] ** 2)
                gp.addx(x, 'x')
                return gp
            truehp = gvar.sample(hp)
            truegp = gpfactory(truehp)
            trueprior = truegp.prior()
            data_fixed = gvar.sample(trueprior)
            def data_variable(hp):
                return {k: v + hp['log(sdev)'] for k, v in data_fixed.items()}
        
            for data in [data_fixed, data_variable]:
                fits = []
                kws = [
                    dict(method='nograd', minkw=dict(options=dict(xatol=1e-6))),
                    dict(method='gradient'),
                    dict(method='gradient', minkw=dict(method='l-bfgs-b')),
                    dict(method='fisher'),
                ]
                for kw in kws:
                    kwargs = dict(data=data)
                    kwargs.update(kw)
                    kwargs.setdefault('minkw', {}).update(x0=truehp.buf)
    >               fit = lgp.empbayes_fit(hp, gpfactory, **kwargs)

    tests/test_fit.py:177: 
    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
    lsqfitgp/_fit.py:405: in __init__
        result = optimize.minimize(**minargs)
    /opt/hostedtoolcache/Python/3.8.16/x64/lib/python3.8/site-packages/scipy/optimize/_minimize.py:617: in minimize
        return _minimize_lbfgsb(fun, x0, args, jac, bounds,
    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    fun = <scipy.optimize.optimize.MemoizeJac object at 0x7fbc7b6fe850>
    x0 = array([-0.50046867]), args = ()
    jac = <bound method MemoizeJac.derivative of <scipy.optimize.optimize.MemoizeJac object at 0x7fbc7b6fe850>>
    bounds = [(None, None)], disp = None, maxcor = 10, ftol = 2.220446049250313e-09
    gtol = 1e-05, eps = 1e-08, maxfun = 15000, maxiter = 15000, iprint = -1
    callback = <lsqfitgp._fit.empbayes_fit._Callback object at 0x7fbc7b77d2b0>
    maxls = 20, finite_diff_rel_step = None, unknown_options = {}, m = 10
    pgtol = 1e-05, factr = 10000000.0, n = 1
    new_bounds = (array([-inf]), array([inf]))
    sf = <scipy.optimize._differentiable_functions.ScalarFunction object at 0x7fbc7b95abe0>
    func_and_grad = <bound method ScalarFunction.fun_and_grad of <scipy.optimize._differentiable_functions.ScalarFunction object at 0x7fbc7b95abe0>>

        def _minimize_lbfgsb(fun, x0, args=(), jac=None, bounds=None,
                             disp=None, maxcor=10, ftol=2.2204460492503131e-09,
                             gtol=1e-5, eps=1e-8, maxfun=15000, maxiter=15000,
                             iprint=-1, callback=None, maxls=20,
                             finite_diff_rel_step=None, **unknown_options):
            """
            Minimize a scalar function of one or more variables using the L-BFGS-B
            algorithm.
        
            Options
            -------
            disp : None or int
                If `disp is None` (the default), then the supplied version of `iprint`
                is used. If `disp is not None`, then it overrides the supplied version
                of `iprint` with the behaviour you outlined.
            maxcor : int
                The maximum number of variable metric corrections used to
                define the limited memory matrix. (The limited memory BFGS
                method does not store the full hessian but uses this many terms
                in an approximation to it.)
            ftol : float
                The iteration stops when ``(f^k -
                f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= ftol``.
            gtol : float
                The iteration will stop when ``max{|proj g_i | i = 1, ..., n}
                <= gtol`` where ``pg_i`` is the i-th component of the
                projected gradient.
            eps : float or ndarray
                If `jac is None` the absolute step size used for numerical
                approximation of the jacobian via forward differences.
            maxfun : int
                Maximum number of function evaluations.
            maxiter : int
                Maximum number of iterations.
            iprint : int, optional
                Controls the frequency of output. ``iprint < 0`` means no output;
                ``iprint = 0``    print only one line at the last iteration;
                ``0 < iprint < 99`` print also f and ``|proj g|`` every iprint iterations;
                ``iprint = 99``   print details of every iteration except n-vectors;
                ``iprint = 100``  print also the changes of active set and final x;
                ``iprint > 100``  print details of every iteration including x and g.
            callback : callable, optional
                Called after each iteration, as ``callback(xk)``, where ``xk`` is the
                current parameter vector.
            maxls : int, optional
                Maximum number of line search steps (per iteration). Default is 20.
            finite_diff_rel_step : None or array_like, optional
                If `jac in ['2-point', '3-point', 'cs']` the relative step size to
                use for numerical approximation of the jacobian. The absolute step
                size is computed as ``h = rel_step * sign(x0) * max(1, abs(x0))``,
                possibly adjusted to fit into the bounds. For ``method='3-point'``
                the sign of `h` is ignored. If None (default) then step is selected
                automatically.
        
            Notes
            -----
            The option `ftol` is exposed via the `scipy.optimize.minimize` interface,
            but calling `scipy.optimize.fmin_l_bfgs_b` directly exposes `factr`. The
            relationship between the two is ``ftol = factr * numpy.finfo(float).eps``.
            I.e., `factr` multiplies the default machine floating-point precision to
            arrive at `ftol`.
        
            """
            _check_unknown_options(unknown_options)
            m = maxcor
            pgtol = gtol
            factr = ftol / np.finfo(float).eps
        
            x0 = asarray(x0).ravel()
            n, = x0.shape
        
            if bounds is None:
                bounds = [(None, None)] * n
            if len(bounds) != n:
                raise ValueError('length of x0 != length of bounds')
        
            # unbounded variables must use None, not +-inf, for optimizer to work properly
            bounds = [(None if l == -np.inf else l, None if u == np.inf else u) for l, u in bounds]
            # LBFGSB is sent 'old-style' bounds, 'new-style' bounds are required by
            # approx_derivative and ScalarFunction
            new_bounds = old_bound_to_new(bounds)
        
            # check bounds
            if (new_bounds[0] > new_bounds[1]).any():
                raise ValueError("LBFGSB - one of the lower bounds is greater than an upper bound.")
        
            # initial vector must lie within the bounds. Otherwise ScalarFunction and
            # approx_derivative will cause problems
            x0 = np.clip(x0, new_bounds[0], new_bounds[1])
        
            if disp is not None:
                if disp == 0:
                    iprint = -1
                else:
                    iprint = disp
        
            sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,
                                          bounds=new_bounds,
                                          finite_diff_rel_step=finite_diff_rel_step)
        
            func_and_grad = sf.fun_and_grad
        
            fortran_int = _lbfgsb.types.intvar.dtype
        
            nbd = zeros(n, fortran_int)
            low_bnd = zeros(n, float64)
            upper_bnd = zeros(n, float64)
            bounds_map = {(None, None): 0,
                          (1, None): 1,
                          (1, 1): 2,
                          (None, 1): 3}
            for i in range(0, n):
                l, u = bounds[i]
                if l is not None:
                    low_bnd[i] = l
                    l = 1
                if u is not None:
                    upper_bnd[i] = u
                    u = 1
                nbd[i] = bounds_map[l, u]
        
            if not maxls > 0:
                raise ValueError('maxls must be positive.')
        
            x = array(x0, float64)
            f = array(0.0, float64)
            g = zeros((n,), float64)
            wa = zeros(2*m*n + 5*n + 11*m*m + 8*m, float64)
            iwa = zeros(3*n, fortran_int)
            task = zeros(1, 'S60')
            csave = zeros(1, 'S60')
            lsave = zeros(4, fortran_int)
            isave = zeros(44, fortran_int)
            dsave = zeros(29, float64)
        
            task[:] = 'START'
        
            n_iterations = 0
        
            while 1:
                # x, f, g, wa, iwa, task, csave, lsave, isave, dsave = \
    >           _lbfgsb.setulb(m, x, low_bnd, upper_bnd, nbd, f, g, factr,
                               pgtol, wa, iwa, task, iprint, csave, lsave,
                               isave, dsave, maxls)
    E           ValueError: failed to initialize intent(inout) array -- input not fortran contiguous

So it's probably a scipy 1.5 bug. I switched to scipy==1.5.4.

2023-03-17
==========

Optimize the BART kernel:

  - Cast integers to floats when passing them to digamma, using the floating
    type deduced from the input arguments (DONE)

  - Remove the type forcing in the vectorizing wrapper (DONE)

  - Run the bart tests (DONE)

  - Check the tests fail if I remove the float cast => they fail, but not
    because of the lack of accuracy

  - Make a test for the lack of accuracy (DONE)

  - Duplicate the internal implementation (DONE)

  - Rewrite the input of the duplicate in terms of 0 <= ix, iy <= n (DONE)

  - Add an option to the vectorizing wrapper to use the alternative impl (the
    inputs change interpretation) (DONE)

  - Add an option to correlation too (double interpretation) (DONE)

  - Make a unit test that compares the two implementations (DONE)

  - Add an option to BART (single interpretation) (DONE)

  - Add an option to BART to accept directly indices (DONE)

  - Make BART.indices_from_coord return structured arrays by default => too
    complicated right now, because I don't have a quick way of making a
    StructuredArray out of a flat array (I should implement
    _array.unstructured_to_structured). I think I should drop the all-flat
    format and instead map the operations over StructuredArray as a pytree,
    including the splitting counts and points. Operations on subarray fields
    would still be rolled with _unique and _searchsorted, so it's efficient to
    compile if needed (high p) but keeps a consistent and legible format and
    does not waste memory with filler splitting points (e.g. with a ton of
    dummies and some continuous covariates). Then I convert to flat format only
    before calling BART.correlation.

  - Make a unit tests to compare index/non-index input (DONE)

  - Write a script to benchmark the two implementations at fixed maxd (DONE)

  - Optimize the alternate implementation (DONE)

........

The lbfgsb bug is not fixed by any version of scipy up to the latest, but the
latest does give a more informative error message. Currently I suspect it does
not like the gradient because the problematic parameter is called `g`. I have to
install the old requirements on my computer and see what fails.

2023-03-18
==========

Optimize!

Make the internal implementation a no-op and see how long it takes => 0.45ms
Make it a matmul reduction => 3.8ms
Make it a sum(mult) reduction => 36ms
Make it a pre-reduction => 0.68ms

Conclusion: there is no signicant overhead, and pre-reduction is effective.

I need to compute

    anyn0 = any((ix - iy) & w)

Since I'm doing ix = 0 where w = 0, I have ~w -> ~(ix - iy), so ix - iy -> w,
so (ix - iy) & w = ix - iy:

    anyn0 = any(ix - iy)
          = any(ix ≠ iy)
          = ix ≠ iy

Pre-reduced version:

    hx = hash(ix)
    hy = hash(iy)
    anyn0 = hx ≠ hy

As hash I take https://github.com/ztanml/fast-hash. So:
  - reimplement the hash in jax (DONE)
  - compile the c version with hardcoded inputs and print their hash (DONE)
  - check the hashes change a lot by flipping one bit of the input (DONE)
  - make a unit test that my implementation gives the same values (DONE)
  - make a unit test that checks that single bit-flipping changes an expected
    50% of the output bits (do a chisquared test) (lax has popcount) (DONE)
  - try to unroll the hash (DONE)
  - use the hash in BART._correlation2 (DONE)
  - time it => 0.82ms (DONE)
  - try to introduce collision checking with lax.cond (DONE)
  - time it (DONE)
  - if it slows down, drop it (DONE)
  - if it's still fast, try taking the fwd/rev derivative (NA)
  - it it slows down, try using stop_gradient somewhere on the collision check 
    (NA)

I optimized the maxd=0 case and it works fine. Now I have maxd=1.

    1    nout = nminus + nplus
    2    Wn = jnp.sum(jnp.where(n, w, 0))
    3    Q = 1 - (1 - gamma) * pnt[1]
    4    sump = Q * jnp.sum(jnp.where(n, w * nout / n, 0)) # <-- @
    5    return jnp.where(anyn0, 1 - pnt[0] * (1 - sump / Wn), 1)

1) I have to avoid computing nout. It's used only in (4).

2) This is fine.

3) Ok.

4) Darn, I can't see a way to avoid crossing ix with iy. If their difference
was always positive it would be ok.

5) Ok, this is already O(n^2). I think I can avoid using anyn0: if n0=0, then
nout == n, so it's sump = Q * Wn => no wait it depends on Q. Crap. Whatever.

=> I still got quite an improvement by replacing the summation with a matmul
(40ms -> 10ms).

Now it's maxd=2.

2023-03-19
==========

Starting point:

    Old implementation:
    902 ms ± 5.23 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    New implementation:
    487 ms ± 11 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Matmul:
    990 µs ± 27.3 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)

(I converted the matmul to jax for a fair comparison, it's faster.)

Note from the bart repo:

      * digamma(1 + nminus0) =
            = digamma(1 + nminus + n0) =
            = digamma(1 + maximum(ix, iy)) =
            = maximum(digamma(1 + ix), digamma(1 + iy))
      * digamma(1 + nplus0) =
            = digamma(1 + nplus + n0) =
            = digamma(1 + n - minimum(ix, iy)) =
            = maximum(digamma(1 + n - ix), digamma(1 + n - iy))

After using it:

    New implementation:
    141 ms ± 408 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

Next optimization: I replace a single sum-mult with matmul:

    110 ms ± 576 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

Good! A lot of difference just for that! Again for another one:

    63.3 ms ± 549 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

By removing conditions and gathering, I get

    50.4 ms ± 201 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

Now to go further I need to try to rewrite as much as possible of the summation
term as separate calculations on ix and iy that get combined only at the end.

To do next:
  - commit (DONE)
  - install the current version in the bart repo and re-run the bakeoff (DONE)
  - compute Wnplus and Wnminus with the max-min trick => was not useful (DONE)
  - optimize recursion restart by moving the base cases in the internal function (DONE)
  - implement the CrossKernel.outer mechanism to avoid recomputing digamma

DISASTER! Since in the recursion reset gamma gets shape n^2, some of my
optimizations break down, and I'm back to square one:

    Old implementation:
    899 ms ± 5.32 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    New implementation:
    319 ms ± 1.99 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Matmul:
    1.07 ms ± 3.88 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)

New priority: optimize the recursion restart case.

It looks weird that performance drops so sharply though, it should be a clean
O(n^2) without p. What's going on? => try to implement the restart internally
and see if it is sufficient to fix it up.

2023-03-22
==========

I tried optimizing the calculation of inv_Wnplus and inv_Wnminus by splitting
them on calculations on ix/iy with maximum/minimum, but it was slower by 5ms.
While doing that, I noticed that I could write them right away as O(p)
calculations. Then I wondered if minimum/maximum are intrisically slow compared
to where, so I used where for the digamma term, and---surprise---I gained 5ms!

........

I'll try to simplify

    inv_Wnminus + inv_Wnplus - 2 * inv_Wn =

    = where(nplus0, inv_Wn, inv_Wnmod) +
    + where(nminus0, inv_Wn, inv_Wnmod) +
    - 2 * inv_Wn =

    = where(n - min(x, y), inv_Wn, inv_Wnmod) +
    + where(max(x, y), inv_Wn, inv_Wnmod) +
    - 2 * inv_Wn =

    = where(x < y,
        where(n - x, inv_Wn, inv_Wnmod) + where(y, inv_Wn, inv_Wnmod) - 2 * inv_Wn,
        where(n - y, inv_Wn, inv_Wnmod) + where(x, inv_Wn, inv_Wnmod) - 2 * inv_Wn,
    ) =

If x < y, then y > 0 because x ≥ 0. Also, n - x > n - y ≥ 0, so n - x > 0. So

    = where(x < y,
        0,
        where(n - y, inv_Wn, inv_Wnmod) + where(x, inv_Wn, inv_Wnmod) - 2 * inv_Wn,
    ) =

Analogously, if x > y, it's 0. The only interesting case is x == y:

    = where(n - x, inv_Wn, inv_Wnmod) +
    + where(x, inv_Wn, inv_Wnmod) +
    - 2 * inv_Wn =

Since we can assume n > 0 (if n = 0 the whole term is suppressed), then
either x > 0 or n - x > 0. So it's

    = where(x & (n - x), 0, inv_Wnmod - inv_Wn)

Overall, it's

    = where(n0, 0, where(x & (n - x), 0, inv_Wnmod - inv_Wn))

Note: first by mistake I used | instead of &. It's 3ms faster than &. What??
=> I tried rewriting the expression with ~|~, and with where(where()), but it's
still 46ms instead of 43ms. Weird.

I'm not using it after all, so I'll paste it here:

    inv_W_corr = jnp.where(n0,
        0, jnp.where(ix.astype(bool) & (n - ix).astype(bool),
            0, inv_Wnmod - inv_Wn))
    terms1 = (S + t) * (inv_Wn * nout + inv_W_corr)
    # and drop inv_Wnplus, inv_Wnminus

.........

I tried maxd=4, reset=2, and now it's

    Old implementation:
    898 ms ± 4.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    New implementation:
    97.2 ms ± 403 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

Somehow a thing I did improved it from 320ms to 100ms. Nice.

The time is about twice as the single-step calculation, as expected. To do:
  - optimize the n^2 gamma case separately => already as fast as possible (DONE)
  - put part of the reset mechanism inside the correlation function (remember to
    keep the old version when using the old impl) (DONE)

To be continued: I have to call directly BART.correlation to pass array gamma.

.........

To do: modify skipifabstract to not enter ensure_compile_time_eval, and see if
that solves the leaked tracers problems. => Nope it does not. Maybe the creation
of the tracing error report is sufficient to leak tracers? But
playground/jax/14776 does not fail. Maybe it's sufficient under more convoluted
conditions?

2023-03-25
==========

Compilation with small inputs is faster than compilation with large inputs.
Maybe it's the checks under skipifabstract?

=> It's 700ms to compile & evaluate (vs. 50ms to evaluate after compilation,
vs. 0.5ms to compile & evaluate on small inputs.) If I avoid using
ensure_compile_time_eval under skipifabstract, then it raises to 830ms.
Unexpected! I thought it was the forced early evaluation that was taking time,
instead it seems to be at least somewhat the jax tracing exception mechanism.
Maybe I should disable checks altogether under jit?

I commented out all usages of skipifabstract in _bart.py, and I obtain

    New implementation:
    50.2 ms ± 501 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
    Compilation time
    139 ms ± 480 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
    Full calculation with compilation:
    436 ms ± 3.25 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

(The 0.5ms was fake, it was caching the compiled function, it was actually
200ms.)

So it's quite lower, but still 140+50 << 440. What's taking so much time to
compile for the larger arrays?

Idea: use jax functionality to lower the function without evaluating it. Result:

    New implementation:
    49.8 ms ± 372 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
    lower+compile time on token inputs:
    134 ms ± 500 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
    lower+compile time on full inputs:
    309 ms ± 4.28 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Full calculation with compilation:
    355 ms ± 1.91 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

So it's taking 2x time on the larger inputs. Is it doing operations on them? Or
it's just the added complication?

After some playing, I'm confident it's genuine compilation times which vary
depending on the shapes, and not calculations depending on the data.

Conclusions: doing the checks under skipifabstract is slow. I should try out
checkify. This requires modifying empbayes_fit to apply checkify.

Question: what's the overhead of checkify? I hope it's small compared to jit.

To do:
  - undo the commenting out of skipifabstract (DONE)
  - commit (DONE)
  - branch (DONE)
  - use checkify in bart (DONE)
  - measure overhead (DONE)
  - check that the first-call times are compilation + compiled run (DROPPED)
  - if reasonable, implement it in empbayes_fit (DROPPED)

doing no vs. 9 resets makes compilation time go from 300ms to 250ms, so it's not
the bottleneck compared to the checks, although that may change with derivatives

..........

jax 0.4.7 will remove the old jit implementation. When it comes out, check if
the leak problems are solved, if not: pull, branch tag v0.14, and make a release
0.14.1 with jax,jaxlib<=0.4.6 in the requirements. => They are not solved.

..........

Overhead: it's damn slow, but I think it's the splits check, since the splitting
points are a folded-in constant. I'll try checkify without checks.

=> checkify without checks hangs. Why?? I tried it on a simple function with
just one check, it did not hang but I could see the user check was still
constructed into the error object, although it did not throw, so user checks can
not be really disabled.

Putting splits as external argument, the checkify+compilation time is 760ms.

I consider the attempt failed. I guess using stuff in jax.experimental is a bad
idea.

With skipifabstract back, and keeping splits unfolded, compilation time is
620ms.

..........

To do:
  - do timings (compilation and run) with reverse derivatives, with and without
    reset (DONE)
  - try to optimize compilation and run times

With m = 2, d = 10:

    Compiled calculation:
    52.3 ms ± 143 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compilation time:
    595 ms ± 808 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compiled calculation with derivative:
    321 ms ± 1.17 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compilation time with derivative:
    2.01 s ± 4.98 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

With m = 2, d = 1:

    Compiled calculation:
    45.7 ms ± 368 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compilation time:
    404 ms ± 10.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compiled calculation with derivative:
    162 ms ± 640 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
    Compilation time with derivative:
    594 ms ± 2.12 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

So derivatives exacerbate the effect of resetting multiple times, both on
compilation and execution time.

With m = 1, d = 1:

    Compiled calculation:
    9.97 ms ± 275 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compilation time:
    208 ms ± 3.78 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compiled calculation with derivative:
    17 ms ± 57.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
    Compilation time with derivative:
    357 ms ± 2.64 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

With m = 1, d = 10:

    Compiled calculation:
    13.2 ms ± 735 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compilation time:
    364 ms ± 4.56 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compiled calculation with derivative:
    120 ms ± 928 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
    Compilation time with derivative:
    1.54 s ± 3.63 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

m = 0, d = 1:

    Compiled calculation:
    800 µs ± 12.9 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)
    Compilation time:
    167 ms ± 1.07 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
    Compiled calculation with derivative:
    2.44 ms ± 26.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
    Compilation time with derivative:
    272 ms ± 6.32 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

m = 0, d = 10:

    Compiled calculation:
    831 µs ± 65.3 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compilation time:
    242 ms ± 1.33 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compiled calculation with derivative:
    28.1 ms ± 177 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
    Compilation time with derivative:
    611 ms ± 4.57 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

First try: use scan instead of loop.

m = 0, d = 10:

    Compiled calculation:
    3.1 ms ± 18.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
    Compilation time:
    193 ms ± 582 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
    Compiled calculation with derivative:
    32.5 ms ± 588 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
    Compilation time with derivative:
    382 ms ± 1.98 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

So I have reduced the compilation times, but the running time increases.

m = 1, d = 10:

    Compiled calculation:
    15.4 ms ± 65.3 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compilation time:
    238 ms ± 1.23 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compiled calculation with derivative:
    141 ms ± 1.66 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
    Compilation time with derivative:
    604 ms ± 8.59 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

Here the running time increase is smaller, about +15%.

m = 2, d = 10:

    Compiled calculation:
    52.1 ms ± 405 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compilation time:
    434 ms ± 1.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compiled calculation with derivative:
    327 ms ± 2.59 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compilation time with derivative:
    910 ms ± 5.39 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

Here the running time changes negligibly, but the compilation time improves
quite a bit.

I have to check if there are regressions with d = 1.

m = 0, d = 1:

    Compiled calculation:
    911 µs ± 38.5 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)
    Compilation time:
    184 ms ± 4.68 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compiled calculation with derivative:
    2.77 ms ± 62.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
    Compilation time with derivative:
    323 ms ± 1.71 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

m = 1, d = 1:

    Compiled calculation:
    10.3 ms ± 723 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compilation time:
    219 ms ± 1.45 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compiled calculation with derivative:
    20 ms ± 459 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
    Compilation time with derivative:
    471 ms ± 6.52 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

m = 2, d = 1:

    Compiled calculation:
    45.8 ms ± 322 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compilation time:
    418 ms ± 2.77 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compiled calculation with derivative:
    163 ms ± 516 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
    Compilation time with derivative:
    736 ms ± 4.23 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

Conclusion: there's some regression but I'd say not enough to justify keeping a
double implementation.

I try using a custom scan that unrolls the first iteration, and the second too
if there are only two iterations total.

@@@@@ m = 0, d = 1 @@@@@

    Compiled calculation:
808 µs ± 13.4 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)
    Compilation time:
169 ms ± 768 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
    Compiled calculation with derivative:
2.35 ms ± 38.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
    Compilation time with derivative:
269 ms ± 1.41 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

@@@@@ m = 0, d = 2 @@@@@

    Compiled calculation:
797 µs ± 15.4 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)
    Compilation time:
179 ms ± 539 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
    Compiled calculation with derivative:
3.9 ms ± 43.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
    Compilation time with derivative:
312 ms ± 4.26 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

@@@@@ m = 0, d = 10 @@@@@

    Compiled calculation:
2.91 ms ± 64.4 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compilation time:
201 ms ± 1.26 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compiled calculation with derivative:
32.2 ms ± 718 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
    Compilation time with derivative:
427 ms ± 4.99 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

@@@@@ m = 1, d = 1 @@@@@

    Compiled calculation:
10.1 ms ± 206 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compilation time:
205 ms ± 1.74 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compiled calculation with derivative:
16.8 ms ± 174 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
    Compilation time with derivative:
358 ms ± 2.86 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

@@@@@ m = 1, d = 2 @@@@@

    Compiled calculation:
10.3 ms ± 251 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compilation time:
223 ms ± 1.52 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compiled calculation with derivative:
25.6 ms ± 379 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
    Compilation time with derivative:
479 ms ± 1.59 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

@@@@@ m = 1, d = 10 @@@@@

    Compiled calculation:
14.9 ms ± 56.4 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compilation time:
257 ms ± 1.63 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compiled calculation with derivative:
131 ms ± 766 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
    Compilation time with derivative:
696 ms ± 4.65 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

@@@@@ m = 2, d = 1 @@@@@

    Compiled calculation:
45.2 ms ± 346 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compilation time:
395 ms ± 1.49 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compiled calculation with derivative:
158 ms ± 355 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
    Compilation time with derivative:
593 ms ± 3.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

@@@@@ m = 2, d = 2 @@@@@

    Compiled calculation:
45.2 ms ± 274 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compilation time:
421 ms ± 4.39 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compiled calculation with derivative:
168 ms ± 359 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
    Compilation time with derivative:
744 ms ± 3.14 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

@@@@@ m = 2, d = 10 @@@@@

    Compiled calculation:
51 ms ± 401 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compilation time:
456 ms ± 3.82 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compiled calculation with derivative:
313 ms ± 4.54 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compilation time with derivative:
1.03 s ± 3.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

I improved the compilation times. Currently on m=2 compiling with the derivative
is about 2x compiling without, I don't think I can do better than this.

Yet I don't like that computing the derivative takes 3x the runtime at d=1, and
6x at d=10.

First obvious optimization: for m=2, I can use d=5 in practice, since
convergence is faster.

@@@@@ m = 2, d = 3 @@@@@

    Compiled calculation:
48.4 ms ± 1.53 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compilation time:
475 ms ± 19.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compiled calculation with derivative:
187 ms ± 325 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
    Compilation time with derivative:
1 s ± 4.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

@@@@@ m = 2, d = 5 @@@@@

    Compiled calculation:
48.1 ms ± 167 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compilation time:
457 ms ± 2.63 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compiled calculation with derivative:
230 ms ± 2.35 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compilation time with derivative:
1.01 s ± 4.19 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

d=5 gets me 310ms -> 230ms.

But is there a way to optimize the expressions for the reverse mode derivative
calculation?

The derivatives pass through pnt, so maybe I could gather pnt as much as
possible.

@@@@@ m = 2, d = 1 @@@@@

    Compiled calculation:
45.1 ms ± 204 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compilation time:
398 ms ± 2.25 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compiled calculation with derivative:
160 ms ± 188 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
    Compilation time with derivative:
582 ms ± 5.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

@@@@@ m = 2, d = 2 @@@@@

    Compiled calculation:
46.1 ms ± 143 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compilation time:
431 ms ± 7.96 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compiled calculation with derivative:
168 ms ± 1.36 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
    Compilation time with derivative:
708 ms ± 6.22 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

@@@@@ m = 2, d = 10 @@@@@

    Compiled calculation:
51.4 ms ± 340 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compilation time:
456 ms ± 2.59 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compiled calculation with derivative:
274 ms ± 2.83 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compilation time with derivative:
945 ms ± 2.83 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

Result: execution without derivative might be slightly slower, execution with
derivative does 313(5) -> 274(3).

I check that returning only the derivative does not change the times:

@@@@@ m = 2, d = 10 @@@@@

    Compiled calculation:
52 ms ± 583 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compilation time:
454 ms ± 4.42 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compiled calculation with derivative:
273 ms ± 2.37 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compilation time with derivative:
948 ms ± 2.67 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

=> not changed.

I'ts still 5x the time at d=10, 3x at d=1. I thought I could get away with much
less since the expensive part of the calculation does not involve the derived
variables at all.

Forward derivative:

@@@@@ m = 2, d = 10 @@@@@

    Compiled calculation:
51.5 ms ± 663 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compilation time:
456 ms ± 2.74 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    Compiled calculation with derivative:
63.3 ms ± 590 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
    Compilation time with derivative:
539 ms ± 11.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

2023-04-03
==========

Right now the gradient of the likelihood in forward is O(n^3d) because I do
K⁺∂K/∂θ and then take the trace. It would be O(n^3 + n^2d) if instead I first
formed K⁺ and then contracted both indices at once. I'm marking this in the
sketch of the new linalg system.

2023-04-15
==========

Debugging the lbfgs bug.

I can not install the old stuff because binaries for arm are missing and jaxlib
does not run on rosetta2.

=> special-case the failing test

........

Todo next: try to reproduce the failing gvar installation locally (in a fresh
venv, without using cached wheels) and fix it. Installing first numpy separately
did not work on CI.

Installing from tests-requirements.txt does not trigger the problem locally.

Installing lsqfitgp does not trigger the problem locally.

=> It's pip 23.1, I pinned it and it's fixed.

2023-04-16
==========

Release notes for v0.15:

This release mostly features further improvements to the BART kernel. According to the benchmarks I am going to show at EUROCIM 2023, GP regression with this kernel and hyperparameter tuning beats the original cross-validated BART MCMC both in statistical and computational performance. Following the ACIC challenge 2022 results, this implies that it's probably SoTA on the standard causal inference in decounfounded observational study setting, although that is still to be verified.

### BART
  * Strongly optimized implementation, both on memory usage and running time.
  * Computation is batched by default to avoid creating large intermediate matrices when there are many covariates.
  * Good choices for technical parameters are documented.
  * `BART.correlation` also accepts total splits and indices w.r.t. the grid instead of split count separations.
  * Example script with full BART inference.

### Hyperparameters fit (`empbayes_fit`)
  * New parameters:
      - `covariance` to set how to estimate the posterior covariance matrix
      - `fix` to fix some hyperparameters to the initial values
      - `mlkw` to pass additional keyword arguments to `GP.marginal_likelihood`
      - `forward` to use forward autodiff for the likelihood gradient; it uses a partially reverse autodiff calculation to avoid the $(n^3d)$ complexity of full-forward mode
  * Removed the `'hessian'` and `'hessmod'` values of parameter `method`. Use `'fisher'` instead. **<-- BREAKING CHANGE**
  * New attributes:
      - `prior`: the prior on the hyperparameters, as gvars
      - `initial`: the starting values for the MAP search
      - `fix`: the mask indicating locked hyperparameters
      - `gpfactory`: the `gpfactory` argument
  * Additional logging: per task timing and function calls count.
  * Copula factories for the Beta and Inverse Gamma distributions, in the submodule `copula`.

### `StructuredArray`
  * New function `unstructured_to_structured` to create a `StructuredArray` from an unstructured array.
  * Arrays are now read-only. Setting the fields requires a JAX-like syntax with `.at[field].set(value)`. **<-- BREAKING CHANGE**
  * Added methods `squeeze` and `astype`, although the latter refuses to change the type.
  * Pytree unflattening tries to infer shape and dtype if the leaves are not compatible with the original structure. The inference is incomplete and ambiguous. **<-- BREAKING CHANGE**
  * Implemented numpy functions `ix`, `empty_like`, and `empty`.

### Kernels
  * New `CrossKernel` parameter `batchbytes` to batch the calculation, and corresponding method `batch`. Requires jax-traceability of the kernel implementation.
  * The `Decaying` kernel gets an explicit real exponent to indicate infinite divisibility.

### GP
  * By default, even if `checksym=False`, the full covariance matrix is computed for diagonal blocks, unless the additional new option `halfmatrix=True` is set. **<-- BREAKING CHANGE**

### Dependencies
  * Pinned jax and jaxlib to 0.4.6. Earlier versions severly break the optimization of the BART kernel, while later versions break the `jit` option in `empbayes_fit`. Sorry for this pin, I do know that dependency pins in libraries are evil. **<-- BREAKING CHANGE**
  * Tested on Python 3.11

2023-04-24
==========

The source distribution is including docs/, examples/, playground/, pyenv/. I
have to remove them somehow.

2023-06-23
==========

CI failures:
 - tests/test_raniter.py::test_raniter_packing (I think it's about missing seed)
 - old tests: gvar and lsqfit do not install, probably fixed by pinning pip

2023-06-24
==========

pip is already pinned. What? Maybe they backported something? I'll try locally.

Locally, gvar 11.10.1 -> 11.11 works, lsqfit <12 -> 13 works too. But locally
it's using setup.py install, while online it's trying to build a wheel
apparently => use the option --no-use-pep517.

2023-07-05
==========

If I cycle over all datasets in playground/acic2022/track2.py, then I'll fill
the RAM with the gvar global covariance matrix. switch_gvar keeps a full
history, I have to clear gvar._GVAR_LIST. (Maybe open an issue, a minimal
interface would be switch_gvar(keep_history: bool). => DONE)

=> Answer by Lepage:
1) use switch_gvar and restore_gvar in pairs
2) gvar.gvar = gvar.gvar_factory()

I prefer 1, looks less hacky, can be packaged in a context manager.

2023-07-07
==========

Possible next things to do on acic:
- regress together treatment and outcome
- sample hyperparameters (DONE)
- cycle over all datasets
- check heterogeneous effects
- add other summary covariates by reading the per-patient files
- fit covariate weights
- sample hyperparameters for ps

2023-07-10
==========

Sampling hyperparameters is very slow, 5.6s per prediction. I guess it's mostly
compilation time, the batcher+kernel probably gets compiled thrice, one for each
covariance matrix block.

I should jit a cached prediction function in bayestree.bart. => Success! 6
predictions/s.

Sampling the hyperparameters does not make a relevant difference on dataset
0001.

2023-07-11
==========

I want to regress together treatment and outcome.

1) Map treatment to continuous values as in ps fit (reuse script variables)
   (DONE)
3) Implement separate hyperparameters (mean, var, error var) in bayestree.bart
   by passing arrays of integer indicators for each
4) Concatenate continuous treatment with outcome
4a) Concatenate covariates with themselves
4b) Add a binary covariate that marks treatment (0) / outcome (1)
4c) Convert Z to ternary
5) Run the regression
6) Separate the results in ps + outcome
7) Compare ps with the one from the standalone treatment fit
8) Impute the missing outcomes and compute SATT as usual

How do I deal with the Z covariate? I can't use it because it's an outcome of
the regression. I need to transform it to a ternary covariate:

    Binary Z   Outcome indicator   Ternary Z
    0          0                   1
    0          1                   0
    1          0                   1
    1          1                   2

Ternary Z determines Outcome indicator. Should I drop the latter? Adding it
makes propensity score and outcome slighly less correlated a priori.

=> This plan does not make much sense now that I noticed my way of regressing
the propensity score is crap.

2023-07-17
==========

I want to make a release to use the latest bart interface improvements in code I
hand out for lectures.

Release notes for v0.17:

### BART with `lsqfitgp.bayestree.bart`
 * Example on the ACIC data challenge
 * Error weights
 * New methods `bart.gp()`, `bart.data()`, `bart.pred()`
 * All methods can sample hyperparameters with Laplace
 * The prior mean is a free hyperparameter
 * The prior on the error variance is scaled with the data, in particular `bart` now works well with non-standardized data

### `raniter`
 * Accept user provided random number generator
 * Solved bug with scalar elements in dictionaries

### `empbayes_fit`
 * The arguments in `gpfactorykw` are now passed through a function call instead of being closed over. This implies that, if `jit=True`, large arrays can be passed efficiently without being hardcoded in the compiled code, and that arguments not understood by JAX can not be passed through `gpfactorykw`.

2023-07-18
==========

Urgent: fix the usage of weights in examples/acic for the treatment fit,
currently it overfits badly.

......... on it .........

I used the weights as usual in a regression, as denominators under the error
variance. However this does not map to a meaningful probability when the
regression function is squeezed through the probit.

These weights are unit counts. The most obvious choice would be to map the
discrete Z to 1 / N. However, the continuous Z represents the probability that Z
was indeed 1, so this is not the correct usage: I'm as sure as any other Z about
its value, what changes is how much information it gives about other outcomes.
So I have to apply the weight to the standard deviation, but in a different way
than usual.

On second thought, using weights for the treatment regression does not make
sense. The treatment was assigned at the hospital level. I should use the number
of patients as covariate, both in the treatment and outcome regression.

=> Correct, but not sufficient to fix the overfitting. The problem seems to be
that the error sdev of the treatment fit gets optimized to a very small value,
0.07.

Fitting a similar dataset with dbarts.bart2 with proper probit works.

(Note: dataset 12 upsets the treatment fit)

=> I thought about it and I may have understood the reason. Since I pass
discrete z_continuous values, the error variance is correctly inferred to be
very small.

2023-07-27
==========

I want to write a deterministic p.s.d. matrix generator with the following
properties:

 1) Depends on one continuous parameter with a smooth formula
 2) Specified rank
 3) Image well-conditioned
 4) Both the spectrum and the null space depend on the parameter
 5) Random orientation

I can start from ToeplitzBase.mat. Let s be the parameter. Determine the
eigenvalues with

    where(arange(n) < rank, 1 + eps + cos(s + arange(n))), 0)

Look at a distribution to check this formula makes sensible spectra. Then
generate a random nxn orthogonal matrix. Then select a pair of indices at
random, and build a rotation on that submatrix with angle s. Apply the two
transformations to the diagonal matrix. Of course, draw the random properties
once and for all: unit test classes are re-instantiated for each test.

=> DONE

2023-07-28
==========

I completed Cholesky with the new decomposition interface. Next steps: use it in
GP (DONE), then in empbayes_fit (DONE).

2023-07-29
==========

Seems to solve the tracing problems. Next:
- solve new bugs due to the jax 0.4.14 update (DONE)
- optimize the likelihood calculation in _decomp.py (DONE)
- optimize the derivatives in _fit.py (DONE)
- use the rng fixture for random generators in all tests (DONE)
- try to use pytest's cache for test_special.py (DONE)
- try to speed up test_pred.py (DONE)
- profile the loading of test_kernels.py (DONE)

..........

Idea for the next release title: "dear lord deliver us from pins", which ChatGPT
refines to "Our code, hallowed be thy dependencies, but deliver us from pins."
After a bit more of ChatGPTing, it's "Our code, hallowed be thy dependencies,
but deliver us from pins and torment," such that "pins" possibly makes you
recall "pain." I'm not confident this works on English speakers since I'm not
one myself.

2023-07-31
==========

`pytest --lf` runs only the tests that failed in the last invocation.

..........

The pytest cache is not appropriate to commit because it's mixed in with the
information on failed tests. I have to design a small caching system from
scratch.

The end result I want is functions I call to output basic python values, that
behind the scenes check if the result is in cache. Maybe python has other
pre-made caching systems for this? => No, it doesn't.

- make a file tests/cache.py
- define a class DiskCachedDict
- it takes a file path on init
- it behaves like a dict, but loads values from disk on init, and writes them
  back on del
- define a class DiskCachedCallable
- takes a DiskCachedDict on init
- it's __call__ is a decorator that takes in a function and spits out a function
  that uses the cache with a identifier based on the function name

=> DONE

.........

The profiler says that most of the loading time of test_kernels is jax
compilation. Sorting by cumtime, it's all in creating the BART hyperparameters
list, calling BART.splits_from_coord. => Fixed by jitting the method.

Now the total pytest dry running time is < 2s.

.........

Since I'm doing maintenance, I could finish it before starting again on the main
work. Things to do:

 - use the src/ layout (DONE)
 - avoid packaging extraneous python source files in the source distribution 
   (DONE)
 - use tox for the tests
 - reintroduce pdf9.py by dropping woodbury (DONE)
 - make a release to remove the jax pin (DONE)

=> tox is too much work right now so I'm skipping it

I'm trying to add Windows instead. First error: can not load the test_special
pickle cache because it requires to instantiate a PosixPath. Where the heck is a
PosixPath hidden in the pickle??

Alternative plans:
 1) Understand where the PosixPath come out from and get rid of it
 2) Fix the json cache backend and use it in place of pickle

2023-08-01
==========

I profiled the test suite, result sorted by cumtime. Saved it
playground/testprofile-2023-07-31.txt.

............

Othe plan:
 3) Hack pathlib.PosixPath temporarily

..........

I chose (2), json. It was easier than expected: gzip.open supports text mode, it
just isn't the default.

New problem: opening and saving the cache is not idempotent. This is a problem
because it is committed. I have to change the logic, and I should also set
things up to detect if the cache keys are corrupted due to changes in json
encoding.

Solution:
 - rename DiskCacheDict to CommittedDict
 - if the file exists, CommittedDict ignores a writing request, printing a
   message, and refuses to set new keys
 - if the file does not exists, works like now

=> DONE

..........

The new cache loaded on Windows. The next major hitch is that jax crashed during
the kernels tests. I can't do much about it, so the convenient thing to do would
be to let the windows test run each time without stopping the workflow from
completing, and notice when they stop crashing.

Workflow jobs have an `if:` field that sets condition for running. The default
is `success()` which is true if required jobs were successful. I try to write my
own expression:

    if: success() || (failure() && (only windows failed))

    if: success() || (needs.tests.important_failures is empty),
    then in the tests job I use outputs: with an if to mark failures somehow?

Ok I can't wrap my head around how to do this quickly. I'd probably have to
define a conditional step in test that outputs the matrix info, and then write
an unwieldy expression to check only windows failed.

=> Found it! use continue-on-error with a matrix switch.

It fails because experimental is not defined apart from windows. I have to make
the expression evaluate to true if experimental is the empty string.

..........

The online coverage report is empty.

The coverage debug log on CI shows nothing weird apart from there being 0 lines
covered in the coverage data files.

I can not discern anything by looking directly at the CI coverage data files.

I try running offline.

...........
.   
.   test_fit.py looks slow. It takes 45s. I try using the jit. => Now it takes 10s.
.   
.   Some tests in test_special.py look slow too. Maybe jitting the special functions
.   would be useful? => adding jax.jit in _bessel.py below custom_jvp makes the
.   tests fail with concretization errors, I'll look into it another time (DONE)
.   
...........

Offline it mostly works, although as usual the lines tested in the examples are
ignored, but for those executed to import modules.

Looking at the CI log of unit tests, coverage is already complaining of no
collected things. Ok I think I understand: locally I'm installing lsqfitgp in
editable mode, on CI in normal mode, so the source files are copied to another
location.

Coverage says that the source spec can be a importable module name, so I can
just remove the path from .coveragerc.

............

Why does the examples not count for coverage? Maybe it's exec?

I think I got it. Running only one example, the code I expect to find covered
is covered. coverage does not accumulate coverage, it overwrites COVERAGE_FILE
at each invocation. => fixed

............

New problem: tests (ubuntu-latest) fails with

FAILED tests/test_special.py::test_power_diff[qzero-xnearzero-medrange-neg] - TypeError: 'mappingproxy' object does not support item assignment
FAILED tests/test_special.py::test_power_diff[qzero-xnearzero-shortrange-neg] - TypeError: 'mappingproxy' object does not support item assignment
FAILED tests/test_special.py::test_power_diff[qzero-xnearzero-tinyrange-neg] - TypeError: 'mappingproxy' object does not support item assignment
FAILED tests/test_special.py::test_power_diff[qone-xnearzero-medrange-pos] - TypeError: 'mappingproxy' object does not support item assignment
FAILED tests/test_special.py::test_power_diff[qone-xnearzero-medrange-neg] - TypeError: 'mappingproxy' object does not support item assignment
FAILED tests/test_special.py::test_power_diff[qone-xnearzero-shortrange-pos] - TypeError: 'mappingproxy' object does not support item assignment
FAILED tests/test_special.py::test_power_diff[qone-xnearzero-shortrange-neg] - TypeError: 'mappingproxy' object does not support item assignment
FAILED tests/test_special.py::test_power_diff[qone-xnearzero-tinyrange-pos] - TypeError: 'mappingproxy' object does not support item assignment
FAILED tests/test_special.py::test_power_diff[qone-xnearzero-tinyrange-neg] - TypeError: 'mappingproxy' object does not support item assignment
FAILED tests/test_special.py::test_power_diff[qeven-xnearzero-medrange-pos] - TypeError: 'mappingproxy' object does not support item assignment
FAILED tests/test_special.py::test_power_diff[qeven-xnearzero-medrange-neg] - TypeError: 'mappingproxy' object does not support item assignment
FAILED tests/test_special.py::test_power_diff[qeven-xnearzero-shortrange-pos] - TypeError: 'mappingproxy' object does not support item assignment
FAILED tests/test_special.py::test_power_diff[qeven-xnearzero-shortrange-neg] - TypeError: 'mappingproxy' object does not support item assignment
FAILED tests/test_special.py::test_power_diff[qeven-xnearzero-tinyrange-pos] - TypeError: 'mappingproxy' object does not support item assignment
FAILED tests/test_special.py::test_power_diff[qeven-xnearzero-tinyrange-neg] - TypeError: 'mappingproxy' object does not support item assignment
FAILED tests/test_special.py::test_power_diff[qodd-xnearzero-medrange-pos] - TypeError: 'mappingproxy' object does not support item assignment
FAILED tests/test_special.py::test_power_diff[qodd-xnearzero-medrange-neg] - TypeError: 'mappingproxy' object does not support item assignment
FAILED tests/test_special.py::test_power_diff[qodd-xnearzero-shortrange-pos] - TypeError: 'mappingproxy' object does not support item assignment
FAILED tests/test_special.py::test_power_diff[qodd-xnearzero-shortrange-neg] - TypeError: 'mappingproxy' object does not support item assignment
FAILED tests/test_special.py::test_power_diff[qodd-xnearzero-tinyrange-pos] - TypeError: 'mappingproxy' object does not support item assignment
FAILED tests/test_special.py::test_power_diff[qodd-xnearzero-tinyrange-neg] - TypeError: 'mappingproxy' object does not support item assignment

It'all non-skipped xnearzero variants (22/24). Locally it works. On all other CI
environments it works. 

Example missing key: 
    
    key = '["zeta_series_power_diff", [9.999999999999999e-101, 3, -4e-16], {}]'

Maybe it was saved as 1e-100?

=> Yes, looks like it is and is saved as 1e-100, but for some reason on
ubuntu-22 now it gets written as 9.999999999999999e-101.

Searching in the log, I am sure it's always the 9.999999999999999e-101 that's
causing problems.

So the keys on file contain 1e-100, while the decorator sees
9.999999999999999e-101. I have to convert any 9.999999999999999e-101 that
appears in the json key to 1e-100.

2023-08-02
==========

The missing key was always the same only because the order is fixed and the test
stops on the first error. Now the missing key is another number. It's

    cache          1.1006941712522216e-96
    ubuntu-latest  1.1006941712522214e-96

I'd like to patch this without re-committing the >1 MB cache again. A solution
would be to write all floats in the keys with 14 digits, while in the items I'd
prefer to retain full precision.

Plan:
- upon loading the cache, parse all keys back from json, re-serialize them
- change the key serializer to use 14 digits for floats

Problem: python's json does not allow to change the float format. I have to
change everything to make it more sensible.

The central question is wether the inputs are actually changing or if it's just
the representation. Python specifies that floats are printed with enough digits
that their exact original value can be reconstructed, and I checked that the two
representations above lead to different floats. So maybe numpy on ubuntu is
generating slightly different values out of np.logspace(), they would take that
freedom since it should not be so important.

So my caching system has to be tolerant of actually different input values.

Another desideratum I didn't take into account is that it should not be
necessary to recommit a large cache each time I change one bit of the test
module.

Solution: the cache should be provided as a per-test fixture. The cache object
writes a file per test in a directory, using the (sanitized) test name as file
name. => better: it represents the path as a directory. Example usage:

    in file tests/cippa/lippa.py:

        def test_ciao(cached):
            x = cached('f1', f, *args1)
            y = cached('f2', f, *args2)
            assert x == y

    this creates the following files:

        tests
            cache
                cippa
                    lippa.pickle

    lippa.pickle contains:

        {'f1': x, 'f2': y}

So the cache object never tries to look at the arguments. It's the caller
responsibility to give unique names.

As it works now, if the file lippa.pickle exists, the cache refuses to write new
keys into it.

=> DONE, using json for portability

..........

Now it is time to make a release. I'll wait for the CI to complete successfully.

Release notes for v0.18:

# Our code, hallowed be thy dependencies, but deliver us from pins and torment

Recent version of `lsqfitgp` would impose version 0.4.6 of `jax` and `jaxlib` due to incompatibilities with newer releases. This limitation is now gone. Thanks to the experimental Windows support added to `jax`, `lsqfitgp` can be `pip install`ed on Windows. The unit tests currently have some failures and crash midway, so I guess it's not fully usable.

### New decomposition system

The covariance matrix decomposition system has been scrapped and rewritten. Consequences:

  * All solvers but `'chol'` are gone, they were never useful in practice anyway. I may add new solvers in the future. As such, it is not necessary to specify `GP(... solver='chol')` to speed up the fit.

  * It is not possible to pass a decomposition in place of `givencov` to use the Woodbury matrix formula.

  * Operations with decompositions can not be differentiated; instead, there is a method that computes the Normal density and its derivatives with custom code. This is faster because it avoids redundant calculations.

  * `GP.marginal_likelihood` loses the `separate` option. * `lsqfitgp.raniter` will fail if the matrix is not p.s.d. instead of issuing a warning and then working around the problem.

  * The `'fisher'` method in `empbayes_fit` now works decently enough that it can be used in practice, same for `covariance='fisher'`.

### Bugs fixed

  * `empbayes_fit(..., minkw=dict(method='trust-constr')` would not work

.........

Note: make a jax issue about the tracing errors hierarchy.

.........

CI ok but for coverage report. It says

    No source for code: '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/lsqfitgp/_Deriv.py'.

The "print coverage debug" item is un-uncollapsible. I download the coverage
data and look at it locally.

=> The problem, as I expected, is that lsqfitgp is not installed in the coverage
report part, so the sources are not found in the same place.

Possible solutions:
- Install lsqfitgp in the coverage report part and add installation paths to
  .coveragerc. This is feasible if I can use glob syntax for the paths, since
  Python's minor version appear in the installation directory.
- Use an editable install, locally this makes coverage locate the source files
  in the source directory instead of the installation path.

The second option looks quicker and more robust. => coverage supports globbing.

I test it locally with the CI files. Right now, the online error is reproduced.
I add the paths with the glob patterns.

.........

Next thing to do: get back to applications.

I have to fix the treatment fit in the acic example. The short way is using
something from statsmodels instead of bart. The long way is implementing a
logistic regression, or more in general a glm.

2023-08-03
==========

First use statsmodels, such that at least it is not total crap, then read
Rasmussen's chapter on binary classification.

I fit the treatment with a standard logistic regression. The end result for the
SATT is poorer than with the total overfit I had before. I understand how this
happens, but I need to try to understand what is the proper statistical way of
putting down everything.

With complete overfit, I get ps = Z. So when I impute the missing potential
outcomes, there's perfect overlap on the variable ps. BART reasons in terms of
observed range of the variable, so observed and imputed outcomes are maximally
separated along ps. The non-interacting component w.r.t. ps is minimally
observed. This increases the posterior variance.

The question then is why shouldn't this be the correct way of making the
inference. The answer is that this won't work out of sample; I don't notice it
here because I'm only estimating in-sample statistics. Does it makes sense
though to use Z_obs in place of PS when I only care about in-sample? You know
it won't generalize, but.

So, for the sake of methodology and giving good examples, I want to make the
fit work with an actual externally valid PS instead of Z.

First thing: look at the PS distribution by treatment group. To make a maximally
informative plot, I have to make the representation agree with the way BART
processes the PS. So use the rank.

Result: they seem decently separated. I expect I could recover the performance
of the in-sample trick by tuning the BART prior.

First idea: increase the weight of PS. Maybe by 5? => The posterior variance
increases, but by a very small amount. I should go extreme and put half the
weight on ps. => It doesn't work. The difference is minimal. I don't really
understand how these weights influence the result.

Fit with uniform weights:

    alpha = 0.80(20) (0 -> intercept only, 1 -> any)
    beta = 0.92(72) (0 -> any, ∞ -> no interactions)
    mean = 1169(193)
    latent sdev = 250(78) (large -> conservative extrapolation)
    data total sdev = 223
    error sdev (avg weighted) = 115.9(7.1)
    error sdev (unweighted) = 2236(137)

Fit with half the weight on ps:

    alpha = 0.76(23) (0 -> intercept only, 1 -> any)
    beta = 2.5(1.3) (0 -> any, ∞ -> no interactions)
    mean = 1203(276)
    latent sdev = 354(74) (large -> conservative extrapolation)
    data total sdev = 223
    error sdev (avg weighted) = 123.1(6.0)
    error sdev (unweighted) = 2376(116)

So putting the weight on ps did:
  - increase beta
  - increase mean variance
  - increase error variance

2023-08-04
==========

The variance increase, I guess, is because the fit is poorer. The increased beta
means that it's using less interactions. I think the story is that the weights
apply recursively, so it's not like having 50% of the total variance on f_ps(ps)
and 50% on the other functions, all the other variables prefer to interact with
ps at any order, this constraint makes it less amenable to use interactions to
explain the data, and so interactions drop.

What I really want is something like bcf, with two barts, one on ps and the
other not, with similar variances. This is not like putting ≈50% of the internal
bart weight on ps.

Plan:
 - skim the bcf paper (DONE)
    - automatic coding version of bcf
        Equation 5.6, section 5.3, page 980:

            y_i = μ(x_i) + τ̃(x_i) b_z_i + ϵ_i,
            b_0 ~ N(0, 1/2),
            b_1 ~ N(0, 1/2).

            "Our experiments below all use this parameterization, and it is the
            default implementation in our software package."

        Problem with this model: I have to do Laplace on b_z, because they are
        multiplied by τ̃, but since their prior is symmetric and τ̃'s prior is
        even, there will be two posterior modes with swapped bs and opposite τ̃.
        I would expect this to affect their MCMC inference too, but maybe the
        modes are well separated enough that you randomly get stuck in one and
        don't care otherwise. Maybe it would be fine for me as well?

        I think it's worthwhile to find a well-identified alternative.

        The extract the variable scale of τ, leaving τ̃, and put into the b
        terms. I guess this is convenient to implement a Gibbs sampler. I don't
        have this design constraint, so I can directly put a parameter in [0, 1]
        that shifts the levels of z:

            y_i = μ(x_i) + τ(x_i) (z_i - z_0) + ϵ_i,
            z_0 ~ Beta(a, b).

        So if z_0 = 0/1, μ is the control/treatment response surface, while τ is
        always the treatment - control effect. If z_0 = 1/2, then μ is the mean
        of the treatment and response surfaces.
    - log-transform the outcome
        I wasn't doing this, obvious mistake. My outcomes are all positive. I
        should look at the distribution of Y and decide on a transformation,
        my a priori guess is log.

        => Afterward I recalled I had already looked into it and it's already
        quite Normal. It's because the outcome is already an average. Changing
        the scale would complicate the inference, since the target estimand is
        for differences over the original scale.

        That said, using log improves the inference.
    - specific choices for the prior distribution and hyperparameters
        Hypers of μ:
            scale: half-Cauchy with prior median = 2 std(y)
            alpha = 0.95
            beta = 2
        Hypers of τ:
            scale: half-Normal with prior median = std(y)
            alpha = 0.25
            beta = 3
 - skim kokandakar 2022 to see if they do something important differently (DONE)
    - package flexBCF
        - I failed to install it, opened an issue => fixed
        - I fail to load it, opened an issue
            This time the author could not fix it. Things to do:
                - try to install directly from source
                - reinstall the latest version of R and try again
    - they use as covariate the CBPS of Imai and Ratkovic (2014)
        The CBPS tries to enforce covariate balancing irrespective of treatment
        model well-specification. The abstract says this is good for matching
        and weighting. Since I'm doing a nonparametric regression with all the
        covariates included, I expect that the CBPS would not give me an
        advantage, also considering that the implementation does a logistic
        linear regression.
    - they use Linero's variable selection scheme
        They say it did not help.
    - does not one-hot encode categorical covariates
        I think I understand why it does not work. If I split values in
        dimensions, BART never considers the hypothesis that similar
        coefficients for a bunch of values might mean that it's the same
        coefficient. It outright won't search for sparsity in that way. Like an
        ordinary Bayesian linear regression wouldn't.

        Since this seems an overwhelmingly general problem, I wonder what he's
        going to do to fix it in the cited article. The first thing that comes
        to my mind is enconding the values in binary, so each indicator splits
        in half the values space. The no interaction part, the larger in BART,
        then assumes that the means in hierarchical groups are related in the
        same way across groups the same size.

        Example with 8 values:

            1   2   3   4   5   6   7   8

        A: 1234 vs 5678
        B: 12 vs 34, 56 vs 78
        C: 1 vs 2, 3 vs 4, 5 vs 6, 7 vs 8

        This does look somewhat sensible.

        The natural next question is why shouldn't one do this across
        covariates. Answer: concatenating the binary encoding of each covariate
        is already the binary encoding of all the covariates. It's not exactly
        like fixing the splits at binary cutpoints for a regression tree,
        because here I can decide to split immediately on any bit, while with a
        tree I'd have to split in order.

        Using BART with binary covariates in general is a bit weird because the
        point of BART is that it is happy to follow any shape along each
        covariate, but does not like interactions between them. With binary
        covariates, there's no shape, and the game is all on the interactions.

        Also, my approximations to compute the covariance function may be too
        rough here, since they rely on sticking to depth 2 and repeating it
        forever. I don't know.

        What they propose in Deshpande (2023): arbitrary decision rules. In
        particular, throw a coin for each level of the covariate to put it to
        the left or right. This is invariant under permutation, which is
        desirable.

        Since their prior seems so generic, where's the catch? When you let the
        model do anything, the difficulty must move somewhere else. I see two
        possibilities: 1) that prior is restricting hypotheses in some other
        way, 2) controlling the MCMC will be nasty. For (1), it could be that
        they fix the typical size of the groupings, while BART splits only along
        a fixed order, but with uniformly random size.
 - search on google scholar if there has been new developments (DONE)
    I only found two new articles searching "Bayesian causal forest", and one
    is not accessible. The other is Caron et al. (2022). However, that article
    just adds in the sparsity method of Linero. Nothing new then.
 - implement bcf as a new class in lsqfitgp.bayestree (DONE)
 - see if it does better than bart+ps on acic's dataset 0001

2023-08-05
==========

I'm trying to set up a mirror. Apparently, I can not do this with a pre-packaged
option in github, I need to configure an action to push automatically to the
mirror repo.

- use the mirror-repository action
https://github.com/marketplace/actions/mirror-repository
- configure ci to run only on the original repo
    - use the condition ${{ github.repository == "Gattocrucco/lsqfitgp" }}
    - put the condition in jobs.<job>.if

=> I dropped it because it is not functional. I would like an official mirroring
system that clearly flags the other repo as alias, to put eventually my repo
under the repo list of an organization without relinquishing possession.

2023-08-12
==========

I think I have encountered the following bug: when I pass a BufferDict through
a jit boundary, if the BufferDict contains arrays, then the pytree def is
compared to check for preexisting compilations, and two things happen:
    1) The random value in the skeleton makes the comparison fail;
    2) If the BufferDict contains arrays, they are compared directly, and the
       comparison fails due to underfined truth value.
Check if this actually happens, and if so fix it. (DONE)

..........

I finished BCF and tried it on acic2022_0001. It works much better than BART-ps.
However, since the true effect here is close to zero, I wonder if bcf is broken
and just shrinks the effect. To check this, I can shift the outcomes of the
treated units by some amount and see if the result follows suit.

=> I guessed right, bcf always gives effect 0, while bart-ps follows the shift.

=> The mistake was simple: I switched Z twice, so I was computing the difference
between in-sample and out-of-sample on the same units.

Result: bcf gives a result quite similar to bart-ps.

..........

I'm still not satisfied. I don't have clever ideas to improve the model. Things
to try:

    - fit ps with a nonparametric model instead of logistic regression, I can
      first use an R bart package and then implement my own

    - fit the outcome with a linear regression y ~ (z + ps) * x, see if it's
      indeed poorer that bart

    - add a separate regression term to bcf that depends on ps only

    - use DID bcf like kokandakar 2023, I have to use z=post * Z and fit
      pretreatment outcomes too

    - log transform the outcomes

...........

I tried to set kernelkw=dict(intercept=False). I pre-registered that nothing
relevant would change because I'm fitting both the mean and the scale. Indeed
the only visible effect was decreasing a bit the fitted scale.
