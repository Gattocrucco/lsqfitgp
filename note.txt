La formula con le gvar per il processo gaussiano è

    y* = y*_p + Kxsx (Kxx + Cy)^-1 (y - y_p)

Avevo già controllato che questa formula mi dà la matrice di covarianza
corretta per y*. Però vengono fuori anche le correlazioni tra y* e y, y_p,
y*_p. Che cosa rappresentano statisticamente?

Assumiamo che y e y(*)_p siano indipendenti. Allora

    Cov[y*, y*_p] = Cov[y*_p] = Kxsxs

    Cov[y*, y_p] = Cov[-Kxsx (Kxx + Cy)^-1 y_p, y_p] =
                 = -Kxsx (Kxx + Cy)^-1 Kxx
    
    Cov[y*, y] = Kxsx (Kxx + Cy)^-1 Cy

2022-04-17
==========

Cose da fare per le PDF:

1)  Mi invento io delle funzioni e dei dati dal culo e faccio un fit,
    solo roba lineare (FATTO)
2)  Aggiungo roba quadratica (sempre dal culo) (FATTO)
3)  Aggiungo l'errore sulle M (FATTO)
4)  Uso delle PDF vere anziché culiche
5)  Aggiungo i dati veri

2022-04-18
==========

Come implemento addproc etc.?

1)  Chiamo _KernelDeriv -> _CrossKernel (FATTO)
1a) Sposto la logica di diff fuori da fun e giro i test (FATTO)
1b) Modifico diff in modo da tener conto che le x a sinistra possono essere di
    tipo diverso dalle x a destra (SALTARE)
2)  Sposto _binary in _KernelBase e tolgo il controllo che gli scalari siano
    positivi, come classe uso type(self) anziché Kernel (FATTO)
3)  Sovrascrivo _binary in Kernel controllando che l'eventuale scalare sia
    positivo e poi chiamo super (FATTO)
4)  Aggiungo un metodo rescale a _KernelBase, funziona come diff cioè fa
    trasf. diverse per x e y, se sono diverse (definito con is not) cambia la
    classe dell'output a _CrossKernel (FATTO)
5)  Definisco nuove classi _Proc, _ProcKernel, _ProcTransf e _ProcDeriv
    analoghe di _Element, _Points e _Transf (NON FARE _ProcDeriv) (FATTO)
5a) Aggiungo un attributo proc di tipo _Proc a _Points (FATTO)
6)  Definisco _GP._crosskernel che calcola il kernel per due _Points
    ricorsivamente, con metodi analoghi a quelli che uso per implementare
    _makecovblock: _crosskernel_kernel_kernel, _crosskernel_transf_any,
    _crosskernel_deriv_any (NON FARE _deriv) (FATTO)
7)  _crosskernel_kernel_kernel semplicemente restituisce il kernel se i due
    _ProcKernel coincidono, altrimenti un sigleton _ZeroCov (FATTO)
8)  _crosskernel_deriv_any calcola il kernel e poi chiama diff, _ZeroCov viene
    passato (SALTARE)
9)  _crosskernel_transf_any cicla sugli elementi calcolando i kernel e
    combinandoli con _binary, gli _ZeroCov vengono ignorati, gli scalari nulli
    non producono _ZeroCov altrimenti si incasinano le derivate, alla fine se è
    uno _ZeroCov viene passato (FATTO)
10) Aggiungo GP.addproc, addproctransf e addprocderiv, i _Proc vengono infilati
    in un dizionario apposito, possono avere gli stessi nomi degli x (NON FARE
    addprocderiv) (FATTO)
11) Aggiungo proc= ad addx, deriv= continua ad avere la sua implementazione a
    parte in _Points e _makecovblock_points (FATTO)
12) Controllo che le x abbiano lo stesso tipo, per adesso (FATTO)
13) Riimplemento la somma di componenti con questa roba (FATTO)

Problema serio: come faccio a specificare cosa derivare se ogni processo ha la
sua x? Cioè: le x sono le stesse, oppure ogni processo ha la sua x?

=> Soluzione rapida e parziale: posso derivare solo singoli processi e non le
trasformazioni. In questo modo continuo ad assumere che le x siano le stesse.

=> Soluzione più completa: quando dico la derivata, devo poter specificare
rispetto a quale processo, o se derivare rispetto a tutte le x, in generale
devo poter specificare vari criteri. Forse è troppo complicato e l'utente non
avrebbe chiaro cosa sta facendo.

L'interfaccia elegante sarebbe questa: quando definisco una trasformazione dei
processi, devo dire se le x sono le stesse o sono diverse. In formule, devo
distinguere

    h(x) = f(x) + g(x)

da

    h(x, y) = f(x) + g(y)

Un modo sensato sarebbe proprio leggersi una stringa del genere. Una versione
più accroccata intermedia sarebbe un'opzione di addproctransf che mi
permette di dire se le x sono tutte le stesse o sono diverse, per fare
combinazioni più strane chiamare più volte addproctransf. Se sono le stesse poi
addx controlla che metti dentro x con lo stesso dtype. Poi però devo inventarmi
un modo di propagare queste informazioni fino a _KernelBase.diff.

--------

What calculation should I do to bypass the limitation that new gvars can not be
correlated with old gvars?

Old = x, new = y

Desired covariance matrix:

    V = [ V_xx   V_xy]
        [ V_yx   V_yy]

z = auxiliary variables, as many as y, independent of x

I have to write y as a combination of z and x:

    y = Az + Bx

Cov[x, y] = Cov[x, Bx] = V_xx B^T = V_xy   ==>  B^T = V_xx^-1 V_xy
Cov[y, y] = Cov[Az, Az] + Cov[Bx, Bx] = A V_zz A^T + B V_xx B_T = V_yy  ==>
    ==>  A V_zz A^T = V_yy - V_yx V_xx^-1 V_xy = V/V_xx

I can choose A = I and then V_zz = V/V_xx. I have to decompose V_xx.

gac, I rediscovered conditioning

2022-04-22
==========

The nonlinear fit isn't working. With enough quadratic data the fitted data
(even the linear one) is far from the actual data, but even with one single
nonlinear point the fit starts to fail in some places.

Is this a bug or an excess of nonlinearity?

In the latter case, I expect that reducing the errors on the quadratic data
should effectively linearize it. Then, provided I start the fit already in the
correct neighborhood, it should work.

=> tried: it's even worse!

I'm combining 9 latent points into one single quadratic datapoint. Maybe even
if the error on the datapoint is small, the freedom left to the latent points
makes their nonlinear bias large.

=> Using only one point as quadratic input, still does not work at all. This
suggests it is a trivial bug of some sort.

Bug found: I was passing the prior through the nonlinear function and then
sampling, instead of sampling and then passing the sample through the nonlinear
function. Since the prior is wide, it was being affected a lot by the
nonlinearity.

--------

Next thing to do: pdf6.py, fit hyperparameters and quadratic data together. I
foresee that it will be slow because I can't take derivatives through lsqfit
with autograd. I should read how lsqfit.empbayes_fit is implemented.

Actually, I could first add errors to M and M2 instead of fitting kernel
hyperparameters.

2022-04-23
==========

I must also devise a convenient extension of predfromfit for the case where I
condition on some stuff with predfromdata and on some other with lsqfit. For
the time being, since the sum rule constraints are exact, I think that
conditioning twice is valid (but I should check to be sure, maybe add a test
case).

--------

The fit with the errors on M is not working. Two alternative explanations:

  * It can't work because there is too much uncertainty. But I would expect the
    fit to return an appropriately large sdev in this case, which is not
    happening.

  * The sheer amount of the M parameters (2700) makes the minimization "happy"
    too early because M is easy to fit.

To check this, I can hold M fixed to its mean (different from the true value
used to generate the data) without fitting it. If the result I get is still
nonsense, then probably it's a statistical and not a numerical problem.

=> The fit makes sense. This indicates that it's a numerical problem. Possible
quick hack: very low termination tolerance.

=> The quick fix didn't work, but starting from the true values gives a good
result, which confirms it is a numerical problem.

Anyway now I found out that in the actual problem M is actually parametrized by
just a few numbers despite being a large matrix, so whatever.

To do next time: pdf7.py, i.e., fit hyperparameters on top of nonlinear fit
(will be damn slow!)

Following steps:
  * pdf8.py scale the number of datapoints/parameters to a realistic one
  * pdf9.py change the fake data, the grids and the kernel to realistic ones

--------

I did a quick benchmark of pdf6. Most of the time is spent in evaluating the
jacobian of the model function with gvar. Time per call:

    fcn(floats): 200 us
    fcn(gvars): 230 ms
    forward jacobian with jax: 30 ms
    backward jacobian with jax: 3 ms
    jacobian with autograd: seconds

I don't expect to find a way to easily optimize the gvar calculation. 7 times
slower than the compiled jax version seems already quite good considering it's
naive sparse operations.

The other half of the time spent by the fit is computing as svd in trf, I
assume it's the svd of the jacobian. So even optimizing the derivative
calculation would give at most a factor of two. Maybe there are other faster
methods in least_squares for a large dense jacobian? => maybe lm? look also
into GSL

If I did things manually, maybe I could take into account that the jacobian is
split in two blocks, one the identity and one dense, and then multiplied by
something. If this something is a diagonalization instead of cholesky then
maybe I can obtain the svd of the jacobian.

2022-04-25
==========

If I set p0 in fitargs of lsqfit.empbayes_fit, will it still adapt p0
automatically? => Yes, it works.

Idea: since I know the components of M and M2, could I first transform the grid
of points for each component? => Maybe it is not doable for the quadratic data,
and moreover in the real problem there are more datapoints than grid points.

--------

I talked to Alessandro about the details of the complete PDF fit. Summary:

The grid of x points is
[
    1.9999999999999954e-07, # start logspace
    3.034304765867952e-07,
    4.6035014748963906e-07,
    6.984208530700364e-07,
    1.0596094959101024e-06,
    1.607585498470808e-06,
    2.438943292891682e-06,
    3.7002272069854957e-06,
    5.613757716930151e-06,
    8.516806677573355e-06,
    1.292101569074731e-05,
    1.9602505002391748e-05,
    2.97384953722449e-05,
    4.511438394964044e-05,
    6.843744918967896e-05,
    0.00010381172986576898,
    0.00015745605600841445,
    0.00023878782918561914,
    0.00036205449638139736,
    0.0005487795323670796,
    0.0008314068836488144,
    0.0012586797144272762,
    0.0019034634022867384,
    0.0028738675812817515,
    0.004328500638820811,
    0.006496206194633799,
    0.009699159574043398,
    0.014375068581090129,
    0.02108918668378717,
    0.030521584007828916,
    0.04341491741702269,
    0.060480028754447364,
    0.08228122126204893,
    0.10914375746330703, # end logspace, start linspace
    0.14112080644440345,
    0.17802566042569432,
    0.2195041265003886,
    0.2651137041582823,
    0.31438740076927585,
    0.3668753186482242,
    0.4221667753589648,
    0.4798989029610255,
    0.5397572337880445,
    0.601472197967335,
    0.6648139482473823,
    0.7295868442414312,
    0.7956242522922756,
    0.8627839323906108,
    0.9309440808717544,
    1, # end linspace
]

Number of datapoints: 4535 total, of which 3089 linear. The relative errors are
tipically 2-10 %, with some exceptions down to 0.1 % and up to 50 %.

It is convenient to describe the PDFs with a change of basis, see:

https://eko.readthedocs.io/en/latest/theory/FlavorSpace.html#qcd-evolution-basis

First, for each quark, define
    
    q+ = q + qbar,
    q- = q - qbar.

The gluon g is left alone. Then

    Sigma = sum_q q+
    V     = sum_q q-
    
    V3  = u- - d-
    V8  = u- + d- - 2s-
    V15 = u- + d- + s- - 3c-
    
    T3  = u+ - d+
    T8  = u+ + d+ - 2s+
    T15 = u+ + d+ + s+ - 3c+

So the Ts are like the Vs but with q+ instead of q-. The constraints are:

    For all f: f(1) = 0

    Total momentum: int dx x (Sigma(x) + g(x)) = 1

    The integrals of the Vs are
        V   3
        V3  1
        V8  3
        V15 3
    
    Sigma and g diverge power-like for x -> 0, the Ts and Vs don't
    
    For g and Sigma: x^2 f(x) -> 0 for x -> 0
    
    For Vs and Ts: x f(x) -> 0 for x -> 0

2022-04-28
==========

How should I parameterize the PDFs in the fit? I should pick the basis which
makes the kernel easier to write. The divergent behaviour is specified in the
Sigma-T-V basis, so I should use it. I could also do like they are doing with
neural networks and multiply a base process by x^-a (1-x)^b, but only Sigma
and g, while the others would get (1-x)^b. Should I let each process have its
own b? Dunno.

--------

I'm having a problem with pdf8, the fit fails due to zeros in the covariance
matrix. The zeros must be numerical. I think it is due to the absurdly high
priors and data I'm putting in due to the divergence of Sigma(x) and g(x).
I should define the transformation w.r.t. x*Sigma and x*g probably, and only
use x from 1e-5 or 1e-4 onward for data.

2022-04-29
==========

In the end the main problem with pdf8.py was the zero error on f(1) due to the
constraint.

To do next time: implement Kernel.xtransf and use a linear interpolation of the
grid with ExpQuad instead of Gibbs(scalefun=x). But before that check the order
thing of the Gibbs kernel.

Implement a decomp parameter to lgp.raniter to see if eigcut- solves the
rough samples problem of gvar.sample (the corresponding functionality in
gvar.sample is broken).

I want to let xSigma and xg go power-like for x->0 with an exponent between
-1 and 1. The most convenient way is defining them as some process rescaled by
x^a, but this messes up the integrals. How do I fix this? What I need in
general is a kernel for a process whose derivative goes like x^a.

--------

Tentative:

    f ~ GP

And the correlation length of f(x) is x (uniform in log(x)), so

    f'(x) ~ 1/x

Where now with ~ we mean "goes like" instead of "is distributed as". So
defining the transformed process

    g(x) = x^(a+1)/(a+2) f(x)

We have
    
    g'(x) = x^a (a+1)/(a+2) f(x) + x^(a+1)/(a+2) f'(x) ~
          ~ (a+1 + 1)/(a+2) (x^a 1 + x^(a+1)/x) =
          = x^a

Then we take g'(x) to be x Sigma(x) or x g(x).

--------

More generally, let w(x) be the change of variable that makes the correlation
length constant. Then the scale function w.r.t. x is

    s(x) = 1/|w'(x)|.

So if f(x) ~ 1, we have f'(x) ~ 1/s(x). Let g(x) be the primitive of x Sigma(x)
or x g(x), which we define as

    g(x) = u(x) f(x)

for a generic factor u(x) which we want to determine such that

    g'(x) ~ x^a.

We have

    g'(x) = u'(x) f(x) + u(x) f'(x) ~
          ~ u'(x) + u(x)/s(x) =             (assuming w'(x) > 0)
          = u'(x) + w'(x) u(x) = x^a
    
This differential equation has the solution

    u(x) = exp(-w(x)) (int dx x^a exp(w(x)) + constant)

To check that we can reobtain the previous result, let
    
    s(x) = x  =>  w(x) = log(x),

then
    
    u(x) = 1/x (int dx x^a x + c) =
         = 1/x (x^(a+2) / (a+2) + c) =
         = x^(a+1)/(a+2) + c/x

Requiring a finite u(0) to have an integrable g'(x), we have

    g(x) = x^(a+1)/(a+2) f(x)   q.e.d.

2022-05-01
==========

pdf8.py works fitting exponents for xSigma and xg. To do next time: increase
the allowed range of exponents (use a uniform prior), use a realistic number
of datapoints (see above), save the history of hyperparameters and marginal
likelihood in the minimization to estimate the hessian with a fit in the end.
Maybe first implement the hessian thing with a simpler fit.

Once I have this I'd say I can stop for a while on the PDFs and maybe port
lsqfitgp to JAX and do other improvements to be able to optimize the fit.

2022-05-02
==========

New piece of information: there aren't only linear and quadratic data, there's
also ratios and other stuff. How many of them?

I won't do the hessian because lsqfit.empbayes_fit does not let me pass
parameters to scipy.minimize so I can't collect the target outputs with
options=dict(return_all=True).

Fit with many datapoints started at 11:15. => First iteration ended at 11:45.
Thus the complete fit would require about 60 hours. The bottleneck appears to
be the jacobian calculation with gvar, in particular the quadratic data tensor
contraction which sums along a 50x50x9 = 22500 axis.

How do I solve this problem?
    1) Verify that this is the bottleneck.
    2) Wrap fcn using gvar_function and compute the jacobian with jax.

2022-05-05
==========

Commands to get the version number:

grep __version__ lsqfitgp/__init__.py | sed -e "s/__version__ = //" -e "s/'//" -e "s/'//"
python -c 'import lsqfitgp;print(lsqfitgp.__version__)'

2022-05-31
==========

Next thing to do: test jax jit.

2022-06-01
==========

About Fisher scoring: the expected value of the log marginal likelihood is:

    E[-1/2 log(det(V)) -nlog(2π) -1/2 (x-mu)^TV^-1(x-mu)] =
    -1/2 log(det(V)) -nlog(2π) -1/2 V^-1_ijE[(x-mu)_i(x-mu)_j] =
    -1/2 log(det(V)) -nlog(2π) -1/2 V^-1_ij V_ij =
    -1/2 log(det(V)) -nlog(2π) -1/2 tr(V^-1 V) =
    -1/2 log(det(V)) -nlog(2π) -1/2 tr(I) =
    -1/2 log(det(V)) -nlog(2π) -1/2 n

So in practice the thing amounts to computing the hessian of -1/2 log(det(V)),
but the gradient of the full likelihood, and put these into a second order
minimizer. Algorithms that may be appropriate in optimize.minimize: dogleg,
trust-exact.

**** WRONG, SEE 2022-06-06 ****

Other hack-hessian option: external product of the residuals.

    V = LL^T
    V^-1 = L^T^-1 L^-1
    r = L^-1 (x - mu)

    D (-1/2 r^T r) = -r^T Dr
    D^2 (-1/2 r^T r) = -Dr^T Dr -r^T D^2r
    
So I would just keep -J^T J, which is positive definite, instead of the full
hessian. To do this with autodiff I would need to let jax go through the matrix
decomposition. Tentative interface: add an option direct_autodiff to the
__init__ wrapper in DecompAutoDiff.

2022-06-02
==========

The second derivative of logdet does not work. Possibilities:

1) I'm doing the test wrong => unlikely because it fails both with analytical
   solution and finite differences
2) There is a sneaky error in the jvp of solve
3) I have not properly understood how the tracing-stopping works and it doesn't
   with multiple layers to unpack

First things to do:

1) Check finite differences against the partially handwritten solution
2) Implement direct_autodiff and see if it works.

=> with direct_autodiff, the derivatives work, both compared to finite
differences and to the handwritten solution. This leaves (2) and (3), and I
guess (3) is the case since the jvp of solve is tested on its own.

I have no idea on what's going wrong, and I don't feel like diving into jax
tracing, so the next thing to do is implementing the second derivative test
for solve and quad and see if they fail too. Since I expect debugging this
will require some time, after checking that direct_autodiff works in those
cases I should switch to improving pdf*.py and using hessians in empbayes_fit.

2022-06-04
==========

Some benchmarks of toeplitz chol matvec:

In [344]: def trychol(n, func):
     ...:     t = jnp.exp(-1/2 * jnp.linspace(0, 5, n) ** 2)
     ...:     t = t.at[0].add(lgp._toeplitz.eigv_bound(t) * n * finfo(float).eps)
     ...:     b = np.random.randn(n)
     ...:     getattr(lgp._toeplitz, func)(t, b)
     ...: 

In [345]: %timeit trychol(10000, 'chol_solve')
440 ms ± 7.56 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

In [346]: %timeit trychol(10000, 'cholesky')
227 ms ± 1.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

In [347]: %timeit trychol(10000, 'cholesky_jit')
159 ms ± 164 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

In [348]: %timeit trychol(10, 'cholesky_jit')
692 µs ± 214 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)

In [349]: %timeit trychol(10, 'cholesky')
68 ms ± 146 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

In [350]: %timeit trychol(10, 'chol_solve')
910 µs ± 4.26 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)

2022-06-05
==========

The forward jacobian of quad and solve seems more accurate with custom
derivatives than with direct ones, but maybe only for some matrices.

--------

The forward second derivatives work. I can move on to implementing
hyperparameters algorithms.

2022-06-06
==========

Interesting piece of code:

File ~/Documents/Scuola/lsqfitgp/Repository/lsqfitgp/pythonvenv/lib/python3.10/site-packages/scipy/optimize/_trustregion_exact.py:245, in IterativeSubproblem.__init__(self, x, fun, jac, hess, hessp, k_easy, k_hard)
    242 self.dimension = len(self.hess)
    243 self.hess_gershgorin_lb,\
    244     self.hess_gershgorin_ub = gershgorin_bounds(self.hess)
--> 245 self.hess_inf = norm(self.hess, np.Inf)
    246 self.hess_fro = norm(self.hess, 'fro')
    248 # A constant such that for vectors smaler than that
    249 # backward substituition is not reliable. It was stabilished
    250 # based on Golub, G. H., Van Loan, C. F. (2013).
    251 # "Matrix computations". Forth Edition. JHU press., p.165.

--------

I noticed that I made a mistake in computing the Fisher matrix, I took the
expected value *before* differentiating. The correct expression is

    E[∂∂l] = 1/2 tr(K^-1 ∂K K^-1 ∂K)

How do I implement it in jax? => Use lax.stop_gradient(K_dot) in solve_vjp

2022-06-07
==========

Jax stuff
---------

From jax #9973:
jax.config.update('jax_default_matmul_precision', jax.lax.Precision.HIGHEST)
The default is not highest.

jax.lax.cond is the jittable equivalent of if
see https://github.com/google/jax/discussions/10306 about evaluating or not both
branches of a conditional.

https://github.com/google/jax/issues/5507#issuecomment-767931338
"JAX handles NumPy constants with zero strides in a smart way."

To pass a marked-as-zero tangent to jax.jvp, use the special dtype
jax.dtypes.float0. May work also for integer inputs not to be differentiated.

jax.tree_util.tree_map allows mapping a function through a set of trees. I
could use it with StructuredArray in place of my current custom traverser.
To handle array fields, transpose everything before and after the reduction,
to have automatic broadcasting (does tree_map support broadcasting?)

see https://github.com/josipd/jax/blob/master/jax/experimental/jambax.py to
convert numba functions to jax primitives

https://github.com/google/jax/discussions/10004 about using Fisher matrices

2022-06-08
==========

Next things to do:

1) Write pdf9.py stripping pdf8.py of all non-linear things and using
empbayes_fit for the hyperparameters (DONE)

2) Scale pdf9.py to a large dataset (SOMEWHAT DONE)

3) Write pdf10.py like pdf9.py but using real PDF data

2022-06-10
==========

I did the calculation of the Fisher information when the residuals (i.e., the
mean vector) depend on the hyperparameters. The result is that I have to add a
term ∂r^T K^-1 ∂r, i.e., quad(rjac).

2022-06-13
==========

pdf9.py with 500 datapoints runs smoothly, but does not give something
compatible with the truth. Even judging by eye, the fitted correlation length
is too short, the valence shell looks shrinked. The minimum seems robust to
change of fitting method. Ideas about this:

1) Instrument properly empbayes_fit
2) Maybe the link matrix with iid normal entries is particularly pathological
   for some reason. Like, the effect tend to cancel out statistically as I
   increase the size of the matrix, so it is numerically inaccurate doing
   the inference.

Another fit that does not work is the periodic one in test_fit. Maybe exploring
that will clear my understanding of using Laplace with a Gaussian process.

2022-06-17
==========

Thing that I could cite: Snelson et al., "Warped Gaussian Processes". It's a
bit trivial but people expect you to cite everything. Everything was redone
for Gaussian processes.

2022-06-20
==========

sinc(x) = sin(x)/x

sinc'(x) = cos(x)/x - sin(x)/x^2
         = (cos(x)x - sin(x))/x^2 =
         = cos(x) (x - tan(x)) / x^2

I have to compute a - b with a - b ~ 0:

a - b = b (a/b - 1)
a - b = log exp(a - b) =
      = log(exp(a/b - 1)^b) =
      = b log exp(a/b - 1)

Does not solve the problem, a/b is already imprecise.

2022-06-29
==========

I learned that the FK tables (the matrices PDF -> linear data) are 90 %
diagonal and the entries are mostly positive.

2022-07-04
==========

AR kernel
---------

- generate the companion matrix of the characteristic polynomial
- diagonalize it to get the roots
- evaluate the polynomial but for each root in turn to get the coefficients
  (not really sure about this part)
- evaluate the combination of powers

Alternative: parameterize with roots and amplitudes, then add class method
to convert parameters to coefficients

Alternative: rotate the yule-walker equations

Question: is the exponential mixture parametrization valid with roots with
multiplicity? For high enough distance I'm sure it is, there may be problems
near zero => Guy on wikipedia
https://en.wikipedia.org/wiki/Talk:Autoregressive_model#Multiplicities_in_characteristic_polynomial
says the general form is a combination of t^r y^-t, with y a root and r
an integer going up to y's multiplicity - 1. (ask Luca for a book about this)

Tentative interface: four parametrizations:
1) w : (p,)
      autoregressive weights
2) y : (p,)
      roots > 1
   a : (p,)
      amplitudes >= 0 for the terms y^-t
3) y : (n,)
      roots
   a : (n, m)
      amplitudes for the terms t^0 y^-t, ..., t^m-1 y^-t (p = n * m)
4) c : (p + 1,)
      first terms of the autocovariance function

=> Problem: there are nontrivial positivity constraints on the coefficients
in the case with multiplicity

=> Actually, even though (2) is always pos def, the amplitudes are uniquely
determined by the roots through the char poly. So I should take the roots
and amplitudes, generate the polynomial, compute the acf, and solve a linear
system for the amplitudes. I can't do this right away from the coefficients
because I need the multiplicity of the roots; my intuition suggests that
it is numerically more stable to evolve the acf instead of using the
analytical expression. The speed should be analogous in practical cases.

Problem with evolving the acf: the maximum lag, and thus the length of the
array, is determined by the data. This is not permitted under the jit =>
add a parameter maxlag, fill with nan for lags not computed.

New tentative parametrizations:
1) w : (p,)
      autoregressive weights
   maxlag : int
      maximum precomputed lag, outside will be filled with nan (must be
      specified even without jit---otherwise the eventual failure under jit is
      confusing)
2) c : (p + 1,)
      first terms of the autocovariance function
   maxlag : int
      maximum precomputed lag, outside will be filled with nan
3) y : (p,) complex
      roots > 1, multiplicity inferred with strict comparison (need a bit
      of work to group roots under the jit due to fixed size, but can be
      done)
4) f : (p,) complex
      y = exp(f)
5) y : (n,) complex
      Roots
   m : (n,) int
      Multiplicities
6) f, m like (4)-(5)

Problem: the complex roots must be conjugate-paired. If I take the real part,
implicitly it's like I'm duplicating any complex root which isn't paired.
However this also means that the actual AR order is higher than the number of
roots provided.

Actually, must the roots necessarily come in pairs? => Yes, and this holds
also with any multiplicity
https://en.wikipedia.org/wiki/Complex_conjugate_root_theorem

This poses continuity problems in the parametrization. I guess that as the
imaginary part of a complex root goes to zero, there will be numerical accuracy
issues since the solution must tend to a multi-pole filter. What should happen
is that the imaginary part of the amplitude diverges to compensate the period
of the sine going to infinity, this gives a linear slope in the limit.

2022-07-05
==========

New AR parametrization:

 1) w : (p,)
        autoregressive weights
    maxlag : int
        maximum precomputed lag, outside will be filled with nan (must be
        specified even without jit---otherwise the eventual failure under jit is
        confusing)
 2) c : (p + 1,)
        first terms of the autocovariance function
    maxlag
 3) lnr : (n,) real
        logarithm of real roots (multiplicity 1), > 0
    lnc : (m,) complex
        logarithm of complex roots (multiplicity 2), real part > 0
    mr : (n,) int
    mc : (m,) int
        multiplicities of roots, if None assumed 1, no check for equality

Problem: how do I implement the multiplicity-dependent terms in a
jit-compatible way? The size of the system changes based on the data. => Put
the roots in an array with multiplicity indicated by repetition. After sorting,
scan keeps track of repetition (a fixed size state is sufficient for this) and
generates the appropriate powers of t as needed along the way.

Problem 2: zero imaginary part in complex roots. I think this can be solved
with a pseudoinverse without further problems.

Problem 3: sign of real roots. I can take the sign of `lnr` as the sign and
its absolute value as logarithm. (The reason for using logarithms is that
roots near 1 would be numerically inaccurate. Also, the logarithm is
interpretable as the inverse of the correlation length.)

New AR parametrization:

 1) w : (p,)
        autoregressive weights
    maxlag : int
        maximum precomputed lag, outside will be filled with nan (must be
        specified even without jit---otherwise the eventual failure under jit
        is confusing)

    Implementation: compute the acf with inverse YW and evolve it up to
    maxlag. Do not check that the w are valid.

 2) c : (p + 1,)
        first terms of the autocovariance function
    maxlag

    Implementation: compute the coefficients with YW and evolve the acf like
    above. Do not check c is valid.

 3) slnr : (n,) real
        logarithm of real roots (multiplicity = 1 x exact repetitions), the
        sign is the sign of the root and the absolute value is the logarithm
    lnc : (m,) complex
        logarithm of complex roots (multiplicity = 2 x exact repetitions), real
        part > 0
    
    The multiplicity is assessed with strict equality. Very close yet distinct
    roots are treated as separate and lead to numerical instability. The order
    of the AR is n + 2 * m. Complex roots which are actually real or almost
    real lower the actual order but are not numerically problematic.
    
    Implementation: sort, then loop keeping track of repetitions generating the
    coefficients of the linear system for the amplitudes of the acf. Do a
    pseudoinverse to accomodate real or almost real complex roots. Generate the
    polynomial from the roots and compute the acf (the RHS) with inverse YW.
    Finally evaluate the analytical expression of the acf.

Problem: it would be very useful for the user to have a check on the validity
of w and c.

For w I can generate the polynomial, compute the roots and check they are above
1. In practice I expect the roots to be quite close to 1 because otherwise the
correlation length is less than 1 time step. I estimate this check to be
numerically accurate only up to ~tens of coefficients. Since I obtain the roots
as eigenvalues, I could use as tolerance the same I use for positivity checks.
I fear additional numerical instability I'm unaware of.

For c I can generate w from YW and check w. Since YW is invertible, obtaining a
valid w means that c was valid.

The most annoying task in all this is porting polyfromroots to jax. I bet that
a naive implementation would be too inefficient and inaccurate.

Also add the parameter conversion functions as static methods.

--------

Problem: even though the roots as logarithms preserve the numerical accuracy
near 1, I have to determine phi and then gamma from the characteristic
polynomial. Phi can not come out accurate in general because I need to pass
from the actual roots, also think about AR(1) where phi = 1/root. However this
affects only the amplitudes, not the decay. Moreover, with slow decay, if I 
use as RHS the first p terms, they will all be almost equal, so the system for
the amplitudes is very degenerate. I should pick the lags based on the
correlation lengths. => Or is there some way to directly obtain the phi for
the decimated process? That way I can move the "effective" roots away from 1.
=> I think that multiplying all the logarithms by an integer constant would
be sufficient.

2022-07-06
==========

Complete reasoning: using the analytical expression of the autocovariance,
I can obtain the one of the decimated process simply by raising all roots to
the decimation. The amplitudes do not change.

Question: is this an autoregressive process? Because the amplitudes are
determined by the roots. => Yes because I can expand recursively to obtain the
lagged y. However, the order will in general be infinite.

Thinking in Fourier space, the relationship between the amplitudes w/ and w/o
dilated roots is not trivial. Maybe in the end the first idea, determining
analytically the amplitudes, was the right one. The denominator is a product of
differences of the roots, which can be computed accurately with complex
sum(logaddexp).

Actually, logaddexp won't be numerically accurate on a difference. In general
it is impossible to recover accuracy because the roots can be arbitrarily close
in relative error (even in logarithm parametrization) and so their difference
is inherently imprecise. This means that log(roots) is not the best
parametrization for accuracy, I need to use diff(sort(log(roots)), prepend=0).

Or is it more convenient log (r_i+1 - r_i)/r_i, with r_0 = 1? This would be in
(-oo, oo) and be about the same value for evenly spaced lengthscales (need to
check this). Then if I compute the logarithm of the amplitudes with proper
combinations of logsumexp, I can write the autocovariance as a logsumexp itself,
which maybe allows to have arbitrarily close roots.

This parametrization implies that I have to treat all roots as complex. Might
as well take the real part only in the end. Wait, do I sort by real part or by
modulus? Maybe the parametrization should handle separately real parts and
phases => The phase is bad as parameter, must use numbers in (-oo, oo).

2022-07-07
==========

A random walk on the phase as prior does not make much sense, so maybe the
phases should be kept separate and not parametrized incrementally?

Tentative: use

    z_i = log (|r_i+1| - |r_i|)/|r_i|,  r_0 = 1,
    
and x_i, y_i such that arg(r_i) = atan2(y_i, x_i).

The quantities I need to obtain from xyz are log(r_i) and r_i - r_j for any
pair i, j. => I can already see it does not work: the differences in the
phases are not numerically accurate.

If I use z_i = log(r_i+1/r_i), a uniform i.i.d. prior on arg(z_i) implies a
uniform i.i.d. prior on arg(r_i).

    r_i = r_i/r_i-1 r_i-1/r_i-2 ... r_1/r_0
    
    log(r_i) = z_i-1 + z_i-2 + ... + z_0 = sum_k=0^i-1 z_k
    log(r_0) = 0
    
    d_i = r_i+1 - r_i = exp(sum_k=0^i z_k) - exp(sum_k=0^i-1 z_k) =
        = exp(sum_k=0^i-1 z_k) (exp(z_i) - 1) =
        = r_i expm1(z_i)

Can I break the accuracy of this? Take

    r_1 ~ 1, r_2 ~ -1, r_3 ~ 1.
    
So we have that r_1 is very close to r_3, but r2 is distant from both.

    r_2/r_1 ~ -1  -->  z_1 ~ 0 + i pi
    r_3/r_2 ~ -1  -->  z_2 ~ 0 + i pi
    
    d_1 = r_1 (e^z_1 - 1) ~ 1 (-1 -1) = -2
    d_2 = r_2 (e^z_2 - 1) ~ -1 (-1 -1) = 2
    
    r_3 - r_1 = d_1 + d_2 ~ 0  -->  cancellation!

If the parametrization is ordered w.r.t. the modulus, then it statisfies r_i >
1 but distant roots can be closer than neighbor roots because the phases
rotates. With lexicographical order there is the same problem. Indeed, I can
have paths on the complex plane such that roots get arbitrarily close again
after an arbitrarily long time (as a spiral). Indeed the fundamental problem
is that it is not possible to sort complex numbers topologically.

What is the least evil then? Depending on how the user wants to put the roots
around, different parametrizations would be appropriate. The base
parametrization should be convenient for generic use, and not impede any
sensible strategy from being adopted.

First I can send the donut into the open circle with the transformation 1/r.
Then maybe I can send the border of the circle to infinity somehow, to have
a parametrization in full R^2. What is the appropriate way here to send a
radius in [0, 1) to one in [0, infty)? It should probably be something that
plays nicely with complex numbers.

However I think it can't be done with an olomorphic function because the
radial stretching doesn't preserve angles.

Tentative:

    |z| = -log(1 - 1/|r|)
    arg z = -arg r

For |r| -> oo, |z| ~ 1/|r| -> 0

For |r| -> 1+, |z| = -log (|r| - 1)/|r| -> -log(|r| - 1) -> inf

If r = 1 + e, e << 1, the lengthscale is 1/log r = 1/log(1 + e) ~ 1/e. The
logarithm of the lengthscale is ~ log(1/e) = -log e = -log(r - 1).

Thus |z|, if somewhat large, is interpretable as the logarithm of the
correlation length.

For r -> oo, scale is 1/log r while z is 1/r. So scale is 1/log(1/z), z is

    s = 1/log r
    log r = 1/s
    r = exp(1/s)
    z = 1/r = exp(-1/s)

Thus for small z, infinitesimal changes bring the correlation length away from
zero, which makes sense since below 1 at some point it's all white noise the
same.

For |r| -> oo, z behaves like (complex) 1/r, since

    |z| ~ 1/|r|, and
    arg z = -arg r.

This means that locally in 0 the circle is sent to itself, this means that the
derivatives can be defined. The stretch becomes "less olomorphic" moving
towards the border.

Now about the differences. Since there is no general compact strategy to
represent accurate differences, I need the user to pass (optionally) the full
matrix of pairwise differences. They have to be in z space to have domain R^2.

    1 - 1/|r| = exp(-|z|)
    1/|r| = 1 - exp(-|z|)
    |r| = 1/(1 - exp(-|z|)) = -1/expm1(-|z|)
    arg r = -arg z
    r = |r|exp(i arg r)
    
Ho d = z - z' (oppure su modulo e fase) e voglio calcolare r - r' usando d.
    
(Usare |z|^2 = -log(1 - 1/|r|^2) mi aiuterebbe? Boh)

    r - r' = |r|exp(i arg r) - |r'|exp(i arg r') =
           = |r|exp(i arg r)(1 - |r'/r|exp(i(arg r' - arg r)))
    
    |r'/r| = (1 - exp(-|z|)) / (1 - exp(-|z'|)) =
           = (exp(|z|') - exp(-|z| + |z'|)) / (exp(|z'|) - 1) = ?
    
    1/|r'| - 1/|r| = (1 - exp(-|z|')) - (1 - exp(-|z|)) =
                   = exp(-|z|) - exp(-|z'|) =
                   = exp(-|z|) (1 - exp(|z| - |z'|)) =
                   = -exp(-|z|) expm1(|z| - |z'|)
    
    |r'/r| = |r'|(1/|r| - 1/|r'|) - 1 --> ?

Ok I have it:

    |r| - |r'| = 1/(1 - exp(-|z|)) - 1/(1 - exp(-|z'|)) =
               = (1 - exp(-|z'|) - 1 + exp(-|z|)) / ((1 - exp(-|z|))(1 - exp(-|z'|))) =
               = |r||r'|exp(-|z'|)(exp(|z'| - |z|) - 1) =
               = |r||r'|exp(-|z'|)expm1(|z'| - |z|)
    
    r - r' = |r|exp(i arg r) - |r'|exp(i arg r') =
           = |r|exp(i arg r) - |r|exp(i arg r') + |r|exp(i arg r') - |r'|exp(i arg r') =
           = |r|exp(i arg r) (1 - exp(i (arg r' - arg r))) + (|r| - |r'|)exp(i arg r') =
           = -r expm1(i (arg r' - arg r)) + (|r| - |r'|)exp(i arg r')

Mmm but I'm actually interested in 1/(r - r'). So
    
    x = 1/r
    |x| = 1 - exp(-|z|)
    arg x = arg z
    
    r - r' = 1/x - 1/x' =
           = (x' - x)/(xx')
    
    1/(r - r') = xx'/(x' - x)
    
    |x'| - |x| = exp(-|z|) - exp(-|z'|) =
               = exp(-|z'|) (exp(|z'| - |z|) - 1)
    
    x' - x = |x'|exp(i arg x') - |x|exp(i arg x) =
           = (|x'| - |x|)exp(i arg x) + |x'| (1 - exp(i (arg x - arg x'))) =
           = exp(-|z'|) (exp(|z'| - |z|) - 1) exp(i arg x) +
             + (1 - exp(-|z|')) (1 - exp(i (arg x - arg x')))
        
    xx' = |x||x'|exp(i (arg x + arg x'))

About the same, can't really write it in a better way. Now it remains to get
the differences in |z| and arg z from the one in z = a + ib.

    |z| - |z'| = √(a^2 + b^2) - √(a'^2 + b'^2) =
               = |z'|(|z/z'| - 1) =
               = |z'|(√((a^2 + b^2)/(a'^2 + b'^2)) - 1)

I have sqrtm1, so let's go inside

    A = a - a'
    B = b - b'

    |z|^2 = a^2 + b^2 =
          = (a' + A)^2 + (b' + B)^2 =
          = a'^2 + b'^2 + 2(a'A + b'B) + A^2 + B^2 =
          = |z'|^2  +  2(a'A + b'B) + A^2 + B^2

Then the phase

    tan arg z' = b'/a'
               = (b - B)/(a - A) =
               = b/(a - A) - B/(a - A) =
               = b/a 1/(1 - A/a) - B(a - A)
    
    (f = arg z - arg z')
    
               = tan (arg z - f) =
               = (tan arg z + tan f) / (1 - tan arg z tan f)

Etc., I guess the calculation can be expressed in terms of cosm1. => No, wait:
I can do it inverting the expression for z - z' in terms of |z| - |z'|, it
contains 1 - exp(i (arg z - arg z')), which is directly what I need.

    z' - z = (|z'| - |z|)exp(i arg z) + |z'| (1 - exp(i (arg z - arg z'))) -->
    
    --> 1 - exp(i (arg z - arg z')) =
        = (z' - z)/|z'| + (|z| - |z'|)/|z'| z/|z|
        
Maybe I could have a parametrization in terms of |z|, a, b where

    z = |z|(a + ib)/hypot(a, b)
    
to have uniform phase distribution but |z| away from zero. => But then |z|
would have to be parametrized with log|z|, log(log(scale))... not convenient.

2022-07-08
==========

The roots at infinity mean that the effective order of the AR is lower, so
shrinkage to z = 0 selects for short AR.

2022-07-14
==========

To parametrize arbitrary roots effectively, I have to find a way to map an
arbitrary invalid difference matrix to a valid one, or in general a
latent parametrization of a difference matrix.

2022-07-16
==========

release notes for 0.12:

--removed maternX2
--removed ratquad
--added cauchy
--added maxdim check
--support numpy functions on StructuredArray
--conversion StructuredArray -> array
--conversion StructuredArray -> unstructured
--Constant and White support arbitrary input (not just numerical)
--Matern now works with any derivative for any nu, extended to nu=0
--fix derivatives for GammaExp
--Gibbs works in nd
--fractional -> bifractional
--extend fractional to negative axis
--rename ppkernel -> wendland, accept real parameter
--extend fourier to arbitrarily large n
--hole effect kernel
--bessel kernel
--pink kernel
--color kernel
--sinc kernel
--stationary frac brownian
--causalexpquad
--decaying
--log
--circular
--MA
--AR
--BART

2022-07-17
==========

Are all the amplitudes of the AR correlation always positive? (With an
appropriate convention for phases)

The characteristic polynomial is P(x) = 1 - phi @ x^n, and all the roots have
|x| > 1. P(0) = 1 + 0j, but I don't know how to extend the positivity on the
complex plane. Maybe there is something about radii like with power series.

I dunno. I should expose a function to compute the amplitudes and try to get
them negative to see if it's a good idea.

=> It's not a good idea, obvious in hindsight.

2022-07-19
==========

Above I wrote that I can compute the AR amplitudes accurately if I start from
the differences between the roots. However, as the roots get close together,
the amplitudes diverge with opposite phases with a precise cancellation to make
the exponentials into a power law. This means that probably I also need the
next order differences, and so on.

But when the roots are close I expect the shape to be more or less fixed. It's
the damn power law I know exactly. So maybe I can change formula when
the roots are closer than something. The only thing that I still can't compute
is the final variance. I have the same problem in computing gamma, where with
very small roots I know it's practically constant, however the normalization
heavily depends on how close they are.

Since huge normalizations are not really interesting, I can solve this kind of
problems by always normalizing the variance to 1.

2022-07-20
==========

Can I write exp(x) - 1 - x as a hypergeometric function?

    exp(x) - 1 - x = 
    = sum_k=2^oo x^k / k! =
    = x^2 sum_k=0^oo x^k / (k + 2)! =
    = x^2 sum_k=0^oo k!/(k+2)! x^k / k! =
    
        k!/(k+2)! = 1·2···k / 1·2···k+2 = 1/2 (1)_k/(3)_k
    
    = x^2/2 sum_k=0^oo (1)_k/(3)_k x^k / k! =
    = x^2/2 1F1(1, 3, x)

However in scipy 1F1 is implented by summing the taylor series, and 1F1 is not
implemented in jax, so whatever.

--------

I think that the space of allowed phi is a simplex whose vertices are the
polynomials with roots either -1 or 1. To prove it, I need to prove that
    1) any convex cobination of valid phi is valid
    2) any valid phi can be expressed as a convex combination of the vertices
Regarding 1), see
https://en.wikipedia.org/wiki/Geometrical_properties_of_polynomial_roots
https://en.wikipedia.org/wiki/Rouché%27s_theorem

--------

Method to allow arbitrarily many large roots in np.poly:
    1) find the smallest in modulus non-zero root x_0
    2) rescale all roots such that |x_0| = 1
    3) invert roots, let zero roots be inf (check complex does not produce nan)
    4) use poly
    5) reverse
    6) rescale with cumprod(|x_0|)

--------

What is the meaning of the phi in the center of the simplex? It is zero at odd
lags. How does a truncated phi appear in baricentric?

2022-07-21
==========

Method to obtain the amplitudes of the AR covariance:

    1) The z transform (with sign +) of gamma is 1/P(z)
    2) Thus gamma_m = 1/m! d^m/dz^m 1/P(z)|_z=0
    3) Decompose 1/P(z) as sum_k c_k / (1 - lambda_k z)
    4) The c_k are given by Hamilton (1994, p. 34-35), or in general see
       https://en.wikipedia.org/wiki/Partial_fraction_decomposition
    5) Use that d^m/dz^m 1/(1 - az)^l = (m+l-1)!/(l-1)! a^m / (1 - az)^m+l =
                                      = (l)_m a^m / (1 - az)^m+l

When two roots are close, assuming no other roots are close, I just need the
difference to write a stable formula. When (inverse) roots are close to zero,
even if they are close together, I think the correct limit is zero.

--------

scipy.special.binom does not obey https://dlmf.nist.gov/1.2.E6. maybe open an
issue.

--------

I think that https://dlmf.nist.gov/25.11.E7 is wrong or incoherent with
https://dlmf.nist.gov/1.2.E6, according to which (n k) = 0 if n is an integer <
k. => Nope, it just means the last term and some of the higher terms of the
summation vanish, the first terms of the summation are nonzero because in (n k)
n < 0.

2022-07-23
==========

Even after computing in a numerically stable way cos(π/2(1-s)), the periodic
zeta becomes inaccurate close enough to even s. The problems just begin at
smaller scale (surely < 1e-4 compared to 1e-2 before). Since the inaccuracy
increases smoothly as x comes closer to half integer, which is the condition
under which the series for the hurwitz zeta has larger terms, it must be some
problem there. Since I obtain the pochhammer symbol and the factorial with
direct multiplication, I guess the most likely culprit is scipy's zeta. Maybe
it's inaccurate near negative odd integers but not exactly at negative odd
integers, possibly because it uses the reflection formula.

=> Nope, scipy's zeta seems accurate enough for small odd integer s (< 10 ULP).
On the other hand, it goes to >100 ULP for large negative s in general,
possibly because of the cosine in the reflection formula. Maybe open an issue.

So the culprit could be scipy's zeta pole? => nope, it's perfect:

    In [685]: special.zeta(1+1e-15)*(1+1e-15-1)
    Out[685]: 1.0000000000000004
    
    In [686]: special.zeta(1+2e-16)*(1+2e-16-1)
    Out[686]: 0.9999999999999998

Other problem: the inaccuracy appears even for s that should be dealt with by
the polylog series. What??

    In [675]: lgp._patch_jax.periodic_zeta_real(.5,12+1e-15).item()
    Out[675]: -0.9997576851438581

    In [676]: periodic_zeta_real(.5,12+1e-15).item()
    Out[676]: -0.9997577024369573

Went through it with pdb, the function correctly returns large s, and large s
and small s agree to 6 ulp:

    ipdb> p z_smalls.item()
    -0.9997576851438587
    ipdb> p z_larges.item()
    -0.9997576851438581

So maybe mpmath is wrong? I already knew it was probably using a different
formula for integer s in the polylog, so it could be possible.

My implementation:

    In [688]: for x in lgp._patch_jax.periodic_zeta_real(.5,12+10.**-arange(1,20)):
         ...:     print(x)
         ...: 
    -0.9997738459102947
    -0.9997593517493558
    -0.9997578523212023
    -0.999757701866772
    -0.9997576868162014
    -0.9997576853110928
    -0.9997576851605816
    -0.9997576851455304
    -0.9997576851440254
    -0.9997576851438748
    -0.9997576851438598
    -0.9997576851438583
    -0.9997576851438581
    -0.9997576851438581
    -0.9997576851438581
    -0.9997576851438581
    -0.9997576851438581
    -0.9997576851438581
    -0.9997576851438581

Mpmath:

    In [689]: for x in periodic_zeta_real(.5,12+10.**-arange(1,20)):
         ...:     print(x)
         ...: 
    -0.9997738459102948
    -0.9997593517493558
    -0.9997578523212024
    -0.9997577018667723
    -0.9997576868162014
    -0.9997576853110931
    -0.9997576851605819
    -0.9997576851455319
    -0.9997576851440595
    -0.999757685144186
    -0.9997576851451117
    -0.9997576851619757
    -0.999757685340211
    -0.9997576864396291
    -0.9997577024369573
    -0.9997576851438582
    -0.9997576851438582
    -0.9997576851438582
    -0.9997576851438582

Ok, seems mpmath sure enough. Can I fix it brutally by increasing the precision?

    In [692]: @np.vectorize
         ...: def periodic_zeta_real(x, s):
         ...:     with mpmath.workdps(32):
         ...:         arg = mpmath.exp(2j * mpmath.pi * x)
         ...:         return float(mpmath.polylog(s, arg).real)
         ...: 

    In [693]: for x in periodic_zeta_real(.5,12+10.**-arange(1,20)):
         ...:     print(x)
         ...: 
    -0.9997738459102948
    -0.9997593517493558
    -0.9997578523212024
    -0.9997577018667723
    -0.9997576868162014
    -0.999757685311093
    -0.9997576851605817
    -0.9997576851455305
    -0.9997576851440254
    -0.9997576851438749
    -0.9997576851438599
    -0.9997576851438583
    -0.9997576851438582
    -0.9997576851438582
    -0.9997576851438582
    -0.9997576851438582
    -0.9997576851438582
    -0.9997576851438582
    -0.9997576851438582

--------

The closest thing to a gamma formula for integer + epsilon is
https://dlmf.nist.gov/5.9.E8.

2022-07-24
==========

jnp.polyval does not respect integer types, while numpy.polyval does. Maybe
open an issue.

--------

_gamma_incr is in general accurate enough (< 10 ULP) apart from x = 1, e = -0.4:

ipdb> p np.stack([np.ceil(np.abs((g2-g1)/g1)/np.finfo(float).eps).astype(int).squeeze(),(100*e).astype(int)],1)
array([[  2, -50],
       [  2, -49],
       [  3, -48],
       [  3, -47],
       [  4, -46],
       [  2, -45],
       [  4, -44],
       [  4, -43],
       [  3, -42],
       [145, -41],
       [ 78, -40],
       [ 41, -39],
       [ 21, -38],
       [ 12, -37],
       [  7, -36],
       [  5, -35],
       [  5, -34],
       [  3, -32],
       [  2, -32],
       [  2, -31],
       [  2, -30],
       [  2, -29],
       [  2, -28],
       [  3, -27],
       [  3, -26],
       [  3, -25],
       [  3, -24],
       [  2, -23],
       [  2, -21],
       [  3, -21],
       [  2, -20],
       [  3, -19],
       [  2, -18],
       [  2, -17],
       [  4, -15],
       [  3, -14],
       [  2, -14],
       [  2, -13],
       [  3, -12],
       [  1, -10],
       [  1,  -9],
       [  4,  -8],
       [  3,  -8],
       [  3,  -7],
       [  4,  -6],
       [  2,  -4],
       [  3,  -3],
       [  4,  -2],
       [  3,  -2],
       [  2,  -1],
       [  0,   0],
       [  3,   1],
       [  3,   2],
       [  3,   3],
       [  3,   4],
       [  3,   5],
       [  2,   6],
       [  4,   7],
       [  4,   7],
       [  4,   8],
       [  4,   9],
       [  3,  10],
       [  3,  12],
       [  3,  13],
       [  3,  14],
       [  4,  15],
       [  4,  16],
       [  5,  17],
       [  3,  18],
       [  4,  19],
       [  4,  20],
       [  3,  20],
       [  3,  21],
       [  4,  23],
       [  5,  24],
       [  3,  25],
       [  4,  26],
       [  5,  27],
       [  4,  28],
       [  5,  29],
       [  5,  30],
       [  5,  31],
       [  4,  32],
       [  4,  33],
       [  5,  34],
       [  6,  35],
       [  5,  36],
       [  4,  37],
       [  4,  38],
       [  5,  39],
       [  5,  40],
       [  7,  41],
       [  7,  42],
       [  2,  43],
       [  6,  44],
       [  2,  45],
       [  6,  46],
       [  7,  47],
       [  7,  48],
       [  7,  49],
       [  5,  50]])

Who's the culprit? Trying values of x in 1...2 it appears that:
  - the problematic values of e more or less translate according to x
  - the problem starts to disappear at x=1.8
  - the inaccuracy is in general much worse (2000 ULP)

Whatever, I don't need it for the periodic zeta.

--------

Failing qdiff tests:

qzero-xother        600-2000 ULP    
qeven-xnearzero     10^14-18 ULP    (not real problem, was ~underflow)

qzero-xother-medrange: the only really bad guy is x=0.16, a=-0.02
qzero-xother-shortrange: 25 bad guys, all have x=0.16, various a
qzero-xother-tinyrange: 25 bad guys, all have x=0.16, various a

Since there are 25 a x 1 q, 25 is all the times x was 0.16. So 0.16 is a
particularly bad value that breaks the series. Must be some sort of error
resonance in the multiplications. Maybe I should compute all terms individually
instead of using cumprod?

=> Wait it's q=0, there's no series involved! (And there would not be in any
case a series in x so whatever I was confused.) The only operations involving
x are x ** q and expm1(-a log(x)).

Since q=0, x**q = 1. And it does not work for any value of a, so it probably
isn't expm1, it should be log(x).

Checking expm1(-a log(x)), jax against mpmath, with a=-0.02 and x=0.16, gives 0
ULP of error.

So, it's actually x=0.15999999999999998, but the single evaluation does not
change.

Nor if I use exactly the same input arrays of the test.

Look at the worst precision for qzero-xother-medrange:

    In [5]: x.squeeze()[i[-100:,0]]
    Out[5]: 
    array([0.1 , 0.14, 0.12, 0.24, 0.12, 0.12, 0.1 , 0.14, 0.14, 0.26, 0.24,
           0.26, 0.16, 0.2 , 0.1 , 0.14, 0.14, 0.1 , 0.14, 0.12, 0.24, 0.1 ,
           0.16, 0.2 , 0.2 , 0.12, 0.2 , 0.24, 0.14, 0.14, 0.2 , 0.2 , 0.18,
           0.22, 0.2 , 0.14, 0.18, 0.1 , 0.24, 0.2 , 0.12, 0.12, 0.12, 0.12,
           0.2 , 0.24, 0.24, 0.12, 0.2 , 0.18, 0.2 , 0.22, 0.12, 0.14, 0.18,
           0.18, 0.2 , 0.14, 0.14, 0.2 , 0.18, 0.16, 0.16, 0.16, 0.18, 0.16,
           0.14, 0.2 , 0.18, 0.16, 0.16, 0.18, 0.16, 0.2 , 0.18, 0.14, 0.18,
           0.18, 0.18, 0.16, 0.18, 0.16, 0.18, 0.18, 0.16, 0.16, 0.16, 0.16,
           0.16, 0.18, 0.16, 0.16, 0.16, 0.18, 0.16, 0.16, 0.18, 0.16, 0.18,
           0.16])

Problem identified: cancellation in pdif + qdif. Worst for x=0.1591562038550412:

    In [64]: f = lambda x, func=func: _patch_jax._power_diff(np.array(x), np.array(0), np.arra
        ...: y(-1e-15)).item() / func(x, 0, -1e-15) - 1

    In [66]: optimize.minimize_scalar(f, bounds=(0.01, 0.5), method='bounded')
    Out[66]: 
         fun: -3.287381478145335e-11
     message: 'Solution found.'
        nfev: 24
         nit: 24
      status: 0
     success: True
           x: 0.1591562038550412

    In [67]: np.expm1(1e-15*np.log(0.1591562038550412)).item()
    Out[67]: -1.8378691448322458e-15

    In [68]: 2*_patch_jax._zeta_zero(-1e-15).item()
    Out[68]: 1.8378770664093433e-15

Possible solution: I can't just set to 0 things below tol, I have to ignore
digits below tol.

In the meanwhile, I may have identified the source of problems for the whole
periodic zeta: inaccurate scipy's zeta zero near -2:

    In [913]: f(2,1e-10)
    Out[913]: -0.030448487956593377

    In [914]: mpmath.diff(mpmath.zeta,-2)
    Out[914]: mpf('-0.030448457058393271')

So the error is in hze. Maybe open an issue on scipy.

--------

scipy's zeta does x % 4 instead of x % 2, and indeed the zeros are less
accurate. However for x multiple of 4 they are still not good. Maybe it depends
on how they compute the Gamma (Lanczos approximation).

My own implementation of zeta works almost perfectly, if not that at z = -16
the zero breaks. The reason is the 1 - s taken for the reflection. I need to
keep s1 split into -q + a, pass q and a to _hurwitz_zeta_series, and also to
zeta, such that a never changes as it is added to larger integers.

2022-07-25
==========

Some residual inaccuracy in my zeta zeros for somewhat large negative s
(1000 ULP) is totally due to jnp.exp(jspecial.gammaln(s)). I guess it's the
gammaln and not the exp. Open an issue on jax.

2022-07-26
==========

jax.random.poisson seems very slow, open an issue

does scipy's zeta just return 1 above a certain s? It should

Is jax's zeta more accurate than scipy's zeta? Or scipy's hurwitz zeta than
scipy's zeta? Check

2022-07-27
==========

In scipy there ought to be a test that checks that zeta(53) == 1 + eps and not
1 (currently ok).

mpmath.polylog(s, 1) != zeta(s), open an issue.

--------

List of possible issues to open:

- mpmath.polylog(s, 1) != zeta(s)
    => turns out the problem is the exponentiation of 2j * pi, if I pass 1
    directly it works fine, I think it is mathematically correct
- jax.random.poisson seems very slow
- jnp.polyval does not respect integer types, while numpy.polyval does
    => OPEN
- jax.scipy.special.gammaln seems too inaccurate for large negative argument
- scipy.special.binom does not obey https://dlmf.nist.gov/1.2.E6
    => there's an open PR #15216

scipy.zeta:
- does scipy's zeta just return 1 above a certain s?
- Is jax's zeta more accurate than scipy's zeta?
- Is scipy's hurwitz zeta more accurate than scipy's zeta?
- test that checks that scipy.zeta(53) == 1 + eps and not 1
- scipy.zeta zeros are inaccurate
- scipy.zeta inaccurate for large negative s
- hurwitz zeta for s < 1, 0 <= a <= 1

open zeta issues on scipy:
#15036
#14073

2022-08-08
==========

Observations on pdf9:

- one source of problems can be the noisy fake data with grid points near 1,
  since there the fit is forced to return a very small number with small
  uncertainty
- using chol instead of the default eigcut+ removes most of the noise from the
  fake data (make chol default? => need an autorange for the eps in this case)
- increasing the number of datapoints fixes everything and the true
  hyperparameters are recovered correctly.

2022-08-09
==========

To scale the linear PDF fit I need to use Woodsbury's formula because the data
has errors. The true data has a nondiagonal error covariance matrix, so to be
efficient I have to invert the matrix once for the whole optimization. The most
general way to add this feature is passing a decomposition to GP.addcov. To
expose the decompositions, I can add a class method to GP.

2022-08-10
==========

How do I add a user-provided decomposition in GP? Should it be generic, or
only for addcov?

Only in addcov would make the code cleaner probably. There's a single point in
which it is visible that the decomposition is passed along with the original
matrix.

Interface: another single-key dictionary argument with the decompositions.

--------

Instead of saving the decomposition in the _Cov object, save it directly to the
solver cache.

2022-08-11
==========

Write a decomposition class that starts from a whole decomposition and provides
the one for a block. => This is wasteful because the calculations involve the
full matrix. I think that all the decompositions would let me chop off a
submatrix easily. => A method that produces a new decomposition for a submatrix.

--------

gvar bug (ISSUE OPENED):

    In [11]: gvar.var(np.ones(1000))
    ---------------------------------------------------------------------------
    ZeroDivisionError                         Traceback (most recent call last)
    Input In [11], in <cell line: 1>()
    ----> 1 gvar.var(np.ones(1000))

    File _utilities.pyx:900, in gvar._utilities.var()

    ZeroDivisionError: float division

2022-08-14
==========

Notes on implementing automatic solving strategy:

a method _recursive_virtual_solver

has an argument virtual_cache, if it is None (def value for root call) it calls
itself with an empty cache

tries various strategies in hardcoded order, first is naive. Accumulates
computational cost top->down. The non-naive cases stop as soon as the minimum
so far is exceeded. So the first thing that happens is that the completely
naive case is traversed to the end and its cost is determined and works as
first minimum.

no wait, the very first is cache lookup. The cache uses keys in order,
frozenset and reordering to be added later. If successful, cache lookup has
cost zero, else +inf. whenever something is virtually decomposed, it's added to
a copy of the virtual cache for children calls.

the strategy must be traced in a tree that can be executed with actual
operations. the method that does this is _recursive_solver.

2022-08-15
==========

Leave out for now the multiple keys case, since it's combinatorial and thus
more difficult to do efficiently. Just cache it.

Instead of having a _recursive_solve method, I can use a class hierarchy.
Each class has methods to apply virtually or concretely the transformation.

Maybe to start I should just hack woodbury in to make pdf9 work.

2022-08-22
==========

M = B A B^T
B = Q R
M = Q R A R^T Q^T

If B is short, then R is not square, and thus can not be inverted with
solve_triangular.

B = R Q
M = R Q A Q^T R^T

In this case R is square and Q rectangular. Q^T is a pseudoinverse of Q.

M^-1 = R^-T Q A^-1 Q^T R^-1

To obtain the RQ decomposition, use

B^T = Q R
B = (Q R)^T = R^T Q^T

2022-08-23
==========

M^-1 is not a valid pseudoinverse! MM^-1 b != b. M is short so I want MM^-1 = I
even though it's a pseudoinverse.

The culprit is Q. QQ^T = I, but Q^TQ not.

--------

I'm having problems implementing even just woodbury into the automatic
decomposition optimizer.

  - I need to notice, in a generic linear transformation, that one of the
    operands is just summed without changes. If I don't do it, the explicit
    square matrix coefficient, which is the identity, must be fully decomposed
    with QR, making the optimization moot.

  - In general I need to decompose things which are not indexed as covariance
    matrices in the GP. I need something more generic, like an optimizer over
    expressions on p.d. matrices.

I can't afford this much time now. Reasonably quick alternative: allow ycov to
be a decomposition, in which case woodbury is used.

2022-08-24
==========

For pdf9, I can't do woodbury only for single-key data because there are also
the constraints.

2022-08-25
==========

pdf9 with data covariance matrix:

    hyperparameters (true, fitted, prior):
    log(scale)           0.94       0.96(16)      -0.7(1.0)
    scale                 2.6       2.62(43)       0.50(50)
    U(alpha_Sigma)       -1.8       0.09(96)       0.0(1.0)
    alpha_Sigma         -0.46       0.04(38)       0.00(40)
    U(alpha_g)          -0.47      -0.91(25)       0.0(1.0)
    alpha_g             -0.18     -0.319(65)       0.00(40)
    
... and with decomposition of the matrix + woodbury:

    hyperparameters (true, fitted, prior):
    log(scale)           0.94       0.94(16)      -0.7(1.0)
    scale                 2.6       2.55(40)       0.50(50)
    U(alpha_Sigma)       -1.8      -0.16(90)       0.0(1.0)
    alpha_Sigma         -0.46      -0.06(35)       0.00(40)
    U(alpha_g)          -0.47      -0.93(24)       0.0(1.0)
    alpha_g             -0.18     -0.324(63)       0.00(40)

It works, but they are too different. In the woodbury case, minimize complained
that "minimization failed: A bad approximation caused failure to predict
improvement."

Also, it worked only with method='hessian'; 'fisher' fails completely, while
'gradient' botches the covariance (while tipically the bfgs inverse hessian
is decent).

What could be the culprit(s)? Maybe the sandwiched covariance being not
exactly but quite degenerate is a problem for Woodbury? Or derivatives
through Woodbury are broken? (If so, why would the linalg tests not notice?)

w/o woodbury and all decomps are eigcut-:

    hyperparameters (true, fitted, prior):
    log(scale)           0.94       0.95(13)      -0.7(1.0)
    scale                 2.6       2.57(33)       0.50(50)
    U(alpha_Sigma)       -1.8      -0.15(81)       0.0(1.0)
    alpha_Sigma         -0.46      -0.06(32)       0.00(40)
    U(alpha_g)          -0.47      -0.74(17)       0.0(1.0)
    alpha_g             -0.18     -0.271(51)       0.00(40)

w/ woodbury and all decomps are eigcut-:

    hyperparameters (true, fitted, prior):
    log(scale)           0.94       0.97(13)      -0.7(1.0)
    scale                 2.6       2.63(35)       0.50(50)
    U(alpha_Sigma)       -1.8      -0.17(99)       0.0(1.0)
    alpha_Sigma         -0.46      -0.07(39)       0.00(40)
    U(alpha_g)          -0.47      -0.95(22)       0.0(1.0)
    alpha_g             -0.18     -0.330(54)       0.00(40)

and the minimizer complains as above.

Conclusion: it was not stable even without woodbury, the solver changes the
result. Maybe I should increase the eps?

w/o woodbury and all decomps are eigcut- and eps=1e-10:

    hyperparameters (true, fitted, prior):
    log(scale)           0.94       0.95(13)      -0.7(1.0)
    scale                 2.6       2.57(33)       0.50(50)
    U(alpha_Sigma)       -1.8      -0.15(81)       0.0(1.0)
    alpha_Sigma         -0.46      -0.06(32)       0.00(40)
    U(alpha_g)          -0.47      -0.74(17)       0.0(1.0)
    alpha_g             -0.18     -0.271(51)       0.00(40)

w/ woodbury and all decomps are eigcut- and eps=1e-10:

    hyperparameters (true, fitted, prior):
    log(scale)           0.94   1 +- 8.1e+04      -0.7(1.0)
    scale                 2.6   3 +- 2.1e+05       0.50(50)
    U(alpha_Sigma)       -1.8  -0.2 +- 3e+03       0.0(1.0)
    alpha_Sigma         -0.46-0.07 +- 1.2e+03       0.00(40)
    U(alpha_g)          -0.47  -2 +- 5.2e+06       0.0(1.0)
    alpha_g             -0.18-0.5 +- 3.4e+05       0.00(40)

Broken!

Next step: test woodbury with unit tests to check at least it works without
degeneracies. Test all possible relevant structures of transformations.

2022-08-30
==========

Next things that I could do:
 - understand the inaccuracy of woodbury
 - add variance hp to pdf9

--------

Summary of observations on woodbury's inaccuracy from the unit test:

  - happens with either degenerate A or C (in A + B C B^T)
  - tall B (i.e., degerate B C B^T) is not a problem
  - gets worse with chol and even more with eigcut-

Here I'm measuring "inaccuracy" with the matrix relative 2-norm between the
inverse computed on the dense matrix and with woodbury, where the reference
matrix for the norm is the first. Is this a relevant measure of inaccuracy? Is
the problem that woodbury does not constitute a moore-penrose pseudo-inverse,
while the dense decompositions do?

Can't be exactly this, because with nonsingular A and degenerate C, the whole
matrix is nonsingular, so I'm computing a true inverse.

Woodbury identity:

    (A + BCB^T)^-1 = A^-1 - A^-1 B (C^-1 + B^T A^-1 B)^-1 B^T A^-1

Wikipedia gives a version for singular C:

    = A^-1 - A^-1 B (I + C B^T A^-1 B)^-1 C B^T A^-1

I can write a new woodbury and test if this solves at least the problems with
singular C. A will be more difficult I fear.

Also, this formula does not require to decompose C. It seems strictly better
than normal woodbury even if C was invertible. What's the catch? Why isn't it
the standard one then? => See 2022-09-06.

--------

The formula for the derivative of A^-1 does not generalize to A^+
https://en.wikipedia.org/wiki/Moore–Penrose_inverse#Derivative

2022-08-31
==========

For nonsingular C and singular A, see Bernstein 2018, Fact 8.4.13 p. 635. But
it works only if the range of B is contained in the range of A. Does it work
for singular C by moving it like in the formula above?

--------

List of useful things from Bernstein 2018:
    
    Fact 8.4.13 p. 635: (A + BCB^t)^+ with singular A, nonsingular C, rB in rA
    Fact 8.4.33 p. 638: (AA^t + BB^t)^+
        Consider Israel and Greville 2003, eq. 1.17 p. 262, for (CC^t)^+
    Fact 8.9.30-31 p. 667: [A B; B^t 0]^+
    Fact 8.12.4 p. 678: [A B; C D]^D
    Proposition 10.2.1: LDLT of pos sdef [A B; B^t C] with singular A and/or C
    Fact 10.9.4 p. 725: det and quad of Wiener covariance matrix
    Fact 10.9.5 p. 725: list of 10 covariance functions
    Fact 10.9.6 p. 726: det and inverse of Wiener on regular grid
    Fact 10.9.7 p. 726: list of 18 complex covariance functions
    Fact 10.9.8 p. 727: list of 6 covariance functions on integers
    Fact 10.9.10-10.9.15 p. 727-728: other covariance functions
    Fact 10.9.18 p. 729: a pd-preserving matrix transformation
    Fact 10.24.2 p. 815: A^+ = A^# = A^D for psd A

Question: for pos sdef A, is it true that A^+ = lim x->0 (A + Ix)^-1?

=> Nope, it's false. The correct limit (for sym A) is

    A^+ = lim x->0 A (A^2 + xI)^-1

So maybe when I do cholesky I should compute A^2, regularize, decompose.
Problem: this formula is not symmetric (=> no (de)correlate), and A^2 is worse
conditioned than A. Can I write the limit as L^T (A^2 + xI)^-1 L with A = LL^T?
=> still bad because I need to decompose A before reg, so not useful. Maybe A
(A^3 + xI)^-1 A works? But no decorrelate still.

And the logdet?

See https://en.wikipedia.org/wiki/Pseudo-determinant

Probably I should not sum all eigenvalues in eigcut+ and svdcut+, but just
the unregulated ones.

2022-09-04
==========

jax 0.3.16 introduces pure_callback, to use python functions within
jit-compiled ones. Coupled with custom_jvp, I think I can use it to quickly add
jit support for some kernels until I implement them in JAX, Matérn in
particular

2022-09-06
==========

The formula

    (A + BCB^T)^-1 = A^-1 - A^-1 B (I + C B^T A^-1 B)^-1 C B^T A^-1

is not directly usable because C B^T A^-1 B is not symmetric in general.
Perhaps I can do something with the outer product decomposition of C:

    C = LL^T (not necessarily cholesky)
    
    (A + BCB^T)^-1
    = A^-1 - A^-1 B (C^-1 + B^T A^-1 B)^-1 B^T A^-1
    = A^-1 - A^-1 B I (C^-1 + B^T A^-1 B)^-1 I B^T A^-1
    = A^-1 - A^-1 B L L^-1 (C^-1 + B^T A^-1 B)^-1 L^-T L^T B^T A^-1
    
        X^-1 Y^-1 Z^-1 = (ZYX)^-1
        
    = A^-1 - A^-1 B L (L^T C^-1 L + L^T B^T A^-1 B L)^-1 L^T B^T A^-1
    
        C^-1 = L^-T L^-1

    = A^-1 - A^-1 B L (L^T L^-T L^-1 L + L^T B^T A^-1 B L)^-1 L^T B^T A^-1
    = A^-1 - A^-1 B L (I + L^T B^T A^-1 B L)^-1 L^T B^T A^-1

Problem: if C is not full-rank, then L is tall, L L^+ != I, and the above proof
does not work replacing L^-1 with L^+. However, since the final expression
does not involve L^+, it could be that it still works. Let's check:

    (A + BCB^T) (A^-1 - A^-1 B L (I + L^T B^T A^-1 B L)^-1 L^T B^T A^-1) =
    
    = A A^-1 +
    - A A^-1 B L (I + L^T B^T A^-1 B L)^-1 L^T B^T A^-1 +
    + B C B^T A^-1 +
    - B C B^T A^-1 B L (I + L^T B^T A^-1 B L)^-1 L^T B^T A^-1
    
    = I +
    - B L (I + L^T B^T A^-1 B L)^-1 L^T B^T A^-1 +
    + B C B^T A^-1 +
    - B C B^T A^-1 B L (I + L^T B^T A^-1 B L)^-1 L^T B^T A^-1
    
    = I +
    - B L I (I + L^T B^T A^-1 B L)^-1 L^T B^T A^-1 +
    + B C B^T A^-1 +
    - B L L^T B^T A^-1 B L (I + L^T B^T A^-1 B L)^-1 L^T B^T A^-1
    
    = I +
    - B L (I + L^T B^T A^-1 B L) (I + L^T B^T A^-1 B L)^-1 L^T B^T A^-1 +
    + B C B^T A^-1

    = I +
    - B L L^T B^T A^-1 +
    + B C B^T A^-1

    = I +
    - B C B^T A^-1 +
    + B C B^T A^-1

    = I
    
It works!

=> 2022-11-19 I'm dumb: this is just a particular case of normal Woodbury.

2022-09-07
==========

However to use it I need the operation X -> X L or X -> L^T X. Currently I only
have X -> L X and X -> L^-1 X.

2022-09-10
==========

Other problem: I can't have custom derivatives for X -> L^T X, and thus
derivatives for the whole decomposition.

Idea: in decompautodiff, instead of assuming that K is the first positional
argument, call a method that takes all the arguments and produces K. The
default implementation returns the first argument, however subclasses can
customize it. Woodbury would return A ± BCB^T.

This means I need a method `matrix()` that returns the decomposed matrix.
decompautodiff provides its implementation, like it does for n().

--------

For Woodbury with singular A, nonsingular C, singular M: see Riedel 1992:

    M = A + BCB^T, with C smaller than A
    
    B = X + Y, where
        im X is in im A
        im Y is orthogonal to im A
        Y is full rank
    
    Z = Y^+ = (Y^T Y)^-1 Y^T
    
    M^+ = A^+ - Z^T X^T A^+ - A^+ X Z + Z^T (C^-1 + X^T A^+ X) Z

How do I split B into X + Y? QR of B?

    B = QR

(B is tall, so Q is tall and R is square)

No wait I need the space of A, not B. Let P_A be the orthogonal projector into
im A, then

    X = P_A B
    Y = (I - P_A) B
    
    X + Y = P_A B + (I - P_A) B = B => ok
    P_A B in im A => ok
    P_A (I - P_A) B = P_A B - P_A B = 0 => ok
    
To compute P_A, I need to diagonalize A.

Add methods improj and nullproj to Decomposition. The cholesky decompositions
work by assuming pos def matrix so they will implement identity and zero, while
diagonalizations work. There shouldn't be undefined cases. Composite
decompositions may have problems as usual.

Note: since Y must be full rank and be orthogonal to im A, it must be
rank A <= size A - size C. This is very limiting. <======== *******

--------

things to do:

    - add composite decomposition support to decompautodiff (DONE)
    - apply it to all existing CDs and rerun tests (DONE)
    - add option "transpose" to correlate, test it (DONE)
    - write Woodbury2 for singular C and test it
    - write Woodbury3 using Bernstein 8.4.33 for singular A and C

--------

Bernstein 8.4.33:

    (AAt + BBt)+ = [A+ (I - BC+)]t E A+ (I - BC+) + (CCt)+
    
    C = (I - AA+) B
    
    E = I - A+B (I - C+C) [I + (I - C+C) Bt (AAt)+ B (I - C+C)]-1 (A+B)t

The BBt term corresponds to the first term in woodbury, since
    
    CCt = (I - AA+) BBt (I - AA+)

Many terms are projectors. I suspect BC+ is a projector too. I need to
check if projectors are respected by the pseudoinverse.

Hypothesis: A = PX, A+ = X+P

Properties to check:

    1   AA+A = A
    2   A+AA+ = A+
    3   (AA+)t = AA+
    4   (A+A)t = A+A

 1) PX X+P PX = PXX+PX = ?

Nope I can't get PX out of it, I would need to get XX+X without the P in
between.

What I can say with projectors is that (PX)+P = (PX)+.

What about PXP? Is it (PXP)+ = PX+P?

 1) PXP PX+P PXP = PXPX+PXP
 
=> nope

Problem: E does not seem to be hermitian. What's up? => It's an error,
backfixed now. Correct reference: Schott 2017, th. 5.15.

Other problem: even though I think BBt aesthetically corresponds to the first
term in woodbury, actually in practice it's AAt, because it's the one whose
inverse I can precompute. Check of feasibility: assume that B is tall. C has
the same shape as B, so CCt is large. The term [I + ...] is small because
the sandwich contains C+C. I have to invert CC+ every time and it's large.
However, since C is tall, maybe it's efficient.

I try with the SVD of C:

    C = UDVt
    Ct = VDUt
    CCt = UDVt VDUt = U D^2 Ut
    (CCt)+ = U D^2+ Ut

It works.

Question: is (AAt)+ = A+tA+?

 1) AAt A+t A+ AAt =
    A (A+A)t A+A At =   (A+A is hermitian)
    A (A+A)^2 At =      (A+A is idempotent)
    A A+A At =          (property 1 of A+)
    A At

Ok it's on wikipedia, I skip the whole check.

Question: can I replace all separate occurrences of A with AAt and (AAt)+,
such that I can use the decomposition of AAt without correlate/decorrelate?

    (AAt + BBt)+ = [A+ (I - BC+)]t E A+ (I - BC+) + (CCt)+
    C = (I - AA+) B
    E = I - A+B (I - C+C) [I + (I - C+C) Bt (AAt)+ B (I - C+C)]-1 (A+B)t

First I address the projector AA+. Does this project on the image of
X = AAt?

    AA+ X = AA+A At = AAt = X

This means that AA+ projects on something containing the image of X. I need to
check that the intersection with the kernel is 0.

    X AA+ = AAt AA+

No wait not the kernel of X, the orthogonal complement of the image of X:

    X+ = (AAt)+ = A+t A+
    X+ AA+ = A+t A+AA+ = A+t A+ = X+
    XX+ = AAt A+t A+ = A (A+A)t A+ = A A+A A+ = AA+

    (I - XX+) AA+ = AA+ - X X+AA+ = AA+ - XX+ = 0  => ok

So I can use the projector of X instead of the one of A. Wait it was way
easier: XX+ = AA+. Whatever.

    C = (I - XX+) B
    
    (X + BBt)+ = (CCt)+ + (I - BC+)t A+t E A+ (I - BC+) =
        = (CCt)+ + (I - BC+)t A+t [I - A+B (I - C+C) [I + (I - C+C) Bt (AAt)+ B (I - C+C)]-1 (A+B)t] A+ (I - BC+) =
    
            M := (I - C+C) [I + (I - C+C) Bt (AAt)+ B (I - C+C)]-1
               = (I - C+C) [I + (I - C+C) Bt X+ B (I - C+C)]-1
               = (I - C+C) [I + (I - C+C) Bt X+ B (I - C+C)]-1 (I - C+C)

        = (CCt)+ + (I - BC+)t A+t [I - A+B M (A+B)t] A+ (I - BC+) =
        = (CCt)+ + (I - BC+)t A+t A+ (I - BC+) - (I - BC+)t A+t A+B M (A+B)t A+ (I - BC+) =
        = (CCt)+ + (I - BC+)t (AAt)+ (I - BC+) - (I - BC+)t A+t A+ B M Bt A+t A+ (I - BC+) =
        = (CCt)+ + (I - BC+)t (AAt)+ (I - BC+) - (I - BC+)t (AAt)+ B M Bt (AAt)+ (I - BC+) =
        = (CCt)+ + (I - BC+)t X+ (I - BC+) - (I - BC+)t X+ B M Bt X+ (I - BC+)

Now I have to replace BBt with VYVt.
    
    BBt = VYVt
    Y = LLt (not necessarily cholesky)
    B = VL
    CCt = (I - XX+) BBt (I - XX+) = (I - XX+)V Y Vt(I - XX+)

2022-09-17
==========

The projectors of the kind I - UU+ are tipically very low-rank. For
decompositions which assume the matrix is pd (like cholesky), the projector
is zero i.e. a product of empty vectors.

How should I implement them? Alternatives:

  - Produce them manually
  - Introduce new methods in Decomposition

In the second case:

  - Return a decomposition of the projector
  - Provide a quad/solve operation

The projectors appearing in the formula are:

  * I - XX+
  * I - C+C
  * I - BC+

Since C is rectangular, only the first can be obtained from a Decomposition.
For the other two I have to use the svd manually.

    (CCt)+(CCt) = Ct+ C+ C Ct =
                = Ct+ (C+C) Ct =
                = Ct+ (C+C)t Ct =
                = Ct+ Ct Ct+ Ct =
                = (CC+CC+)t =
                = (CC+)t =
                = CC+

I can't obtain C+C from CCt. It's a smaller matrix.

    C = (I - XX+) B =
      = B - XX+B ->
      
      -> B = C + XX+B
    
    I - BC+ = I - (C + XX+B)C+ =
            = I - CC+ - XX+BC+

2022-09-18
==========

Other thoughts on pinv with cholesky: The limit form of the pseudoinverse is

    A+ = lim x->0 A (A^2 + xI)^-1

(for symmetric A). Maybe this works in practice because: A^2 is more
ill-conditioned, so the low eigenvalues are more "buried" beneath x. But I
still multiply by A so the approximate kernel is respected, without being
counterbalanced with a multiplication by a large number.

Defect: I want a symmetric formula for correlate & decorrelate.

    A = LLt
    
    A2 = LLt LLt
    
    A+ =? lim x->0 Lt (A2 + xI)^-1 L

    A+(x) A A+(x) = Lt (A2 + xI)-1 L LLt Lt (A2 + xI)-1 L

Dunno, let's first check with the inverse:

    A-1 = Lt-1 L-1
    
    L-1 A-1 L = L-1 Lt-1 L-1 L = L-1 Lt-1 => nope
    
    X A-1 Y = A-1
    (X Lt-1) (L-1 Y) = Lt-1 L-1

It's impossible. The other idea was

    A+(x) = A (A3 + xI)-1 A
    
    A+(x) A A+(x) = A (A3 + xI)-1 A3 (A3 + xI)-1 A
    
    A A+(x) A = A2 (A3 + xI)-1 A2 = A3 (A3 + xI)-1 A

In the limit xI (A3 + xI)-1 produces something living in the kernel/cokernel
of A so it's zero as long as it's multiplied by A. A commutes with A3 + xI and
so also with (A3 + xI)-1.

    (AA+(x))t = (A2 (A3 + xI)-1 A)t =
              = (A3 (A3 + xI)-1)t =
              = (A3 + xI)-1 A3 =
              = A2 (A3 + xI)-1 A =
              = AA+(x)

    Operation       Cost [matmul]
    A3              3
    chol(A3 + xI)   1
    L-1 A           1

Usage:

    A3 + xI = LLt
    
    A+ = A (A3 + xI)-1 A =
       = A Lt-1 L-1 A =         (solve)
       = (L-1 A)t (L-1 A)       (quad, decorrelate)
      
    A = A A+ A =
      = A (L-1 A)t (L-1 A) A =
      = (L-1 A2)t (L-1 A2)      (correlate)
    
    det(A3) = det(A)^3
    logdet(A) = 6 logdet L      (logdet)

Question: should I premultiply L-1 by A? Or do it after applying A? If the
operand is a vector/tall matrix, applying L-1 only at the end is more
convenient.

On the other hand I should also care about numerical accuracy. Maybe
keeping L-1 last means that the irregularity prevails because A cannot enforce
its kernel beyond floating point accuracy.

--------

On the logdet of eigcut+/-: maybe I should compute the logdet of eigcut-
by considering all the missing dimensions at the threshold instead of removing
them, for continuity.

Choices:

    - pdet: the marginal likelihood is in some sense more correct
    - det: the marginal likelihood is continuous w.r.t. hyperparameters

Probably the second is more important. => If needed I can make a separate
method logpdet.

--------

How do I compute the logdet with the pinv version of woodbury?

2022-10-31
==========

New things since v0.12:

- Fourier -> Zeta
- GP.decompose
- GP.addcov(decomps=...)
- GP.pred and GP.marginal_likelihood accept a decomposition as y error
  covariance, and use it with woodbury
- lsqfitgp.sample
- empbayes_fit(initial=...)
- empbayes_fit(verbosity=...)
- empbayes_fit.pmean, .pcov, .minargs
- MA(norm=...)
- BART.correlation
- BART optimization for last two levels
- fixed bugs with new jax versions

2022-11-01
==========

There are many serious bugs due to jax updates.

GP prediction with gvars does not work anymore. The cause is that custom_jvp
functions now can not return object dtypes. I think the only case where this is
a problem is in quad_autodiff, so I can just call oldquad if c's dtype is
object. => seems solved.

GammaExp(gamma=2) is not differentiable at distance 0. I guess that now
differentiating x ** 1 at x = 0 gives nan. => the second derivative, not the
first.

--------

Github actions is still emitting warnings. They are due to
JamesIves/github-pages-deploy-action@v4.3.3 and actions/setup-python@v3, I have
to try changing the version.

2022-11-05
==========

After the last update to jax (0.3.24), I've noticed the following xpasses in
linalg tests:

tests/test_linalg.py::TestBlockDiag::test_solve_vec_jac_rev PASSED                                                                 [ 57%]
tests/test_linalg.py::TestBlockDiag::test_solve_matrix_jac_rev PASSED                                                              [ 59%]
tests/test_linalg.py::TestBlockDiag::test_solve_vec_jac_rev_jit PASSED                                                             [ 60%]
tests/test_linalg.py::TestBlockDiag::test_solve_matrix_jac_rev_jit PASSED                                                          [ 61%]
tests/test_linalg.py::TestBlockDiag::test_solve_matrix_jac_rev_matrix PASSED                                                       [ 62%]
tests/test_linalg.py::TestBlockDiag::test_quad_vec_jac_rev PASSED                                                                  [ 63%]
tests/test_linalg.py::TestBlockDiag::test_quad_matrix_jac_rev PASSED                                                               [ 65%]
tests/test_linalg.py::TestBlockDiag::test_quad_vec_jac_rev_jit PASSED                                                              [ 66%]
tests/test_linalg.py::TestBlockDiag::test_quad_matrix_jac_rev_jit PASSED                                                           [ 67%]
tests/test_linalg.py::TestBlockDiag::test_logdet_jac_rev PASSED                                                                    [ 68%]
tests/test_linalg.py::TestBlockDiag::test_logdet_jac_rev_jit PASSED                                                                [ 69%]

tests/test_linalg.py::TestSandwichSVDDiag::test_solve_matrix_hess_fwd_rev PASSED                                                   [ 77%]

tests/test_linalg.py::TestWoodburyDiag::test_solve_vec_jac_rev PASSED                                                              [ 84%]
tests/test_linalg.py::TestWoodburyDiag::test_solve_matrix_jac_rev PASSED                                                           [ 85%]
tests/test_linalg.py::TestWoodburyDiag::test_solve_vec_jac_rev_jit PASSED                                                          [ 86%]
tests/test_linalg.py::TestWoodburyDiag::test_solve_matrix_jac_rev_jit PASSED                                                       [ 87%]
tests/test_linalg.py::TestWoodburyDiag::test_solve_matrix_jac_rev_matrix PASSED                                                    [ 89%]
tests/test_linalg.py::TestWoodburyDiag::test_quad_vec_jac_rev PASSED                                                               [ 90%]
tests/test_linalg.py::TestWoodburyDiag::test_quad_matrix_jac_rev PASSED                                                            [ 91%]
tests/test_linalg.py::TestWoodburyDiag::test_quad_vec_jac_rev_jit PASSED                                                           [ 92%]
tests/test_linalg.py::TestWoodburyDiag::test_quad_matrix_jac_rev_jit PASSED                                                        [ 93%]
tests/test_linalg.py::TestWoodburyDiag::test_logdet_jac_rev PASSED                                                                 [ 95%]
tests/test_linalg.py::TestWoodburyDiag::test_logdet_jac_rev_jit PASSED                                                             [ 96%]

Are they new? Do they still fail in old jax versions? In which version did they
stop failing? Should I unmark them and require jax >= 0.3.24? Or wait to see
if they remain xpasses with further updates or regress back? => They are still
ok in jax 0.3.25, but let's wait until 0.4 which should be coming soon.
=> turns out they work already in 0.3.17, I removed the marks.

2022-11-06
==========

I think I have found a jax bug: jnp.mean(..., where=...) return wrong result
under jit, should be reproducible with 1-length arrays and all go mask.

2022-11-20
==========

In the eigenvalue truncating decompositions, add the missing epsilons to the
determinant to make it smooth.
