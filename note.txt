La formula con le gvar per il processo gaussiano è

    y* = y*_p + Kxsx (Kxx + Cy)^-1 (y - y_p)

Avevo già controllato che questa formula mi dà la matrice di covarianza
corretta per y*. Però vengono fuori anche le correlazioni tra y* e y, y_p,
y*_p. Che cosa rappresentano statisticamente?

Assumiamo che y e y(*)_p siano indipendenti. Allora

    Cov[y*, y*_p] = Cov[y*_p] = Kxsxs

    Cov[y*, y_p] = Cov[-Kxsx (Kxx + Cy)^-1 y_p, y_p] =
                 = -Kxsx (Kxx + Cy)^-1 Kxx
    
    Cov[y*, y] = Kxsx (Kxx + Cy)^-1 Cy

2022-04-17
==========

Cose da fare per le PDF:

1)  Mi invento io delle funzioni e dei dati dal culo e faccio un fit,
    solo roba lineare (FATTO)
2)  Aggiungo roba quadratica (sempre dal culo) (FATTO)
3)  Aggiungo l'errore sulle M (FATTO)
4)  Uso delle PDF vere anziché culiche
5)  Aggiungo i dati veri

2022-04-18
==========

Come implemento addproc etc.?

1)  Chiamo _KernelDeriv -> _CrossKernel (FATTO)
1a) Sposto la logica di diff fuori da fun e giro i test (FATTO)
1b) Modifico diff in modo da tener conto che le x a sinistra possono essere di
    tipo diverso dalle x a destra (SALTARE)
2)  Sposto _binary in _KernelBase e tolgo il controllo che gli scalari siano
    positivi, come classe uso type(self) anziché Kernel (FATTO)
3)  Sovrascrivo _binary in Kernel controllando che l'eventuale scalare sia
    positivo e poi chiamo super (FATTO)
4)  Aggiungo un metodo rescale a _KernelBase, funziona come diff cioè fa
    trasf. diverse per x e y, se sono diverse (definito con is not) cambia la
    classe dell'output a _CrossKernel (FATTO)
5)  Definisco nuove classi _Proc, _ProcKernel, _ProcTransf e _ProcDeriv
    analoghe di _Element, _Points e _Transf (NON FARE _ProcDeriv) (FATTO)
5a) Aggiungo un attributo proc di tipo _Proc a _Points (FATTO)
6)  Definisco _GP._crosskernel che calcola il kernel per due _Points
    ricorsivamente, con metodi analoghi a quelli che uso per implementare
    _makecovblock: _crosskernel_kernel_kernel, _crosskernel_transf_any,
    _crosskernel_deriv_any (NON FARE _deriv) (FATTO)
7)  _crosskernel_kernel_kernel semplicemente restituisce il kernel se i due
    _ProcKernel coincidono, altrimenti un sigleton _ZeroCov (FATTO)
8)  _crosskernel_deriv_any calcola il kernel e poi chiama diff, _ZeroCov viene
    passato (SALTARE)
9)  _crosskernel_transf_any cicla sugli elementi calcolando i kernel e
    combinandoli con _binary, gli _ZeroCov vengono ignorati, gli scalari nulli
    non producono _ZeroCov altrimenti si incasinano le derivate, alla fine se è
    uno _ZeroCov viene passato (FATTO)
10) Aggiungo GP.addproc, addproctransf e addprocderiv, i _Proc vengono infilati
    in un dizionario apposito, possono avere gli stessi nomi degli x (NON FARE
    addprocderiv) (FATTO)
11) Aggiungo proc= ad addx, deriv= continua ad avere la sua implementazione a
    parte in _Points e _makecovblock_points (FATTO)
12) Controllo che le x abbiano lo stesso tipo, per adesso (FATTO)
13) Riimplemento la somma di componenti con questa roba (FATTO)

Problema serio: come faccio a specificare cosa derivare se ogni processo ha la
sua x? Cioè: le x sono le stesse, oppure ogni processo ha la sua x?

=> Soluzione rapida e parziale: posso derivare solo singoli processi e non le
trasformazioni. In questo modo continuo ad assumere che le x siano le stesse.

=> Soluzione più completa: quando dico la derivata, devo poter specificare
rispetto a quale processo, o se derivare rispetto a tutte le x, in generale
devo poter specificare vari criteri. Forse è troppo complicato e l'utente non
avrebbe chiaro cosa sta facendo.

L'interfaccia elegante sarebbe questa: quando definisco una trasformazione dei
processi, devo dire se le x sono le stesse o sono diverse. In formule, devo
distinguere

    h(x) = f(x) + g(x)

da

    h(x, y) = f(x) + g(y)

Un modo sensato sarebbe proprio leggersi una stringa del genere. Una versione
più accroccata intermedia sarebbe un'opzione di addproctransf che mi
permette di dire se le x sono tutte le stesse o sono diverse, per fare
combinazioni più strane chiamare più volte addproctransf. Se sono le stesse poi
addx controlla che metti dentro x con lo stesso dtype. Poi però devo inventarmi
un modo di propagare queste informazioni fino a _KernelBase.diff.

--------

What calculation should I do to bypass the limitation that new gvars can not be
correlated with old gvars?

Old = x, new = y

Desired covariance matrix:

    V = [ V_xx   V_xy]
        [ V_yx   V_yy]

z = auxiliary variables, as many as y, independent of x

I have to write y as a combination of z and x:

    y = Az + Bx

Cov[x, y] = Cov[x, Bx] = V_xx B^T = V_xy   ==>  B^T = V_xx^-1 V_xy
Cov[y, y] = Cov[Az, Az] + Cov[Bx, Bx] = A V_zz A^T + B V_xx B_T = V_yy  ==>
    ==>  A V_zz A^T = V_yy - V_yx V_xx^-1 V_xy = V/V_xx

I can choose A = I and then V_zz = V/V_xx. I have to decompose V_xx.

gac, I rediscovered conditioning

2022-04-22
==========

The nonlinear fit isn't working. With enough quadratic data the fitted data
(even the linear one) is far from the actual data, but even with one single
nonlinear point the fit starts to fail in some places.

Is this a bug or an excess of nonlinearity?

In the latter case, I expect that reducing the errors on the quadratic data
should effectively linearize it. Then, provided I start the fit already in the
correct neighborhood, it should work.

=> tried: it's even worse!

I'm combining 9 latent points into one single quadratic datapoint. Maybe even
if the error on the datapoint is small, the freedom left to the latent points
makes their nonlinear bias large.

=> Using only one point as quadratic input, still does not work at all. This
suggests it is a trivial bug of some sort.

Bug found: I was passing the prior through the nonlinear function and then
sampling, instead of sampling and then passing the sample through the nonlinear
function. Since the prior is wide, it was being affected a lot by the
nonlinearity.

--------

Next thing to do: pdf6.py, fit hyperparameters and quadratic data together. I
foresee that it will be slow because I can't take derivatives through lsqfit
with autograd. I should read how lsqfit.empbayes_fit is implemented.

Actually, I could first add errors to M and M2 instead of fitting kernel
hyperparameters.

2022-04-23
==========

I must also devise a convenient extension of predfromfit for the case where I
condition on some stuff with predfromdata and on some other with lsqfit. For
the time being, since the sum rule constraints are exact, I think that
conditioning twice is valid (but I should check to be sure, maybe add a test
case).

--------

The fit with the errors on M is not working. Two alternative explanations:

  * It can't work because there is too much uncertainty. But I would expect the
    fit to return an appropriately large sdev in this case, which is not
    happening.

  * The sheer amount of the M parameters (2700) makes the minimization "happy"
    too early because M is easy to fit.

To check this, I can hold M fixed to its mean (different from the true value
used to generate the data) without fitting it. If the result I get is still
nonsense, then probably it's a statistical and not a numerical problem.

=> The fit makes sense. This indicates that it's a numerical problem. Possible
quick hack: very low termination tolerance.

=> The quick fix didn't work, but starting from the true values gives a good
result, which confirms it is a numerical problem.

Anyway now I found out that in the actual problem M is actually parametrized by
just a few numbers despite being a large matrix, so whatever.

To do next time: pdf7.py, i.e., fit hyperparameters on top of nonlinear fit
(will be damn slow!)

Following steps:
  * pdf8.py scale the number of datapoints/parameters to a realistic one
  * pdf9.py change the fake data, the grids and the kernel to realistic ones

--------

I did a quick benchmark of pdf6. Most of the time is spent in evaluating the
jacobian of the model function with gvar. Time per call:

    fcn(floats): 200 us
    fcn(gvars): 230 ms
    forward jacobian with jax: 30 ms
    backward jacobian with jax: 3 ms
    jacobian with autograd: seconds

I don't expect to find a way to easily optimize the gvar calculation. 7 times
slower than the compiled jax version seems already quite good considering it's
naive sparse operations.

The other half of the time spent by the fit is computing as svd in trf, I
assume it's the svd of the jacobian. So even optimizing the derivative
calculation would give at most a factor of two. Maybe there are other faster
methods in least_squares for a large dense jacobian? => maybe lm? look also
into GSL

If I did things manually, maybe I could take into account that the jacobian is
split in two blocks, one the identity and one dense, and then multiplied by
something. If this something is a diagonalization instead of cholesky then
maybe I can obtain the svd of the jacobian.

2022-04-25
==========

If I set p0 in fitargs of lsqfit.empbayes_fit, will it still adapt p0
automatically? => Yes, it works.

Idea: since I know the components of M and M2, could I first transform the grid
of points for each component? => Maybe it is not doable for the quadratic data,
and moreover in the real problem there are more datapoints than grid points.

--------

I talked to Alessandro about the details of the complete PDF fit. Summary:

The grid of x points is
[
    1.9999999999999954e-07, # start logspace
    3.034304765867952e-07,
    4.6035014748963906e-07,
    6.984208530700364e-07,
    1.0596094959101024e-06,
    1.607585498470808e-06,
    2.438943292891682e-06,
    3.7002272069854957e-06,
    5.613757716930151e-06,
    8.516806677573355e-06,
    1.292101569074731e-05,
    1.9602505002391748e-05,
    2.97384953722449e-05,
    4.511438394964044e-05,
    6.843744918967896e-05,
    0.00010381172986576898,
    0.00015745605600841445,
    0.00023878782918561914,
    0.00036205449638139736,
    0.0005487795323670796,
    0.0008314068836488144,
    0.0012586797144272762,
    0.0019034634022867384,
    0.0028738675812817515,
    0.004328500638820811,
    0.006496206194633799,
    0.009699159574043398,
    0.014375068581090129,
    0.02108918668378717,
    0.030521584007828916,
    0.04341491741702269,
    0.060480028754447364,
    0.08228122126204893,
    0.10914375746330703, # end logspace, start linspace
    0.14112080644440345,
    0.17802566042569432,
    0.2195041265003886,
    0.2651137041582823,
    0.31438740076927585,
    0.3668753186482242,
    0.4221667753589648,
    0.4798989029610255,
    0.5397572337880445,
    0.601472197967335,
    0.6648139482473823,
    0.7295868442414312,
    0.7956242522922756,
    0.8627839323906108,
    0.9309440808717544,
    1, # end linspace
]

Number of datapoints: 4535 total, of which 3089 linear. The relative errors are
tipically 2-10 %, with some exceptions down to 0.1 % and up to 50 %.

It is convenient to describe the PDFs with a change of basis, see:

https://eko.readthedocs.io/en/latest/theory/FlavorSpace.html#qcd-evolution-basis

First, for each quark, define
    
    q+ = q + qbar,
    q- = q - qbar.

The gluon g is left alone. Then

    Sigma = sum_q q+
    V     = sum_q q-
    
    V3  = u- - d-
    V8  = u- + d- - 2s-
    V15 = u- + d- + s- - 3c-
    
    T3  = u+ - d+
    T8  = u+ + d+ - 2s+
    T15 = u+ + d+ + s+ - 3c+

So the Ts are like the Vs but with q+ instead of q-. The constraints are:

    For all f: f(1) = 0

    Total momentum: int dx x (Sigma(x) + g(x)) = 1

    The integrals of the Vs are
        V   3
        V3  1
        V8  3
        V15 3
    
    Sigma and g diverge power-like for x -> 0, the Ts and Vs don't
    
    For g and Sigma: x^2 f(x) -> 0 for x -> 0
    
    For Vs and Ts: x f(x) -> 0 for x -> 0

2022-04-28
==========

How should I parameterize the PDFs in the fit? I should pick the basis which
makes the kernel easier to write. The divergent behaviour is specified in the
Sigma-T-V basis, so I should use it. I could also do like they are doing with
neural networks and multiply a base process by x^-a (1-x)^b, but only Sigma
and g, while the others would get (1-x)^b. Should I let each process have its
own b? Dunno.

--------

I'm having a problem with pdf8, the fit fails due to zeros in the covariance
matrix. The zeros must be numerical. I think it is due to the absurdly high
priors and data I'm putting in due to the divergence of Sigma(x) and g(x).
I should define the transformation w.r.t. x*Sigma and x*g probably, and only
use x from 1e-5 or 1e-4 onward for data.

2022-04-29
==========

In the end the main problem with pdf8.py was the zero error on f(1) due to the
constraint.

To do next time: implement Kernel.xtransf and use a linear interpolation of the
grid with ExpQuad instead of Gibbs(scalefun=x). But before that check the order
thing of the Gibbs kernel.

Implement a decomp parameter to lgp.raniter to see if eigcut- solves the
rough samples problem of gvar.sample (the corresponding functionality in
gvar.sample is broken).

I want to let xSigma and xg go power-like for x->0 with an exponent between
-1 and 1. The most convenient way is defining them as some process rescaled by
x^a, but this messes up the integrals. How do I fix this? What I need in
general is a kernel for a process whose derivative goes like x^a.

--------

Tentative:

    f ~ GP

And the correlation length of f(x) is x (uniform in log(x)), so

    f'(x) ~ 1/x

Where now with ~ we mean "goes like" instead of "is distributed as". So
defining the transformed process

    g(x) = x^(a+1)/(a+2) f(x)

We have
    
    g'(x) = x^a (a+1)/(a+2) f(x) + x^(a+1)/(a+2) f'(x) ~
          ~ (a+1 + 1)/(a+2) (x^a 1 + x^(a+1)/x) =
          = x^a

Then we take g'(x) to be x Sigma(x) or x g(x).

--------

More generally, let w(x) be the change of variable that makes the correlation
length constant. Then the scale function w.r.t. x is

    s(x) = 1/|w'(x)|.

So if f(x) ~ 1, we have f'(x) ~ 1/s(x). Let g(x) be the primitive of x Sigma(x)
or x g(x), which we define as

    g(x) = u(x) f(x)

for a generic factor u(x) which we want to determine such that

    g'(x) ~ x^a.

We have

    g'(x) = u'(x) f(x) + u(x) f'(x) ~
          ~ u'(x) + u(x)/s(x) =             (assuming w'(x) > 0)
          = u'(x) + w'(x) u(x) = x^a
    
This differential equation has the solution

    u(x) = exp(-w(x)) (int dx x^a exp(w(x)) + constant)

To check that we can reobtain the previous result, let
    
    s(x) = x  =>  w(x) = log(x),

then
    
    u(x) = 1/x (int dx x^a x + c) =
         = 1/x (x^(a+2) / (a+2) + c) =
         = x^(a+1)/(a+2) + c/x

Requiring a finite u(0) to have an integrable g'(x), we have

    g(x) = x^(a+1)/(a+2) f(x)   q.e.d.

2022-05-01
==========

pdf8.py works fitting exponents for xSigma and xg. To do next time: increase
the allowed range of exponents (use a uniform prior), use a realistic number
of datapoints (see above), save the history of hyperparameters and marginal
likelihood in the minimization to estimate the hessian with a fit in the end.
Maybe first implement the hessian thing with a simpler fit.

Once I have this I'd say I can stop for a while on the PDFs and maybe port
lsqfitgp to JAX and do other improvements to be able to optimize the fit.

2022-05-02
==========

New piece of information: there aren't only linear and quadratic data, there's
also ratios and other stuff. How many of them?

I won't do the hessian because lsqfit.empbayes_fit does not let me pass
parameters to scipy.minimize so I can't collect the target outputs with
options=dict(return_all=True).

Fit with many datapoints started at 11:15. => First iteration ended at 11:45.
Thus the complete fit would require about 60 hours. The bottleneck appears to
be the jacobian calculation with gvar, in particular the quadratic data tensor
contraction which sums along a 50x50x9 = 22500 axis.

How do I solve this problem?
    1) Verify that this is the bottleneck.
    2) Wrap fcn using gvar_function and compute the jacobian with jax.

2022-05-05
==========

Commands to get the version number:

grep __version__ lsqfitgp/__init__.py | sed -e "s/__version__ = //" -e "s/'//" -e "s/'//"
python -c 'import lsqfitgp;print(lsqfitgp.__version__)'

2022-05-31
==========

Next thing to do: test jax jit.

2022-06-01
==========

About Fisher scoring: the expected value of the log marginal likelihood is:

    E[-1/2 log(det(V)) -nlog(2π) -1/2 (x-mu)^TV^-1(x-mu)] =
    -1/2 log(det(V)) -nlog(2π) -1/2 V^-1_ijE[(x-mu)_i(x-mu)_j] =
    -1/2 log(det(V)) -nlog(2π) -1/2 V^-1_ij V_ij =
    -1/2 log(det(V)) -nlog(2π) -1/2 tr(V^-1 V) =
    -1/2 log(det(V)) -nlog(2π) -1/2 tr(I) =
    -1/2 log(det(V)) -nlog(2π) -1/2 n

So in practice the thing amounts to computing the hessian of -1/2 log(det(V)),
but the gradient of the full likelihood, and put these into a second order
minimizer. Algorithms that may be appropriate in optimize.minimize: dogleg,
trust-exact.

**** WRONG, SEE 2022-06-06 ****

Other hack-hessian option: external product of the residuals.

    V = LL^T
    V^-1 = L^T^-1 L^-1
    r = L^-1 (x - mu)

    D (-1/2 r^T r) = -r^T Dr
    D^2 (-1/2 r^T r) = -Dr^T Dr -r^T D^2r
    
So I would just keep -J^T J, which is positive definite, instead of the full
hessian. To do this with autodiff I would need to let jax go through the matrix
decomposition. Tentative interface: add an option direct_autodiff to the
__init__ wrapper in DecompAutoDiff.

2022-06-02
==========

The second derivative of logdet does not work. Possibilities:

1) I'm doing the test wrong => unlikely because it fails both with analytical
   solution and finite differences
2) There is a sneaky error in the jvp of solve
3) I have not properly understood how the tracing-stopping works and it doesn't
   with multiple layers to unpack

First things to do:

1) Check finite differences against the partially handwritten solution
2) Implement direct_autodiff and see if it works.

=> with direct_autodiff, the derivatives work, both compared to finite
differences and to the handwritten solution. This leaves (2) and (3), and I
guess (3) is the case since the jvp of solve is tested on its own.

I have no idea on what's going wrong, and I don't feel like diving into jax
tracing, so the next thing to do is implementing the second derivative test
for solve and quad and see if they fail too. Since I expect debugging this
will require some time, after checking that direct_autodiff works in those
cases I should switch to improving pdf*.py and using hessians in empbayes_fit.

2022-06-04
==========

Some benchmarks of toeplitz chol matvec:

In [344]: def trychol(n, func):
     ...:     t = jnp.exp(-1/2 * jnp.linspace(0, 5, n) ** 2)
     ...:     t = t.at[0].add(lgp._toeplitz.eigv_bound(t) * n * finfo(float).eps)
     ...:     b = np.random.randn(n)
     ...:     getattr(lgp._toeplitz, func)(t, b)
     ...: 

In [345]: %timeit trychol(10000, 'chol_solve')
440 ms ± 7.56 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

In [346]: %timeit trychol(10000, 'cholesky')
227 ms ± 1.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

In [347]: %timeit trychol(10000, 'cholesky_jit')
159 ms ± 164 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

In [348]: %timeit trychol(10, 'cholesky_jit')
692 µs ± 214 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)

In [349]: %timeit trychol(10, 'cholesky')
68 ms ± 146 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

In [350]: %timeit trychol(10, 'chol_solve')
910 µs ± 4.26 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)

2022-06-05
==========

The forward jacobian of quad and solve seems more accurate with custom
derivatives than with direct ones, but maybe only for some matrices.

--------

The forward second derivatives work. I can move on to implementing
hyperparameters algorithms.

2022-06-06
==========

Interesting piece of code:

File ~/Documents/Scuola/lsqfitgp/Repository/lsqfitgp/pythonvenv/lib/python3.10/site-packages/scipy/optimize/_trustregion_exact.py:245, in IterativeSubproblem.__init__(self, x, fun, jac, hess, hessp, k_easy, k_hard)
    242 self.dimension = len(self.hess)
    243 self.hess_gershgorin_lb,\
    244     self.hess_gershgorin_ub = gershgorin_bounds(self.hess)
--> 245 self.hess_inf = norm(self.hess, np.Inf)
    246 self.hess_fro = norm(self.hess, 'fro')
    248 # A constant such that for vectors smaler than that
    249 # backward substituition is not reliable. It was stabilished
    250 # based on Golub, G. H., Van Loan, C. F. (2013).
    251 # "Matrix computations". Forth Edition. JHU press., p.165.

--------

I noticed that I made a mistake in computing the Fisher matrix, I took the
expected value *before* differentiating. The correct expression is

    E[∂∂l] = 1/2 tr(K^-1 ∂K K^-1 ∂K)

How do I implement it in jax? => Use lax.stop_gradient(K_dot) in solve_vjp

2022-06-07
==========

Jax stuff
---------

From jax #9973:
jax.config.update('jax_default_matmul_precision', jax.lax.Precision.HIGHEST)
The default is not highest.

jax.lax.cond is the jittable equivalent of if
see https://github.com/google/jax/discussions/10306 about evaluating or not both
branches of a conditional.

https://github.com/google/jax/issues/5507#issuecomment-767931338
"JAX handles NumPy constants with zero strides in a smart way."

To pass a marked-as-zero tangent to jax.jvp, use the special dtype
jax.dtypes.float0. May work also for integer inputs not to be differentiated.

jax.tree_util.tree_map allows mapping a function through a set of trees. I
could use it with StructuredArray in place of my current custom traverser.
To handle array fields, transpose everything before and after the reduction,
to have automatic broadcasting (does tree_map support broadcasting?)

see https://github.com/josipd/jax/blob/master/jax/experimental/jambax.py to
convert numba functions to jax primitives

https://github.com/google/jax/discussions/10004 about using Fisher matrices

2022-06-08
==========

Next things to do:

1) Write pdf9.py stripping pdf8.py of all non-linear things and using
empbayes_fit for the hyperparameters (DONE)

2) Scale pdf9.py to a large dataset (SOMEWHAT DONE)

3) Write pdf10.py like pdf9.py but using real PDF data

2022-06-10
==========

I did the calculation of the Fisher information when the residuals (i.e., the
mean vector) depend on the hyperparameters. The result is that I have to add a
term ∂r^T K^-1 ∂r, i.e., quad(rjac).

2022-06-13
==========

pdf9.py with 500 datapoints runs smoothly, but does not give something
compatible with the truth. Even judging by eye, the fitted correlation length
is too short, the valence shell looks shrinked. The minimum seems robust to
change of fitting method. Ideas about this:

1) Instrument properly empbayes_fit
2) Maybe the link matrix with iid normal entries is particularly pathological
   for some reason. Like, the effect tend to cancel out statistically as I
   increase the size of the matrix, so it is numerically inaccurate doing
   the inference.

Another fit that does not work is the periodic one in test_fit. Maybe exploring
that will clear my understanding of using Laplace with a Gaussian process.

2022-06-17
==========

Thing that I could cite: Snelson et al., "Warped Gaussian Processes". It's a
bit trivial but people expect you to cite everything. Everything was redone
for Gaussian processes.

2022-06-20
==========

sinc(x) = sin(x)/x

sinc'(x) = cos(x)/x - sin(x)/x^2
         = (cos(x)x - sin(x))/x^2 =
         = cos(x) (x - tan(x)) / x^2

I have to compute a - b with a - b ~ 0:

a - b = b (a/b - 1)
a - b = log exp(a - b) =
      = log(exp(a/b - 1)^b) =
      = b log exp(a/b - 1)

Does not solve the problem, a/b is already imprecise.

2022-06-29
==========

I learned that the FK tables (the matrices PDF -> linear data) are 90 %
diagonal and the entries are mostly positive.

2022-07-04
==========

AR kernel
---------

- generate the companion matrix of the characteristic polynomial
- diagonalize it to get the roots
- evaluate the polynomial but for each root in turn to get the coefficients
  (not really sure about this part)
- evaluate the combination of powers

Alternative: parameterize with roots and amplitudes, then add class method
to convert parameters to coefficients

Alternative: rotate the yule-walker equations

Question: is the exponential mixture parametrization valid with roots with
multiplicity? For high enough distance I'm sure it is, there may be problems
near zero => Guy on wikipedia
https://en.wikipedia.org/wiki/Talk:Autoregressive_model#Multiplicities_in_characteristic_polynomial
says the general form is a combination of t^r y^-t, with y a root and r
an integer going up to y's multiplicity - 1. (ask Luca for a book about this)

Tentative interface: four parametrizations:
1) w : (p,)
      autoregressive weights
2) y : (p,)
      roots > 1
   a : (p,)
      amplitudes >= 0 for the terms y^-t
3) y : (n,)
      roots
   a : (n, m)
      amplitudes for the terms t^0 y^-t, ..., t^m-1 y^-t (p = n * m)
4) c : (p + 1,)
      first terms of the autocovariance function

=> Problem: there are nontrivial positivity constraints on the coefficients
in the case with multiplicity

=> Actually, even though (2) is always pos def, the amplitudes are uniquely
determined by the roots through the char poly. So I should take the roots
and amplitudes, generate the polynomial, compute the acf, and solve a linear
system for the amplitudes. I can't do this right away from the coefficients
because I need the multiplicity of the roots; my intuition suggests that
it is numerically more stable to evolve the acf instead of using the
analytical expression. The speed should be analogous in practical cases.

Problem with evolving the acf: the maximum lag, and thus the length of the
array, is determined by the data. This is not permitted under the jit =>
add a parameter maxlag, fill with nan for lags not computed.

New tentative parametrizations:
1) w : (p,)
      autoregressive weights
   maxlag : int
      maximum precomputed lag, outside will be filled with nan (must be
      specified even without jit---otherwise the eventual failure under jit is
      confusing)
2) c : (p + 1,)
      first terms of the autocovariance function
   maxlag : int
      maximum precomputed lag, outside will be filled with nan
3) y : (p,) complex
      roots > 1, multiplicity inferred with strict comparison (need a bit
      of work to group roots under the jit due to fixed size, but can be
      done)
4) f : (p,) complex
      y = exp(f)
5) y : (n,) complex
      Roots
   m : (n,) int
      Multiplicities
6) f, m like (4)-(5)

Problem: the complex roots must be conjugate-paired. If I take the real part,
implicitly it's like I'm duplicating any complex root which isn't paired.
However this also means that the actual AR order is higher than the number of
roots provided.

Actually, must the roots necessarily come in pairs? => Yes, and this holds
also with any multiplicity
https://en.wikipedia.org/wiki/Complex_conjugate_root_theorem

This poses continuity problems in the parametrization. I guess that as the
imaginary part of a complex root goes to zero, there will be numerical accuracy
issues since the solution must tend to a multi-pole filter. What should happen
is that the imaginary part of the amplitude diverges to compensate the period
of the sine going to infinity, this gives a linear slope in the limit.

2022-07-05
==========

New AR parametrization:

 1) w : (p,)
        autoregressive weights
    maxlag : int
        maximum precomputed lag, outside will be filled with nan (must be
        specified even without jit---otherwise the eventual failure under jit is
        confusing)
 2) c : (p + 1,)
        first terms of the autocovariance function
    maxlag
 3) lnr : (n,) real
        logarithm of real roots (multiplicity 1), > 0
    lnc : (m,) complex
        logarithm of complex roots (multiplicity 2), real part > 0
    mr : (n,) int
    mc : (m,) int
        multiplicities of roots, if None assumed 1, no check for equality

Problem: how do I implement the multiplicity-dependent terms in a
jit-compatible way? The size of the system changes based on the data. => Put
the roots in an array with multiplicity indicated by repetition. After sorting,
scan keeps track of repetition (a fixed size state is sufficient for this) and
generates the appropriate powers of t as needed along the way.

Problem 2: zero imaginary part in complex roots. I think this can be solved
with a pseudoinverse without further problems.

Problem 3: sign of real roots. I can take the sign of `lnr` as the sign and
its absolute value as logarithm. (The reason for using logarithms is that
roots near 1 would be numerically inaccurate. Also, the logarithm is
interpretable as the inverse of the correlation length.)

New AR parametrization:

 1) w : (p,)
        autoregressive weights
    maxlag : int
        maximum precomputed lag, outside will be filled with nan (must be
        specified even without jit---otherwise the eventual failure under jit
        is confusing)

    Implementation: compute the acf with inverse YW and evolve it up to
    maxlag. Do not check that the w are valid.

 2) c : (p + 1,)
        first terms of the autocovariance function
    maxlag

    Implementation: compute the coefficients with YW and evolve the acf like
    above. Do not check c is valid.

 3) slnr : (n,) real
        logarithm of real roots (multiplicity = 1 x exact repetitions), the
        sign is the sign of the root and the absolute value is the logarithm
    lnc : (m,) complex
        logarithm of complex roots (multiplicity = 2 x exact repetitions), real
        part > 0
    
    The multiplicity is assessed with strict equality. Very close yet distinct
    roots are treated as separate and lead to numerical instability. The order
    of the AR is n + 2 * m. Complex roots which are actually real or almost
    real lower the actual order but are not numerically problematic.
    
    Implementation: sort, then loop keeping track of repetitions generating the
    coefficients of the linear system for the amplitudes of the acf. Do a
    pseudoinverse to accomodate real or almost real complex roots. Generate the
    polynomial from the roots and compute the acf (the RHS) with inverse YW.
    Finally evaluate the analytical expression of the acf.

Problem: it would be very useful for the user to have a check on the validity
of w and c.

For w I can generate the polynomial, compute the roots and check they are above
1. In practice I expect the roots to be quite close to 1 because otherwise the
correlation length is less than 1 time step. I estimate this check to be
numerically accurate only up to ~tens of coefficients. Since I obtain the roots
as eigenvalues, I could use as tolerance the same I use for positivity checks.
I fear additional numerical instability I'm unaware of.

For c I can generate w from YW and check w. Since YW is invertible, obtaining a
valid w means that c was valid.

The most annoying task in all this is porting polyfromroots to jax. I bet that
a naive implementation would be too inefficient and inaccurate.

Also add the parameter conversion functions as static methods.

--------

Problem: even though the roots as logarithms preserve the numerical accuracy
near 1, I have to determine phi and then gamma from the characteristic
polynomial. Phi can not come out accurate in general because I need to pass
from the actual roots, also think about AR(1) where phi = 1/root. However this
affects only the amplitudes, not the decay. Moreover, with slow decay, if I 
use as RHS the first p terms, they will all be almost equal, so the system for
the amplitudes is very degenerate. I should pick the lags based on the
correlation lengths. => Or is there some way to directly obtain the phi for
the decimated process? That way I can move the "effective" roots away from 1.
=> I think that multiplying all the logarithms by an integer constant would
be sufficient.

2022-07-06
==========

Complete reasoning: using the analytical expression of the autocovariance,
I can obtain the one of the decimated process simply by raising all roots to
the decimation. The amplitudes do not change.

Question: is this an autoregressive process? Because the amplitudes are
determined by the roots. => Yes because I can expand recursively to obtain the
lagged y. However, the order will in general be infinite.

Thinking in Fourier space, the relationship between the amplitudes w/ and w/o
dilated roots is not trivial. Maybe in the end the first idea, determining
analytically the amplitudes, was the right one. The denominator is a product of
differences of the roots, which can be computed accurately with complex
sum(logaddexp).

Actually, logaddexp won't be numerically accurate on a difference. In general
it is impossible to recover accuracy because the roots can be arbitrarily close
in relative error (even in logarithm parametrization) and so their difference
is inherently imprecise. This means that log(roots) is not the best
parametrization for accuracy, I need to use diff(sort(log(roots)), prepend=0).

Or is it more convenient log (r_i+1 - r_i)/r_i, with r_0 = 1? This would be in
(-oo, oo) and be about the same value for evenly spaced lengthscales (need to
check this). Then if I compute the logarithm of the amplitudes with proper
combinations of logsumexp, I can write the autocovariance as a logsumexp itself,
which maybe allows to have arbitrarily close roots.

This parametrization implies that I have to treat all roots as complex. Might
as well take the real part only in the end. Wait, do I sort by real part or by
modulus? Maybe the parametrization should handle separately real parts and
phases => The phase is bad as parameter, must use numbers in (-oo, oo).

2022-07-07
==========

A random walk on the phase as prior does not make much sense, so maybe the
phases should be kept separate and not parametrized incrementally?

Tentative: use

    z_i = log (|r_i+1| - |r_i|)/|r_i|,  r_0 = 1,
    
and x_i, y_i such that arg(r_i) = atan2(y_i, x_i).

The quantities I need to obtain from xyz are log(r_i) and r_i - r_j for any
pair i, j. => I can already see it does not work: the differences in the
phases are not numerically accurate.

If I use z_i = log(r_i+1/r_i), a uniform i.i.d. prior on arg(z_i) implies a
uniform i.i.d. prior on arg(r_i).

    r_i = r_i/r_i-1 r_i-1/r_i-2 ... r_1/r_0
    
    log(r_i) = z_i-1 + z_i-2 + ... + z_0 = sum_k=0^i-1 z_k
    log(r_0) = 0
    
    d_i = r_i+1 - r_i = exp(sum_k=0^i z_k) - exp(sum_k=0^i-1 z_k) =
        = exp(sum_k=0^i-1 z_k) (exp(z_i) - 1) =
        = r_i expm1(z_i)

Can I break the accuracy of this? Take

    r_1 ~ 1, r_2 ~ -1, r_3 ~ 1.
    
So we have that r_1 is very close to r_3, but r2 is distant from both.

    r_2/r_1 ~ -1  -->  z_1 ~ 0 + i pi
    r_3/r_2 ~ -1  -->  z_2 ~ 0 + i pi
    
    d_1 = r_1 (e^z_1 - 1) ~ 1 (-1 -1) = -2
    d_2 = r_2 (e^z_2 - 1) ~ -1 (-1 -1) = 2
    
    r_3 - r_1 = d_1 + d_2 ~ 0  -->  cancellation!

If the parametrization is ordered w.r.t. the modulus, then it statisfies r_i >
1 but distant roots can be closer than neighbor roots because the phases
rotates. With lexicographical order there is the same problem. Indeed, I can
have paths on the complex plane such that roots get arbitrarily close again
after an arbitrarily long time (as a spiral). Indeed the fundamental problem
is that it is not possible to sort complex numbers topologically.

What is the least evil then? Depending on how the user wants to put the roots
around, different parametrizations would be appropriate. The base
parametrization should be convenient for generic use, and not impede any
sensible strategy from being adopted.

First I can send the donut into the open circle with the transformation 1/r.
Then maybe I can send the border of the circle to infinity somehow, to have
a parametrization in full R^2. What is the appropriate way here to send a
radius in [0, 1) to one in [0, infty)? It should probably be something that
plays nicely with complex numbers.

However I think it can't be done with an olomorphic function because the
radial stretching doesn't preserve angles.

Tentative:

    |z| = -log(1 - 1/|r|)
    arg z = -arg r

For |r| -> oo, |z| ~ 1/|r| -> 0

For |r| -> 1+, |z| = -log (|r| - 1)/|r| -> -log(|r| - 1) -> inf

If r = 1 + e, e << 1, the lengthscale is 1/log r = 1/log(1 + e) ~ 1/e. The
logarithm of the lengthscale is ~ log(1/e) = -log e = -log(r - 1).

Thus |z|, if somewhat large, is interpretable as the logarithm of the
correlation length.

For r -> oo, scale is 1/log r while z is 1/r. So scale is 1/log(1/z), z is

    s = 1/log r
    log r = 1/s
    r = exp(1/s)
    z = 1/r = exp(-1/s)

Thus for small z, infinitesimal changes bring the correlation length away from
zero, which makes sense since below 1 at some point it's all white noise the
same.

For |r| -> oo, z behaves like (complex) 1/r, since

    |z| ~ 1/|r|, and
    arg z = -arg r.

This means that locally in 0 the circle is sent to itself, this means that the
derivatives can be defined. The stretch becomes "less olomorphic" moving
towards the border.

Now about the differences. Since there is no general compact strategy to
represent accurate differences, I need the user to pass (optionally) the full
matrix of pairwise differences. They have to be in z space to have domain R^2.

    1 - 1/|r| = exp(-|z|)
    1/|r| = 1 - exp(-|z|)
    |r| = 1/(1 - exp(-|z|)) = -1/expm1(-|z|)
    arg r = -arg z
    r = |r|exp(i arg r)
    
Ho d = z - z' (oppure su modulo e fase) e voglio calcolare r - r' usando d.
    
(Usare |z|^2 = -log(1 - 1/|r|^2) mi aiuterebbe? Boh)

    r - r' = |r|exp(i arg r) - |r'|exp(i arg r') =
           = |r|exp(i arg r)(1 - |r'/r|exp(i(arg r' - arg r)))
    
    |r'/r| = (1 - exp(-|z|)) / (1 - exp(-|z'|)) =
           = (exp(|z|') - exp(-|z| + |z'|)) / (exp(|z'|) - 1) = ?
    
    1/|r'| - 1/|r| = (1 - exp(-|z|')) - (1 - exp(-|z|)) =
                   = exp(-|z|) - exp(-|z'|) =
                   = exp(-|z|) (1 - exp(|z| - |z'|)) =
                   = -exp(-|z|) expm1(|z| - |z'|)
    
    |r'/r| = |r'|(1/|r| - 1/|r'|) - 1 --> ?

Ok I have it:

    |r| - |r'| = 1/(1 - exp(-|z|)) - 1/(1 - exp(-|z'|)) =
               = (1 - exp(-|z'|) - 1 + exp(-|z|)) / ((1 - exp(-|z|))(1 - exp(-|z'|))) =
               = |r||r'|exp(-|z'|)(exp(|z'| - |z|) - 1) =
               = |r||r'|exp(-|z'|)expm1(|z'| - |z|)
    
    r - r' = |r|exp(i arg r) - |r'|exp(i arg r') =
           = |r|exp(i arg r) - |r|exp(i arg r') + |r|exp(i arg r') - |r'|exp(i arg r') =
           = |r|exp(i arg r) (1 - exp(i (arg r' - arg r))) + (|r| - |r'|)exp(i arg r') =
           = -r expm1(i (arg r' - arg r)) + (|r| - |r'|)exp(i arg r')

Mmm but I'm actually interested in 1/(r - r'). So
    
    x = 1/r
    |x| = 1 - exp(-|z|)
    arg x = arg z
    
    r - r' = 1/x - 1/x' =
           = (x' - x)/(xx')
    
    1/(r - r') = xx'/(x' - x)
    
    |x'| - |x| = exp(-|z|) - exp(-|z'|) =
               = exp(-|z'|) (exp(|z'| - |z|) - 1)
    
    x' - x = |x'|exp(i arg x') - |x|exp(i arg x) =
           = (|x'| - |x|)exp(i arg x) + |x'| (1 - exp(i (arg x - arg x'))) =
           = exp(-|z'|) (exp(|z'| - |z|) - 1) exp(i arg x) +
             + (1 - exp(-|z|')) (1 - exp(i (arg x - arg x')))
        
    xx' = |x||x'|exp(i (arg x + arg x'))

About the same, can't really write it in a better way. Now it remains to get
the differences in |z| and arg z from the one in z = a + ib.

    |z| - |z'| = √(a^2 + b^2) - √(a'^2 + b'^2) =
               = |z'|(|z/z'| - 1) =
               = |z'|(√((a^2 + b^2)/(a'^2 + b'^2)) - 1)

I have sqrtm1, so let's go inside

    A = a - a'
    B = b - b'

    |z|^2 = a^2 + b^2 =
          = (a' + A)^2 + (b' + B)^2 =
          = a'^2 + b'^2 + 2(a'A + b'B) + A^2 + B^2 =
          = |z'|^2  +  2(a'A + b'B) + A^2 + B^2

Then the phase

    tan arg z' = b'/a'
               = (b - B)/(a - A) =
               = b/(a - A) - B/(a - A) =
               = b/a 1/(1 - A/a) - B(a - A)
    
    (f = arg z - arg z')
    
               = tan (arg z - f) =
               = (tan arg z + tan f) / (1 - tan arg z tan f)

Etc., I guess the calculation can be expressed in terms of cosm1. => No, wait:
I can do it inverting the expression for z - z' in terms of |z| - |z'|, it
contains 1 - exp(i (arg z - arg z')), which is directly what I need.

    z' - z = (|z'| - |z|)exp(i arg z) + |z'| (1 - exp(i (arg z - arg z'))) -->
    
    --> 1 - exp(i (arg z - arg z')) =
        = (z' - z)/|z'| + (|z| - |z'|)/|z'| z/|z|
        
Maybe I could have a parametrization in terms of |z|, a, b where

    z = |z|(a + ib)/hypot(a, b)
    
to have uniform phase distribution but |z| away from zero. => But then |z|
would have to be parametrized with log|z|, log(log(scale))... not convenient.

2022-07-08
==========

The roots at infinity mean that the effective order of the AR is lower, so
shrinkage to z = 0 selects for short AR.

2022-07-14
==========

To parametrize arbitrary roots effectively, I have to find a way to map an
arbitrary invalid difference matrix to a valid one, or in general a
latent parametrization of a difference matrix.

2022-07-16
==========

release notes for 0.12:

--removed maternX2
--removed ratquad
--added cauchy
--added maxdim check
--support numpy functions on StructuredArray
--conversion StructuredArray -> array
--conversion StructuredArray -> unstructured
--Constant and White support arbitrary input (not just numerical)
--Matern now works with any derivative for any nu, extended to nu=0
--fix derivatives for GammaExp
--Gibbs works in nd
--fractional -> bifractional
--extend fractional to negative axis
--rename ppkernel -> wendland, accept real parameter
--extend fourier to arbitrarily large n
--hole effect kernel
--bessel kernel
--pink kernel
--color kernel
--sinc kernel
--stationary frac brownian
--causalexpquad
--decaying
--log
--circular
--MA
--AR
--BART

2022-07-17
==========

Are all the amplitudes of the AR correlation always positive? (With an
appropriate convention for phases)

The characteristic polynomial is P(x) = 1 - phi @ x^n, and all the roots have
|x| > 1. P(0) = 1 + 0j, but I don't know how to extend the positivity on the
complex plane. Maybe there is something about radii like with power series.

I dunno. I should expose a function to compute the amplitudes and try to get
them negative to see if it's a good idea.

=> It's not a good idea, obvious in hindsight.

2022-07-19
==========

Above I wrote that I can compute the AR amplitudes accurately if I start from
the differences between the roots. However, as the roots get close together,
the amplitudes diverge with opposite phases with a precise cancellation to make
the exponentials into a power law. This means that probably I also need the
next order differences, and so on.

But when the roots are close I expect the shape to be more or less fixed. It's
the damn power law I know exactly. So maybe I can change formula when
the roots are closer than something. The only thing that I still can't compute
is the final variance. I have the same problem in computing gamma, where with
very small roots I know it's practically constant, however the normalization
heavily depends on how close they are.

Since huge normalizations are not really interesting, I can solve this kind of
problems by always normalizing the variance to 1.

2022-07-20
==========

Can I write exp(x) - 1 - x as a hypergeometric function?

    exp(x) - 1 - x = 
    = sum_k=2^oo x^k / k! =
    = x^2 sum_k=0^oo x^k / (k + 2)! =
    = x^2 sum_k=0^oo k!/(k+2)! x^k / k! =
    
        k!/(k+2)! = 1·2···k / 1·2···k+2 = 1/2 (1)_k/(3)_k
    
    = x^2/2 sum_k=0^oo (1)_k/(3)_k x^k / k! =
    = x^2/2 1F1(1, 3, x)

However in scipy 1F1 is implented by summing the taylor series, and 1F1 is not
implemented in jax, so whatever.

--------

I think that the space of allowed phi is a simplex whose vertices are the
polynomials with roots either -1 or 1. To prove it, I need to prove that
    1) any convex cobination of valid phi is valid
    2) any valid phi can be expressed as a convex combination of the vertices
Regarding 1), see
https://en.wikipedia.org/wiki/Geometrical_properties_of_polynomial_roots
https://en.wikipedia.org/wiki/Rouché%27s_theorem

--------

Method to allow arbitrarily many large roots in np.poly:
    1) find the smallest in modulus non-zero root x_0
    2) rescale all roots such that |x_0| = 1
    3) invert roots, let zero roots be inf (check complex does not produce nan)
    4) use poly
    5) reverse
    6) rescale with cumprod(|x_0|)
