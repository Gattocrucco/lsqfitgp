La formula con le gvar per il processo gaussiano è

    y* = y*_p + Kxsx (Kxx + Cy)^-1 (y - y_p)

Avevo già controllato che questa formula mi dà la matrice di covarianza
corretta per y*. Però vengono fuori anche le correlazioni tra y* e y, y_p,
y*_p. Che cosa rappresentano statisticamente?

Assumiamo che y e y(*)_p siano indipendenti. Allora

    Cov[y*, y*_p] = Cov[y*_p] = Kxsxs

    Cov[y*, y_p] = Cov[-Kxsx (Kxx + Cy)^-1 y_p, y_p] =
                 = -Kxsx (Kxx + Cy)^-1 Kxx
    
    Cov[y*, y] = Kxsx (Kxx + Cy)^-1 Cy

2022-04-17
==========

Cose da fare per le PDF:

1)  Mi invento io delle funzioni e dei dati dal culo e faccio un fit,
    solo roba lineare (FATTO)
2)  Aggiungo roba quadratica (sempre dal culo) (FATTO)
3)  Aggiungo l'errore sulle M (FATTO)
4)  Uso delle PDF vere anziché culiche
5)  Aggiungo i dati veri

2022-04-18
==========

Come implemento addproc etc.?

1)  Chiamo _KernelDeriv -> _CrossKernel (FATTO)
1a) Sposto la logica di diff fuori da fun e giro i test (FATTO)
1b) Modifico diff in modo da tener conto che le x a sinistra possono essere di
    tipo diverso dalle x a destra (SALTARE)
2)  Sposto _binary in _KernelBase e tolgo il controllo che gli scalari siano
    positivi, come classe uso type(self) anziché Kernel (FATTO)
3)  Sovrascrivo _binary in Kernel controllando che l'eventuale scalare sia
    positivo e poi chiamo super (FATTO)
4)  Aggiungo un metodo rescale a _KernelBase, funziona come diff cioè fa
    trasf. diverse per x e y, se sono diverse (definito con is not) cambia la
    classe dell'output a _CrossKernel (FATTO)
5)  Definisco nuove classi _Proc, _ProcKernel, _ProcTransf e _ProcDeriv
    analoghe di _Element, _Points e _Transf (NON FARE _ProcDeriv) (FATTO)
5a) Aggiungo un attributo proc di tipo _Proc a _Points (FATTO)
6)  Definisco _GP._crosskernel che calcola il kernel per due _Points
    ricorsivamente, con metodi analoghi a quelli che uso per implementare
    _makecovblock: _crosskernel_kernel_kernel, _crosskernel_transf_any,
    _crosskernel_deriv_any (NON FARE _deriv) (FATTO)
7)  _crosskernel_kernel_kernel semplicemente restituisce il kernel se i due
    _ProcKernel coincidono, altrimenti un sigleton _ZeroCov (FATTO)
8)  _crosskernel_deriv_any calcola il kernel e poi chiama diff, _ZeroCov viene
    passato (SALTARE)
9)  _crosskernel_transf_any cicla sugli elementi calcolando i kernel e
    combinandoli con _binary, gli _ZeroCov vengono ignorati, gli scalari nulli
    non producono _ZeroCov altrimenti si incasinano le derivate, alla fine se è
    uno _ZeroCov viene passato (FATTO)
10) Aggiungo GP.addproc, addproctransf e addprocderiv, i _Proc vengono infilati
    in un dizionario apposito, possono avere gli stessi nomi degli x (NON FARE
    addprocderiv) (FATTO)
11) Aggiungo proc= ad addx, deriv= continua ad avere la sua implementazione a
    parte in _Points e _makecovblock_points (FATTO)
12) Controllo che le x abbiano lo stesso tipo, per adesso (FATTO)
13) Riimplemento la somma di componenti con questa roba (FATTO)

Problema serio: come faccio a specificare cosa derivare se ogni processo ha la
sua x? Cioè: le x sono le stesse, oppure ogni processo ha la sua x?

=> Soluzione rapida e parziale: posso derivare solo singoli processi e non le
trasformazioni. In questo modo continuo ad assumere che le x siano le stesse.

=> Soluzione più completa: quando dico la derivata, devo poter specificare
rispetto a quale processo, o se derivare rispetto a tutte le x, in generale
devo poter specificare vari criteri. Forse è troppo complicato e l'utente non
avrebbe chiaro cosa sta facendo.

L'interfaccia elegante sarebbe questa: quando definisco una trasformazione dei
processi, devo dire se le x sono le stesse o sono diverse. In formule, devo
distinguere

    h(x) = f(x) + g(x)

da

    h(x, y) = f(x) + g(y)

Un modo sensato sarebbe proprio leggersi una stringa del genere. Una versione
più accroccata intermedia sarebbe un'opzione di addproctransf che mi
permette di dire se le x sono tutte le stesse o sono diverse, per fare
combinazioni più strane chiamare più volte addproctransf. Se sono le stesse poi
addx controlla che metti dentro x con lo stesso dtype. Poi però devo inventarmi
un modo di propagare queste informazioni fino a _KernelBase.diff.

--------

What calculation should I do to bypass the limitation that new gvars can not be
correlated with old gvars?

Old = x, new = y

Desired covariance matrix:

    V = [ V_xx   V_xy]
        [ V_yx   V_yy]

z = auxiliary variables, as many as y, independent of x

I have to write y as a combination of z and x:

    y = Az + Bx

Cov[x, y] = Cov[x, Bx] = V_xx B^T = V_xy   ==>  B^T = V_xx^-1 V_xy
Cov[y, y] = Cov[Az, Az] + Cov[Bx, Bx] = A V_zz A^T + B V_xx B_T = V_yy  ==>
    ==>  A V_zz A^T = V_yy - V_yx V_xx^-1 V_xy = V/V_xx

I can choose A = I and then V_zz = V/V_xx. I have to decompose V_xx.

2022-04-22
==========

The nonlinear fit isn't working. With enough quadratic data the fitted data
(even the linear one) is far from the actual data, but even with one single
nonlinear point the fit starts to fail in some places.

Is this a bug or an excess of nonlinearity?

In the latter case, I expect that reducing the errors on the quadratic data
should effectively linearize it. Then, provided I start the fit already in the
correct neighborhood, it should work.

=> tried: it's even worse!

I'm combining 9 latent points into one single quadratic datapoint. Maybe even
if the error on the datapoint is small, the freedom left to the latent points
makes their nonlinear bias large.

=> Using only one point as quadratic input, still does not work at all. This
suggests it is a trivial bug of some sort.

Bug found: I was passing the prior through the nonlinear function and then
sampling, instead of sampling and then passing the sample through the nonlinear
function. Since the prior is wide, it was being affected a lot by the
nonlinearity.

--------

Next thing to do: pdf6.py, fit hyperparameters and quadratic data together. I
foresee that it will be slow because I can't take derivatives through lsqfit
with autograd. I should read how lsqfit.empbayes_fit is implemented.

Actually, I could first add errors to M and M2 instead of fitting kernel
hyperparameters.

2022-04-23
==========

I must also devise a convenient extension of predfromfit for the case where I
condition on some stuff with predfromdata and on some other with lsqfit. For
the time being, since the sum rule constraints are exact, I think that
conditioning twice is valid (but I should check to be sure, maybe add a test
case).

--------

The fit with the errors on M is not working. Two alternative explanations:

  * It can't work because there is too much uncertainty. But I would expect the
    fit to return an appropriately large sdev in this case, which is not
    happening.

  * The sheer amount of the M parameters (2700) makes the minimization "happy"
    too early because M is easy to fit.

To check this, I can hold M fixed to its mean (different from the true value
used to generate the data) without fitting it. If the result I get is still
nonsense, then probably it's a statistical and not a numerical problem.

=> The fit makes sense. This indicates that it's a numerical problem. Possible
quick hack: very low termination tolerance.

=> The quick fix didn't work, but starting from the true values gives a good
result, which confirms it is a numerical problem.

Anyway now I found out that in the actual problem M is actually parametrized by
just a few numbers despite being a large matrix, so whatever.

To do next time: pdf7.py, i.e., fit hyperparameters on top of nonlinear fit
(will be damn slow!)

Following steps:
  * pdf8.py scale the number of datapoints/parameters to a realistic one
  * pdf9.py change the fake data, the grids and the kernel to realistic ones

--------

I did a quick benchmark of pdf6. Most of the time is spent in evaluating the
jacobian of the model function with gvar. Time per call:

    fcn(floats): 200 us
    fcn(gvars): 230 ms
    forward jacobian with jax: 30 ms
    backward jacobian with jax: 3 ms
    jacobian with autograd: seconds

I don't expect to find a way to easily optimize the gvar calculation. 7 times
slower than the compiled jax version seems already quite good considering it's
naive sparse operations.

The other half of the time spent by the fit is computing as svd in trf, I
assume it's the svd of the jacobian. So even optimizing the derivative
calculation would give at most a factor of two. Maybe there are other faster
methods in least_squares for a large dense jacobian? => maybe lm? look also
into GSL

If I did things manually, maybe I could take into account that the jacobian is
split in two blocks, one the identity and one dense, and then multiplied by
something. If this something is a diagonalization instead of cholesky then
maybe I can obtain the svd of the jacobian.

2022-04-25
==========

If I set p0 in fitargs of lsqfit.empbayes_fit, will it still adapt p0
automatically? => Yes, it works.

Idea: since I know the components of M and M2, could I first transform the grid
of points for each component? => Maybe it is not doable for the quadratic data,
and moreover in the real problem there are more datapoints than grid points.

--------

I talked to Alessandro about the details of the complete PDF fit. Summary:

The grid of x points is
[
    1.9999999999999954e-07, # start logspace
    3.034304765867952e-07,
    4.6035014748963906e-07,
    6.984208530700364e-07,
    1.0596094959101024e-06,
    1.607585498470808e-06,
    2.438943292891682e-06,
    3.7002272069854957e-06,
    5.613757716930151e-06,
    8.516806677573355e-06,
    1.292101569074731e-05,
    1.9602505002391748e-05,
    2.97384953722449e-05,
    4.511438394964044e-05,
    6.843744918967896e-05,
    0.00010381172986576898,
    0.00015745605600841445,
    0.00023878782918561914,
    0.00036205449638139736,
    0.0005487795323670796,
    0.0008314068836488144,
    0.0012586797144272762,
    0.0019034634022867384,
    0.0028738675812817515,
    0.004328500638820811,
    0.006496206194633799,
    0.009699159574043398,
    0.014375068581090129,
    0.02108918668378717,
    0.030521584007828916,
    0.04341491741702269,
    0.060480028754447364,
    0.08228122126204893,
    0.10914375746330703, # end logspace, start linspace
    0.14112080644440345,
    0.17802566042569432,
    0.2195041265003886,
    0.2651137041582823,
    0.31438740076927585,
    0.3668753186482242,
    0.4221667753589648,
    0.4798989029610255,
    0.5397572337880445,
    0.601472197967335,
    0.6648139482473823,
    0.7295868442414312,
    0.7956242522922756,
    0.8627839323906108,
    0.9309440808717544,
    1, # end linspace
]

Number of datapoints: 4535 total, of which 3089 linear. The relative errors are
tipically 2-10 %, with some exceptions down to 0.1 % and up to 50 %.

It is convenient to describe the PDFs with a change of basis, see:

https://eko.readthedocs.io/en/latest/theory/FlavorSpace.html#qcd-evolution-basis

First, for each quark, define
    
    q+ = q + qbar,
    q- = q - qbar.

The gluon g is left alone. Then

    Sigma = sum_q q+
    V     = sum_q q-
    
    T3  = u+ - d+
    T8  = u+ + d+ - 2s+
    T15 = u+ + d+ + s+ - 3c+

    V3  = u- - d-
    V8  = u- + d- - 2s-
    V15 = u- + d- + s- - 3c-
    
So the Vs are like the Ts but with q- instead of q+. The constraints are:

    For all f: f(1) = 0

    Total momentum: sum_f x f(x) = 1

    The integrals of the Vs are
        V   3
        V3  1
        V8  3
        V15 3
    
    Sigma and g diverge power-like for x -> 0, the Ts and Vs don't
    
    For g and Sigma: x^2 f(x) -> 0 for x -> 0
    
    For Vs and Ts: x f(x) -> 0 for x -> 0
